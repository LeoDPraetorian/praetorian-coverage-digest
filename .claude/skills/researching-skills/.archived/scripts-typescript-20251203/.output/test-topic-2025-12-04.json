{
  "topic": "test topic",
  "createdAt": "2025-12-04T02:16:28.860Z",
  "context7": {
    "packages": [
      {
        "id": "/websites/x-tabdeveloping_github_io_turftopic",
        "name": "Turftopic",
        "version": "latest",
        "pageCount": 0,
        "description": "Turftopic is a topic modeling library designed to simplify and streamline the usage of contextually sensitive topic models, providing stable, minimal, and scalable implementations along with extensive",
        "status": "recommended"
      },
      {
        "id": "/emmaguoguo33/test1",
        "name": "test1",
        "version": "latest",
        "pageCount": 0,
        "description": "This is a test project for context7.",
        "status": "recommended"
      },
      {
        "id": "/websites/rs_tokio-test",
        "name": "Tokio Test",
        "version": "latest",
        "pageCount": 0,
        "description": "Tokio Test provides Tokio- and Futures-based testing utilities for Rust, including mock AsyncRead/AsyncWrite types, mock streams, task helpers for testing futures, assertion macros (e.g., assert_ok, a",
        "status": "recommended"
      },
      {
        "id": "/yandex/yandex-taxi-testsuite",
        "name": "Yandex.Taxi Testsuite",
        "version": "latest",
        "pageCount": 0,
        "description": "testsuite: microservices testing framework",
        "status": "recommended"
      },
      {
        "id": "/tibastral/program-test-boilerplate",
        "name": "Program Test Boilerplate",
        "version": "latest",
        "pageCount": 0,
        "description": "- Code Snippets: 3",
        "status": "recommended"
      },
      {
        "id": "/mwgmorningwood/context7-test",
        "name": "Context7 Repo Branch Test",
        "version": "latest",
        "pageCount": 0,
        "description": "Testing repo",
        "status": "recommended"
      },
      {
        "id": "/alibaba/testable-mock",
        "name": "TestableMock",
        "version": "latest",
        "pageCount": 0,
        "description": "TestableMock simplifies unit testing by allowing easy mocking of private methods, static methods, constructors, and any other method without initialization, regardless of how the object was created.",
        "status": "recommended"
      },
      {
        "id": "/teemtee/tmt",
        "name": "TMT",
        "version": "latest",
        "pageCount": 0,
        "description": "TMT is a test management tool that helps organize, execute, and track software tests.",
        "status": "recommended"
      },
      {
        "id": "/testably/testably.abstractions",
        "name": "Testably Abstractions",
        "version": "latest",
        "pageCount": 0,
        "description": "Write testable code by abstracting away system dependencies.",
        "status": "recommended"
      },
      {
        "id": "/super3001/testfwk_arkxtest",
        "name": "Test Fwk Arkx Test",
        "version": "latest",
        "pageCount": 0,
        "description": "This project likely provides a test framework and associated utilities for testing ArkX-related components or systems.",
        "status": "recommended"
      },
      {
        "id": "/x-tabdeveloping/turftopic",
        "name": "Turftopic",
        "version": "latest",
        "pageCount": 0,
        "description": "Turftopic is a Python library for contextual topic modeling that leverages transformer representations, offering state-of-the-art models for various scenarios, easy interpretation, and advanced topic ",
        "status": "recommended"
      },
      {
        "id": "/ethereum/execution-spec-tests",
        "name": "Execution Spec Tests",
        "version": "latest",
        "pageCount": 0,
        "description": "A Python framework and collection of test cases to generate test vectors for Ethereum execution clients",
        "status": "recommended"
      },
      {
        "id": "/test-prof/test-prof",
        "name": "TestProf",
        "version": "latest",
        "pageCount": 0,
        "description": "Ruby Tests Profiling Toolbox",
        "status": "recommended"
      },
      {
        "id": "/clojure/test.check",
        "name": "Test.check",
        "version": "latest",
        "pageCount": 0,
        "description": "test.check is a Clojure property-based testing tool that allows you to write concise, powerful tests by defining properties that should hold true for all inputs, rather than enumerating expected input",
        "status": "recommended"
      },
      {
        "id": "/elifnurdeniz/context7-test",
        "name": "Context7 Test",
        "version": "latest",
        "pageCount": 0,
        "description": "This project serves as an advanced test file.",
        "status": "recommended"
      },
      {
        "id": "/super3001/arkui-xtest-corpus",
        "name": "arkXtest",
        "version": "latest",
        "pageCount": 0,
        "description": "arkXtest is OpenHarmony's automated test framework providing unit testing, UI testing, performance testing, and mock capabilities for developers to write and execute comprehensive test cases.",
        "status": "recommended"
      },
      {
        "id": "/r-lib/testthat",
        "name": "testthat",
        "version": "latest",
        "pageCount": 0,
        "description": "testthat is an R package that makes writing unit tests for R code easy, fun, and addictive, integrating seamlessly into workflows and providing visual feedback on test progress.",
        "status": "recommended"
      },
      {
        "id": "/pytest-dev/pytest-mock",
        "name": "Pytest Mock",
        "version": "latest",
        "pageCount": 0,
        "description": "Thin-wrapper around the mock package for easier use with pytest",
        "status": "recommended"
      },
      {
        "id": "/thomhurst/tunit",
        "name": "Tunit",
        "version": "latest",
        "pageCount": 0,
        "description": "A modern, fast and flexible .NET testing framework",
        "status": "recommended"
      },
      {
        "id": "/websites/testomat_io",
        "name": "Testomat.io",
        "version": "latest",
        "pageCount": 0,
        "description": "Testomat.io is a test management system that helps users create projects, write test cases, run manual and automated tests, and analyze results with detailed reporting and analytics.",
        "status": "recommended"
      },
      {
        "id": "/teamcapybara/capybara",
        "name": "Capybara",
        "version": "latest",
        "pageCount": 0,
        "description": "Acceptance test framework for web applications",
        "status": "recommended"
      },
      {
        "id": "/x-datainitiative/tick",
        "name": "tick",
        "version": "latest",
        "pageCount": 0,
        "description": "tick is a Python 3 module for statistical learning, particularly emphasizing time-dependent modeling, offering tools for generalized linear models, optimization, and inference for systems like point p",
        "status": "recommended"
      },
      {
        "id": "/morganstanley/testplan",
        "name": "Testplan",
        "version": "latest",
        "pageCount": 0,
        "description": "Testplan, a multi-testing framework, because unit tests can only go so far..",
        "status": "recommended"
      },
      {
        "id": "/websites/testbeats",
        "name": "TestBeats",
        "version": "latest",
        "pageCount": 0,
        "description": "TestBeats streamlines test result collaboration by delivering alerts to communication platforms, simplifying test import, and offering AI-powered failure summaries for proactive optimization.",
        "status": "recommended"
      },
      {
        "id": "/airtestproject/poco",
        "name": "Poco",
        "version": "latest",
        "pageCount": 0,
        "description": "A cross-engine test automation framework based on UI inspection",
        "status": "recommended"
      },
      {
        "id": "/websites/brikev_github_io_twd",
        "name": "TWD Test While Developing",
        "version": "latest",
        "pageCount": 0,
        "description": "TWD (Test While Developing) is an in-browser test runner for frontend development, particularly React applications, offering instant feedback, API mocking, and automatic test discovery directly within",
        "status": "recommended"
      },
      {
        "id": "/google/googletest",
        "name": "GoogleTest",
        "version": "latest",
        "pageCount": 0,
        "description": "GoogleTest is Google's C++ test framework, merging GoogleTest and GoogleMock for comprehensive unit testing with features like xUnit style, rich assertions, and parameterized tests.",
        "status": "recommended"
      },
      {
        "id": "/typelevel/weaver-test",
        "name": "Weaver-test",
        "version": "latest",
        "pageCount": 0,
        "description": "Weaver-test is a Scala test framework built on cats-effect and fs2, designed for parallel test execution, composable expectations, aggregated failure reporting, and lazy logging.",
        "status": "recommended"
      },
      {
        "id": "/hypercubed/chuhai",
        "name": "Chuhai",
        "version": "latest",
        "pageCount": 0,
        "description": "Test driven benchmarking.",
        "status": "recommended"
      },
      {
        "id": "/mirage/alcotest",
        "name": "Alcotest",
        "version": "latest",
        "pageCount": 0,
        "description": "Alcotest is a lightweight and colorful test framework for OCaml, providing a simple interface for unit tests with expressive query language for test selection and colorful output.",
        "status": "recommended"
      }
    ],
    "selectedPackages": [
      "/websites/x-tabdeveloping_github_io_turftopic",
      "/emmaguoguo33/test1",
      "/websites/rs_tokio-test",
      "/yandex/yandex-taxi-testsuite",
      "/tibastral/program-test-boilerplate",
      "/mwgmorningwood/context7-test",
      "/alibaba/testable-mock",
      "/teemtee/tmt",
      "/testably/testably.abstractions",
      "/super3001/testfwk_arkxtest",
      "/x-tabdeveloping/turftopic",
      "/ethereum/execution-spec-tests",
      "/test-prof/test-prof",
      "/clojure/test.check",
      "/elifnurdeniz/context7-test",
      "/super3001/arkui-xtest-corpus",
      "/r-lib/testthat",
      "/pytest-dev/pytest-mock",
      "/thomhurst/tunit",
      "/websites/testomat_io",
      "/teamcapybara/capybara",
      "/x-datainitiative/tick",
      "/morganstanley/testplan",
      "/websites/testbeats",
      "/airtestproject/poco",
      "/websites/brikev_github_io_twd",
      "/google/googletest",
      "/typelevel/weaver-test",
      "/hypercubed/chuhai",
      "/mirage/alcotest"
    ],
    "documentation": [
      {
        "packageId": "/websites/x-tabdeveloping_github_io_turftopic",
        "packageName": "x-tabdeveloping_github_io_turftopic",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:35.696Z",
        "content": "### Install Turftopic using pip\n\nSource: https://x-tabdeveloping.github.io/turftopic/index\n\nThis command installs the Turftopic library from the Python Package Index (PyPI). Ensure you have pip installed and accessible in your environment.\n\n```bash\npip install turftopic\n\n```\n\n--------------------------------\n\n### FASTopic Model Initialization and Usage\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nThis section details how to install the turftopic library with the necessary dependencies for FASTopic, and provides a basic Python code example for initializing and fitting the FASTopic model to a corpus.\n\n```APIDOC\n## FASTopic Model\n\n### Description\nImplementation of the FASTopic model with a Turftopic API. The implementation is based on the original FASTopic package, but is adapted for optimal use in Turftopic. You will need to install torch to use this model.\n\n### Installation\n```bash\npip install turftopic[torch]\n## OR:\npip install turftopic[pyro-ppl]\n```\n\n### Usage Example\n```python\nfrom turftopic import FASTopic\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = FASTopic(n_components=10).fit(corpus)\nmodel.print_topics()\n```\n\n### Parameters\n\n**`n_components`** (int) - Required - Number of topics. If you're using priors on the weight, feel free to overshoot with this value.\n\n**`encoder`** (Union[Encoder, str], default: `'sentence-transformers/all-MiniLM-L6-v2'`) - Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n\n**`vectorizer`** (Optional[CountVectorizer], default: `None`) - Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.\n\n**`random_state`** (Optional[int], default: `None`) - Random state to use so that results are exactly reproducible.\n\n**`batch_size`** (Optional[int], default: `None`)\n\n**`DT_alpha`** (float, default: `3.0`) - Sinkhorn alpha between document embeddings and topic embeddings.\n\n**`TW_alpha`** (float, default: `2.0`) - Sinkhorn alpha between topic embeddings and word embeddings.\n\n**`theta_temp`** (float, default: `1.0`) - Temperature parameter of used in softmax to compute topic probabilities in documents.\n\n**`n_epochs`** (int, default: `200`) - Number of epochs to train the model for.\n\n**`learning_rate`** (float, default: `0.002`) - Learning rate for the ADAM optimizer.\n\n**`device`** (str, default: `'cpu'`) - Device to run the model on. Defaults to CPU.\n\n### Source Code\nSource code in `turftopic/models/fastopic.py`\n\n```\n\n--------------------------------\n\n### Basic KeyNMF Topic Modeling with Turftopic and Scikit-learn\n\nSource: https://x-tabdeveloping.github.io/turftopic/index\n\nThis Python code demonstrates the basic usage of Turftopic's KeyNMF model. It fetches the 20Newsgroups dataset using scikit-learn, trains the KeyNMF model on the corpus, and then prints the identified topics. This example assumes familiarity with scikit-learn conventions.\n\n```python\nfrom turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()\n\n```\n\n--------------------------------\n\n### Install FASTopic with PyTorch Support\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nThis command installs the turftopic library with PyTorch support, which is a dependency for using the FASTopic model. Ensure you have pip installed.\n\n```bash\npip install turftopic[torch]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/arxiv_ml\n\nInstalls the `turftopic` library along with `datasets` and `plotly`. It also installs optional dependencies for UMAP-based clustering and datamapplot visualization. This command is executed in the terminal.\n\n```bash\npip install datasets plotly turftopic[umap-learn, datamapplot]\n```\n\n--------------------------------\n\n### Install turftopic with FASTopic support\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nTo use the FASTopic model, you need to install the turftopic library with the appropriate optional dependencies. You can choose between installing with PyTorch or Pyro-PP-L support. This command installs the library and its required packages for FASTopic.\n\n```bash\npip install turftopic[torch]\n## OR:\npip install turftopic[pyro-ppl]\n```\n\n--------------------------------\n\n### Install topic-wizard (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the topic-wizard library, a tool for interactive topic model visualization.\n\n```bash\npip install topic-wizard\n```\n\n--------------------------------\n\n### Install Libraries with Pip\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/ideologies\n\nInstalls necessary Python libraries including Turftopic, Plotly, datasets, and pandas for data manipulation and visualization. These are essential for running the tutorial's code.\n\n```shell\npip install datasets plotly pandas turftopic\n\n```\n\n--------------------------------\n\n### Install datamapplot with Turftopic (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the datamapplot library along with Turftopic, enabling interactive cluster exploration.\n\n```bash\npip install turftopic[datamapplot]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/reviews\n\nInstalls the turftopic library with spaCy support, along with plotly and pandas for data handling and visualization. It also downloads a small English spaCy pipeline for phrase extraction.\n\n```shell\npip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install Turftopic using pip\n\nSource: https://x-tabdeveloping.github.io/turftopic/index\n\nThis command installs the Turftopic library from the Python Package Index (PyPI). Ensure you have pip installed and accessible in your environment.\n\n```bash\npip install turftopic\n\n```\n\n--------------------------------\n\n### FASTopic Model Initialization and Usage\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nThis section details how to install the turftopic library with the necessary dependencies for FASTopic, and provides a basic Python code example for initializing and fitting the FASTopic model to a corpus.\n\n```APIDOC",
            "codeBlocks": [
              {
                "language": "bash",
                "code": "pip install turftopic",
                "context": "https://x-tabdeveloping.github.io/turftopic/index\n\nThis command installs the Turftopic library from the Python Package Index (PyPI). Ensure you have pip installed and accessible in your environment."
              }
            ]
          },
          {
            "title": "FASTopic Model",
            "type": "other",
            "content": "### Description\nImplementation of the FASTopic model with a Turftopic API. The implementation is based on the original FASTopic package, but is adapted for optimal use in Turftopic. You will need to install torch to use this model.\n\n### Installation\n```bash\npip install turftopic[torch]",
            "codeBlocks": []
          },
          {
            "title": "OR:",
            "type": "other",
            "content": "pip install turftopic[pyro-ppl]\n```\n\n### Usage Example\n```python\nfrom turftopic import FASTopic\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = FASTopic(n_components=10).fit(corpus)\nmodel.print_topics()\n```\n\n### Parameters\n\n**`n_components`** (int) - Required - Number of topics. If you're using priors on the weight, feel free to overshoot with this value.\n\n**`encoder`** (Union[Encoder, str], default: `'sentence-transformers/all-MiniLM-L6-v2'`) - Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n\n**`vectorizer`** (Optional[CountVectorizer], default: `None`) - Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.\n\n**`random_state`** (Optional[int], default: `None`) - Random state to use so that results are exactly reproducible.\n\n**`batch_size`** (Optional[int], default: `None`)\n\n**`DT_alpha`** (float, default: `3.0`) - Sinkhorn alpha between document embeddings and topic embeddings.\n\n**`TW_alpha`** (float, default: `2.0`) - Sinkhorn alpha between topic embeddings and word embeddings.\n\n**`theta_temp`** (float, default: `1.0`) - Temperature parameter of used in softmax to compute topic probabilities in documents.\n\n**`n_epochs`** (int, default: `200`) - Number of epochs to train the model for.\n\n**`learning_rate`** (float, default: `0.002`) - Learning rate for the ADAM optimizer.\n\n**`device`** (str, default: `'cpu'`) - Device to run the model on. Defaults to CPU.\n\n### Source Code\nSource code in `turftopic/models/fastopic.py`\n\n```\n\n--------------------------------\n\n### Basic KeyNMF Topic Modeling with Turftopic and Scikit-learn\n\nSource: https://x-tabdeveloping.github.io/turftopic/index\n\nThis Python code demonstrates the basic usage of Turftopic's KeyNMF model. It fetches the 20Newsgroups dataset using scikit-learn, trains the KeyNMF model on the corpus, and then prints the identified topics. This example assumes familiarity with scikit-learn conventions.\n\n```python\nfrom turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()\n\n```\n\n--------------------------------\n\n### Install FASTopic with PyTorch Support\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nThis command installs the turftopic library with PyTorch support, which is a dependency for using the FASTopic model. Ensure you have pip installed.\n\n```bash\npip install turftopic[torch]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/arxiv_ml\n\nInstalls the `turftopic` library along with `datasets` and `plotly`. It also installs optional dependencies for UMAP-based clustering and datamapplot visualization. This command is executed in the terminal.\n\n```bash\npip install datasets plotly turftopic[umap-learn, datamapplot]\n```\n\n--------------------------------\n\n### Install turftopic with FASTopic support\n\nSource: https://x-tabdeveloping.github.io/turftopic/FASTopic\n\nTo use the FASTopic model, you need to install the turftopic library with the appropriate optional dependencies. You can choose between installing with PyTorch or Pyro-PP-L support. This command installs the library and its required packages for FASTopic.\n\n```bash\npip install turftopic[torch]",
            "codeBlocks": [
              {
                "language": "text",
                "code": "### Usage Example",
                "context": "pip install turftopic[pyro-ppl]"
              },
              {
                "language": "text",
                "code": "### Parameters\n\n**`n_components`** (int) - Required - Number of topics. If you're using priors on the weight, feel free to overshoot with this value.\n\n**`encoder`** (Union[Encoder, str], default: `'sentence-transformers/all-MiniLM-L6-v2'`) - Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n\n**`vectorizer`** (Optional[CountVectorizer], default: `None`) - Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.\n\n**`random_state`** (Optional[int], default: `None`) - Random state to use so that results are exactly reproducible.\n\n**`batch_size`** (Optional[int], default: `None`)\n\n**`DT_alpha`** (float, default: `3.0`) - Sinkhorn alpha between document embeddings and topic embeddings.\n\n**`TW_alpha`** (float, default: `2.0`) - Sinkhorn alpha between topic embeddings and word embeddings.\n\n**`theta_temp`** (float, default: `1.0`) - Temperature parameter of used in softmax to compute topic probabilities in documents.\n\n**`n_epochs`** (int, default: `200`) - Number of epochs to train the model for.\n\n**`learning_rate`** (float, default: `0.002`) - Learning rate for the ADAM optimizer.\n\n**`device`** (str, default: `'cpu'`) - Device to run the model on. Defaults to CPU.\n\n### Source Code\nSource code in `turftopic/models/fastopic.py`",
                "context": "\nmodel = FASTopic(n_components=10).fit(corpus)\nmodel.print_topics()"
              },
              {
                "language": "python",
                "code": "from turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()",
                "context": "It fetches the 20Newsgroups dataset using scikit-learn, trains the KeyNMF model on the corpus, and then prints the identified topics. This example assumes familiarity with scikit-learn conventions."
              },
              {
                "language": "bash",
                "code": "pip install turftopic[torch]",
                "context": "s://x-tabdeveloping.github.io/turftopic/FASTopic\n\nThis command installs the turftopic library with PyTorch support, which is a dependency for using the FASTopic model. Ensure you have pip installed."
              },
              {
                "language": "bash",
                "code": "pip install datasets plotly turftopic[umap-learn, datamapplot]",
                "context": "the `turftopic` library along with `datasets` and `plotly`. It also installs optional dependencies for UMAP-based clustering and datamapplot visualization. This command is executed in the terminal."
              }
            ]
          },
          {
            "title": "OR:",
            "type": "other",
            "content": "pip install turftopic[pyro-ppl]\n```\n\n--------------------------------\n\n### Install topic-wizard (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the topic-wizard library, a tool for interactive topic model visualization.\n\n```bash\npip install topic-wizard\n```\n\n--------------------------------\n\n### Install Libraries with Pip\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/ideologies\n\nInstalls necessary Python libraries including Turftopic, Plotly, datasets, and pandas for data manipulation and visualization. These are essential for running the tutorial's code.\n\n```shell\npip install datasets plotly pandas turftopic\n\n```\n\n--------------------------------\n\n### Install datamapplot with Turftopic (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the datamapplot library along with Turftopic, enabling interactive cluster exploration.\n\n```bash\npip install turftopic[datamapplot]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/reviews\n\nInstalls the turftopic library with spaCy support, along with plotly and pandas for data handling and visualization. It also downloads a small English spaCy pipeline for phrase extraction.\n\n```shell\npip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install topic-wizard (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the topic-wizard library, a tool for interactive topic model visualization.",
                "context": "pip install turftopic[pyro-ppl]"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install Libraries with Pip\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/ideologies\n\nInstalls necessary Python libraries including Turftopic, Plotly, datasets, and pandas for data manipulation and visualization. These are essential for running the tutorial's code.",
                "context": "\n```bash\npip install topic-wizard"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install datamapplot with Turftopic (Shell)\n\nSource: https://x-tabdeveloping.github.io/turftopic/model_interpretation\n\nCommand to install the datamapplot library along with Turftopic, enabling interactive cluster exploration.",
                "context": "\n```shell\npip install datasets plotly pandas turftopic"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://x-tabdeveloping.github.io/turftopic/tutorials/reviews\n\nInstalls the turftopic library with spaCy support, along with plotly and pandas for data handling and visualization. It also downloads a small English spaCy pipeline for phrase extraction.",
                "context": "\n```bash\npip install turftopic[datamapplot]"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/emmaguoguo33/test1",
        "packageName": "test1",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:39.258Z",
        "content": "### Create Pull Request with gh CLI\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nExample of using the GitHub CLI (`gh`) to create a pull request with a dynamic title and body containing a summary report.\n\n```Bash\ngh pr create --title \"Prepare release for Dart DART_VERSION / Flutter FLUTTER_VERSION\" --body \"<summary_report_content>\"\n```\n\n--------------------------------\n\n### Get Dart SDK Version\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to retrieve the current Flutter SDK version information in machine-readable JSON format, specifically to extract the Dart SDK version.\n\n```Bash\nflutter --version --machine\n```\n\n--------------------------------\n\n### Update Environment to Beta Channel\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to switch the local Git repository to the 'beta' branch and update the local Flutter SDK to the 'beta' channel.\n\n```Bash\ngit checkout beta\ngit pull origin beta\n```\n\n```Bash\nflutter channel beta\nflutter upgrade\n```\n\n--------------------------------\n\n### Run Dart Analysis and Formatting\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to perform static analysis on Dart code, ensuring it meets fatal info and warning criteria, and to format the code according to Dart standards.\n\n```Bash\ndart analyze --fatal-infos --fatal-warnings\n```\n\n```Bash\ndart format .\n```\n\n--------------------------------\n\n### Update pubspec.yaml SDK Constraint\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nDemonstrates how to read a project's pubspec.yaml file, find the 'environment.sdk' key, and update its value to a new Dart SDK version constraint.\n\n```Dart\n# Assuming pubspec.yaml is parsed into a Map<String, dynamic> called pubspec\n\nif (pubspec.containsKey('environment') && pubspec['environment'] is Map) {\n  var environment = pubspec['environment'] as Map<String, dynamic>;\n  environment['sdk'] = '^DART_VERSION-0'; // Replace DART_VERSION with the actual version\n}\n\n// Then save the modified pubspec content.\n```\n\n--------------------------------\n\n### Run Flutter Tests\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to execute all tests within a Flutter project, excluding specific projects like 'material_3_demo'. Assumes a 'test' directory exists.\n\n```Bash\nflutter test\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Create Pull Request with gh CLI\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nExample of using the GitHub CLI (`gh`) to create a pull request with a dynamic title and body containing a summary report.\n\n```Bash\ngh pr create --title \"Prepare release for Dart DART_VERSION / Flutter FLUTTER_VERSION\" --body \"<summary_report_content>\"\n```\n\n--------------------------------\n\n### Get Dart SDK Version\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to retrieve the current Flutter SDK version information in machine-readable JSON format, specifically to extract the Dart SDK version.\n\n```Bash\nflutter --version --machine\n```\n\n--------------------------------\n\n### Update Environment to Beta Channel\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to switch the local Git repository to the 'beta' branch and update the local Flutter SDK to the 'beta' channel.\n\n```Bash\ngit checkout beta\ngit pull origin beta\n```\n\n```Bash\nflutter channel beta\nflutter upgrade\n```\n\n--------------------------------\n\n### Run Dart Analysis and Formatting\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to perform static analysis on Dart code, ensuring it meets fatal info and warning criteria, and to format the code according to Dart standards.\n\n```Bash\ndart analyze --fatal-infos --fatal-warnings\n```\n\n```Bash\ndart format .\n```\n\n--------------------------------\n\n### Update pubspec.yaml SDK Constraint\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nDemonstrates how to read a project's pubspec.yaml file, find the 'environment.sdk' key, and update its value to a new Dart SDK version constraint.\n\n```Dart",
            "codeBlocks": [
              {
                "language": "Bash",
                "code": "gh pr create --title \"Prepare release for Dart DART_VERSION / Flutter FLUTTER_VERSION\" --body \"<summary_report_content>\"",
                "context": "Source: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nExample of using the GitHub CLI (`gh`) to create a pull request with a dynamic title and body containing a summary report."
              },
              {
                "language": "Bash",
                "code": "flutter --version --machine",
                "context": "://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to retrieve the current Flutter SDK version information in machine-readable JSON format, specifically to extract the Dart SDK version."
              },
              {
                "language": "Bash",
                "code": "git checkout beta\ngit pull origin beta",
                "context": "Source: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to switch the local Git repository to the 'beta' branch and update the local Flutter SDK to the 'beta' channel."
              },
              {
                "language": "Bash",
                "code": "flutter channel beta\nflutter upgrade",
                "context": "git checkout beta\ngit pull origin beta\n```"
              },
              {
                "language": "Bash",
                "code": "dart analyze --fatal-infos --fatal-warnings",
                "context": "com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommands to perform static analysis on Dart code, ensuring it meets fatal info and warning criteria, and to format the code according to Dart standards."
              },
              {
                "language": "Bash",
                "code": "dart format .",
                "context": "```Bash\ndart analyze --fatal-infos --fatal-warnings\n```"
              }
            ]
          },
          {
            "title": "Assuming pubspec.yaml is parsed into a Map<String, dynamic> called pubspec",
            "type": "other",
            "content": "if (pubspec.containsKey('environment') && pubspec['environment'] is Map) {\n  var environment = pubspec['environment'] as Map<String, dynamic>;\n  environment['sdk'] = '^DART_VERSION-0'; // Replace DART_VERSION with the actual version\n}\n\n// Then save the modified pubspec content.\n```\n\n--------------------------------\n\n### Run Flutter Tests\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to execute all tests within a Flutter project, excluding specific projects like 'material_3_demo'. Assumes a 'test' directory exists.\n\n```Bash\nflutter test\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Run Flutter Tests\n\nSource: https://github.com/emmaguoguo33/test1/blob/main/myprompt.md\n\nCommand to execute all tests within a Flutter project, excluding specific projects like 'material_3_demo'. Assumes a 'test' directory exists.",
                "context": "}\n\n// Then save the modified pubspec content."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/websites/rs_tokio-test",
        "packageName": "rs_tokio-test",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:42.905Z",
        "content": "### Assert Ready Ok Usage Example\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/macro\n\nDemonstrates usage of assert_ready_ok macro with a spawned future. Tests that a successful future poll returns Poll::Ready(Ok(..)). Requires futures_util and tokio_test crates.\n\n```Rust\nuse futures_util::future;\nuse tokio_test::{assert_ready_ok, task};\n\nlet mut fut = task::spawn(future::ok::<_, ()>(()));\nassert_ready_ok!(fut.poll());\n```\n\n--------------------------------\n\n### Create StreamMock with Items and Delays\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/stream_mock\n\nExample showing how to use StreamMockBuilder to create a stream that yields items with specific delays. Useful for testing async stream processing logic.\n\n```rust\nuse futures_util::StreamExt;\nuse std::time::Duration;\nuse tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}\n```\n\n--------------------------------\n\n### Poll Future with Spawn\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nThis snippet shows how to poll a future using the `Spawn` struct from the `tokio-test` crate. The Spawn struct allows easy polling of futures without needing to setup pinning or context.\n\n```Rust\nimpl<T: Future> Spawn<T> {\n    /// If `T` is a [`Future`] then poll it. This will handle pinning and the context\n    /// type for the future.\n    pub fn poll(&mut self) -> Poll<T::Output> {\n        let fut = self.future.as_mut();\n        self.task.enter(|cx| fut.poll(cx))\n    }\n}\n```\n\n--------------------------------\n\n### Assert Elapsed Duration in Rust\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/macros\n\nThe `assert_elapsed` macro asserts that a specified duration has elapsed since a given start instant, allowing for a 1ms buffer due to Tokio's timer resolution.  It is commonly used in asynchronous tests to verify time-based operations.\n\n```Rust\nmacro_rules! assert_elapsed {\n    ($start:expr, $dur:expr) => {{ \n        let elapsed = $start.elapsed();\n        // type ascription improves compiler error when wrong type is passed\n        let lower: std::time::Duration = $dur;\n\n        // Handles ms rounding\n        assert!(\n            elapsed >= lower && elapsed <= lower + std::time::Duration::from_millis(1),\n            \"actual = {:?}, expected = {:?}\",\n            elapsed,\n            lower\n        );\n    }};\n}\n```\n\n--------------------------------\n\n### Implement Mock Task and Thread Waker System\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nComplete Rust implementation of a mock task system with thread waker functionality for testing async code. Includes state management for wake notifications, thread synchronization using mutex and condition variables, and RawWaker VTable implementation for low-level waker operations.\n\n```rust\nuse std::sync::{Arc, Condvar, Mutex};\nuse std::task::{Context, RawWaker, RawWakerVTable, Waker};\nuse std::mem;\n\nconst IDLE: u8 = 0;\nconst WAKE: u8 = 1;\nconst SLEEP: u8 = 2;\n\nstruct MockTask {\n    waker: Arc<ThreadWaker>,\n}\n\nstruct ThreadWaker {\n    state: Mutex<u8>,\n    condvar: Condvar,\n}\n\nimpl MockTask {\n    fn new() -> Self {\n        Self {\n            waker: Arc::new(ThreadWaker::new()),\n        }\n    }\n\n    /// Runs the provided closure with a fresh context that references\n    /// this task's waker.\n    fn enter<F, R>(&self, f: F) -> R\n    where\n        F: FnMut(&mut Context<'_>) -> R,\n    {\n        self.waker.clear();\n        let waker = self.waker();\n        let mut cx = Context::from_waker(&waker);\n\n        f(&mut cx)\n    }\n\n    /// Returns `true` if the inner future has received a wake notification\n    /// since the last call to `enter`.\n    fn is_woken(&self) -> bool {\n        self.waker.is_woken()\n    }\n\n    /// Returns the number of references to the task waker\n    ///\n    /// The task itself holds a reference. The return value will never be zero.\n    fn waker_ref_count(&self) -> usize {\n        Arc::strong_count(&self.waker)\n    }\n\n    fn waker(&self) -> Waker {\n        unsafe {\n            let raw = to_raw(self.waker.clone());\n            Waker::from_raw(raw)\n        }\n    }\n}\n\nimpl Default for MockTask {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl ThreadWaker {\n    fn new() -> Self {\n        ThreadWaker {\n            state: Mutex::new(IDLE),\n            condvar: Condvar::new(),\n        }\n    }\n\n    /// Clears any previously received wakes, avoiding potential spurious\n    /// wake notifications. This should only be called immediately before running the\n    /// task.\n    fn clear(&self) {\n        *self.state.lock().unwrap() = IDLE;\n    }\n\n    fn is_woken(&self) -> bool {\n        match *self.state.lock().unwrap() {\n            IDLE => false,\n            WAKE => true,\n            _ => unreachable!(),\n        }\n    }\n\n    fn wake(&self) {\n        // First, try transitioning from IDLE -> NOTIFY, this does not require a lock.\n        let mut state = self.state.lock().unwrap();\n        let prev = *state;\n\n        if prev == WAKE {\n            return;\n        }\n\n        *state = WAKE;\n\n        if prev == IDLE {\n            return;\n        }\n\n        // The other half is sleeping, so we wake it up.\n        assert_eq!(prev, SLEEP);\n        self.condvar.notify_one();\n    }\n}\n\nstatic VTABLE: RawWakerVTable = RawWakerVTable::new(clone, wake, wake_by_ref, drop_waker);\n\nunsafe fn to_raw(waker: Arc<ThreadWaker>) -> RawWaker {\n    RawWaker::new(Arc::into_raw(waker) as *const (), &VTABLE)\n}\n\nunsafe fn from_raw(raw: *const ()) -> Arc<ThreadWaker> {\n    Arc::from_raw(raw as *const ThreadWaker)\n}\n\nunsafe fn clone(raw: *const ()) -> RawWaker {\n    let waker = from_raw(raw);\n\n    // Increment the ref count\n    mem::forget(waker.clone());\n\n    to_raw(waker)\n}\n\nunsafe fn wake(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n}\n\nunsafe fn wake_by_ref(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n\n    // We don't actually own a reference to the unparker\n    mem::forget(waker);\n}\n\nunsafe fn drop_waker(raw: *const ()) {\n    let _ = from_raw(raw);\n}\n```\n\n--------------------------------\n\n### Spawn and Poll a Future with tokio-test\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/task/index\n\nDemonstrates how to use the `tokio_test::task::spawn` function to create a mock task for a future and then poll it to check its readiness. This bypasses the need for manual pinning and context management.\n\n```rust\nuse tokio_test::task;\n\nlet fut = async {};\n\nlet mut task = task::spawn(fut);\n\nassert!(task.poll().is_ready(), \"Task was not ready!\");\n```\n\n--------------------------------\n\n### Poll Stream with Spawn\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nIllustrates how to use `Spawn` to poll a stream and retrieve the next item. Like futures, the `Spawn` struct abstracts away the need to manage pinning or context.\n\n```Rust\nimpl<T: Stream> Spawn<T> {\n    /// If `T` is a [`Stream`] then `poll_next` it. This will handle pinning and the context\n    /// type for the stream.\n    pub fn poll_next(&mut self) -> Poll<Option<T::Item>> {\n        let stream = self.future.as_mut();\n        self.task.enter(|cx| stream.poll_next(cx))\n    }\n}\n```\n\n--------------------------------\n\n### Create and test a mock stream using tokio-test StreamMock in Rust\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/stream_mock/index\n\nThis snippet shows how to construct a StreamMock with predefined items and wait intervals using StreamMockBuilder. It demonstrates asynchronous consumption of the mock stream and asserts timing and value expectations. No external dependencies beyond futures_util and std are required.\n\n```Rust\nuse futures_util::StreamExt;\nuse std::time::Duration;\n tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}\n```\n\n--------------------------------\n\n### Warnings and Documentation Attributes in Rust\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/lib\n\nThis code snippet sets compiler warnings and documentation attributes for the crate. It enforces debug implementations, documentation, Rust 2018 idioms, and unreachable public items. It also configures test attributes to deny warnings and allow dead code and unused variables.\n\n```rust\n#![warn(\n    missing_debug_implementations,\n    missing_docs,\n    rust_2018_idioms,\n    unreachable_pub\n)]\n#![doc(test(\n    no_crate_inject,\n    attr(deny(warnings, rust_2018_idioms), allow(dead_code, unused_variables))\n))]\n```\n\n--------------------------------\n\n### Create StreamMockBuilder in Rust\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/stream_mock/struct\n\nInitializes a new StreamMockBuilder instance. This builder is used to configure and create mock streams for testing asynchronous Rust code.\n\n```Rust\npub struct StreamMockBuilder<T: Unpin> { /* private fields */ }\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Assert Ready Ok Usage Example\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/macro\n\nDemonstrates usage of assert_ready_ok macro with a spawned future. Tests that a successful future poll returns Poll::Ready(Ok(..)). Requires futures_util and tokio_test crates.\n\n```Rust\nuse futures_util::future;\nuse tokio_test::{assert_ready_ok, task};\n\nlet mut fut = task::spawn(future::ok::<_, ()>(()));\nassert_ready_ok!(fut.poll());\n```\n\n--------------------------------\n\n### Create StreamMock with Items and Delays\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/stream_mock\n\nExample showing how to use StreamMockBuilder to create a stream that yields items with specific delays. Useful for testing async stream processing logic.\n\n```rust\nuse futures_util::StreamExt;\nuse std::time::Duration;\nuse tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}\n```\n\n--------------------------------\n\n### Poll Future with Spawn\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nThis snippet shows how to poll a future using the `Spawn` struct from the `tokio-test` crate. The Spawn struct allows easy polling of futures without needing to setup pinning or context.\n\n```Rust\nimpl<T: Future> Spawn<T> {\n    /// If `T` is a [`Future`] then poll it. This will handle pinning and the context\n    /// type for the future.\n    pub fn poll(&mut self) -> Poll<T::Output> {\n        let fut = self.future.as_mut();\n        self.task.enter(|cx| fut.poll(cx))\n    }\n}\n```\n\n--------------------------------\n\n### Assert Elapsed Duration in Rust\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/macros\n\nThe `assert_elapsed` macro asserts that a specified duration has elapsed since a given start instant, allowing for a 1ms buffer due to Tokio's timer resolution.  It is commonly used in asynchronous tests to verify time-based operations.\n\n```Rust\nmacro_rules! assert_elapsed {\n    ($start:expr, $dur:expr) => {{ \n        let elapsed = $start.elapsed();\n        // type ascription improves compiler error when wrong type is passed\n        let lower: std::time::Duration = $dur;\n\n        // Handles ms rounding\n        assert!(\n            elapsed >= lower && elapsed <= lower + std::time::Duration::from_millis(1),\n            \"actual = {:?}, expected = {:?}\",\n            elapsed,\n            lower\n        );\n    }};\n}\n```\n\n--------------------------------\n\n### Implement Mock Task and Thread Waker System\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nComplete Rust implementation of a mock task system with thread waker functionality for testing async code. Includes state management for wake notifications, thread synchronization using mutex and condition variables, and RawWaker VTable implementation for low-level waker operations.\n\n```rust\nuse std::sync::{Arc, Condvar, Mutex};\nuse std::task::{Context, RawWaker, RawWakerVTable, Waker};\nuse std::mem;\n\nconst IDLE: u8 = 0;\nconst WAKE: u8 = 1;\nconst SLEEP: u8 = 2;\n\nstruct MockTask {\n    waker: Arc<ThreadWaker>,\n}\n\nstruct ThreadWaker {\n    state: Mutex<u8>,\n    condvar: Condvar,\n}\n\nimpl MockTask {\n    fn new() -> Self {\n        Self {\n            waker: Arc::new(ThreadWaker::new()),\n        }\n    }\n\n    /// Runs the provided closure with a fresh context that references\n    /// this task's waker.\n    fn enter<F, R>(&self, f: F) -> R\n    where\n        F: FnMut(&mut Context<'_>) -> R,\n    {\n        self.waker.clear();\n        let waker = self.waker();\n        let mut cx = Context::from_waker(&waker);\n\n        f(&mut cx)\n    }\n\n    /// Returns `true` if the inner future has received a wake notification\n    /// since the last call to `enter`.\n    fn is_woken(&self) -> bool {\n        self.waker.is_woken()\n    }\n\n    /// Returns the number of references to the task waker\n    ///\n    /// The task itself holds a reference. The return value will never be zero.\n    fn waker_ref_count(&self) -> usize {\n        Arc::strong_count(&self.waker)\n    }\n\n    fn waker(&self) -> Waker {\n        unsafe {\n            let raw = to_raw(self.waker.clone());\n            Waker::from_raw(raw)\n        }\n    }\n}\n\nimpl Default for MockTask {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl ThreadWaker {\n    fn new() -> Self {\n        ThreadWaker {\n            state: Mutex::new(IDLE),\n            condvar: Condvar::new(),\n        }\n    }\n\n    /// Clears any previously received wakes, avoiding potential spurious\n    /// wake notifications. This should only be called immediately before running the\n    /// task.\n    fn clear(&self) {\n        *self.state.lock().unwrap() = IDLE;\n    }\n\n    fn is_woken(&self) -> bool {\n        match *self.state.lock().unwrap() {\n            IDLE => false,\n            WAKE => true,\n            _ => unreachable!(),\n        }\n    }\n\n    fn wake(&self) {\n        // First, try transitioning from IDLE -> NOTIFY, this does not require a lock.\n        let mut state = self.state.lock().unwrap();\n        let prev = *state;\n\n        if prev == WAKE {\n            return;\n        }\n\n        *state = WAKE;\n\n        if prev == IDLE {\n            return;\n        }\n\n        // The other half is sleeping, so we wake it up.\n        assert_eq!(prev, SLEEP);\n        self.condvar.notify_one();\n    }\n}\n\nstatic VTABLE: RawWakerVTable = RawWakerVTable::new(clone, wake, wake_by_ref, drop_waker);\n\nunsafe fn to_raw(waker: Arc<ThreadWaker>) -> RawWaker {\n    RawWaker::new(Arc::into_raw(waker) as *const (), &VTABLE)\n}\n\nunsafe fn from_raw(raw: *const ()) -> Arc<ThreadWaker> {\n    Arc::from_raw(raw as *const ThreadWaker)\n}\n\nunsafe fn clone(raw: *const ()) -> RawWaker {\n    let waker = from_raw(raw);\n\n    // Increment the ref count\n    mem::forget(waker.clone());\n\n    to_raw(waker)\n}\n\nunsafe fn wake(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n}\n\nunsafe fn wake_by_ref(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n\n    // We don't actually own a reference to the unparker\n    mem::forget(waker);\n}\n\nunsafe fn drop_waker(raw: *const ()) {\n    let _ = from_raw(raw);\n}\n```\n\n--------------------------------\n\n### Spawn and Poll a Future with tokio-test\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/task/index\n\nDemonstrates how to use the `tokio_test::task::spawn` function to create a mock task for a future and then poll it to check its readiness. This bypasses the need for manual pinning and context management.\n\n```rust\nuse tokio_test::task;\n\nlet fut = async {};\n\nlet mut task = task::spawn(fut);\n\nassert!(task.poll().is_ready(), \"Task was not ready!\");\n```\n\n--------------------------------\n\n### Poll Stream with Spawn\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/task\n\nIllustrates how to use `Spawn` to poll a stream and retrieve the next item. Like futures, the `Spawn` struct abstracts away the need to manage pinning or context.\n\n```Rust\nimpl<T: Stream> Spawn<T> {\n    /// If `T` is a [`Stream`] then `poll_next` it. This will handle pinning and the context\n    /// type for the stream.\n    pub fn poll_next(&mut self) -> Poll<Option<T::Item>> {\n        let stream = self.future.as_mut();\n        self.task.enter(|cx| stream.poll_next(cx))\n    }\n}\n```\n\n--------------------------------\n\n### Create and test a mock stream using tokio-test StreamMock in Rust\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/stream_mock/index\n\nThis snippet shows how to construct a StreamMock with predefined items and wait intervals using StreamMockBuilder. It demonstrates asynchronous consumption of the mock stream and asserts timing and value expectations. No external dependencies beyond futures_util and std are required.\n\n```Rust\nuse futures_util::StreamExt;\nuse std::time::Duration;\n tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}\n```\n\n--------------------------------\n\n### Warnings and Documentation Attributes in Rust\n\nSource: https://docs.rs/tokio-test/latest/src/tokio_test/lib\n\nThis code snippet sets compiler warnings and documentation attributes for the crate. It enforces debug implementations, documentation, Rust 2018 idioms, and unreachable public items. It also configures test attributes to deny warnings and allow dead code and unused variables.\n\n```rust\n#![warn(\n    missing_debug_implementations,\n    missing_docs,\n    rust_2018_idioms,\n    unreachable_pub\n)]\n#![doc(test(\n    no_crate_inject,\n    attr(deny(warnings, rust_2018_idioms), allow(dead_code, unused_variables))\n))]\n```\n\n--------------------------------\n\n### Create StreamMockBuilder in Rust\n\nSource: https://docs.rs/tokio-test/latest/tokio_test/stream_mock/struct\n\nInitializes a new StreamMockBuilder instance. This builder is used to configure and create mock streams for testing asynchronous Rust code.\n\n```Rust\npub struct StreamMockBuilder<T: Unpin> { /* private fields */ }\n```",
            "codeBlocks": [
              {
                "language": "Rust",
                "code": "use futures_util::future;\nuse tokio_test::{assert_ready_ok, task};\n\nlet mut fut = task::spawn(future::ok::<_, ()>(()));\nassert_ready_ok!(fut.poll());",
                "context": "est/tokio_test/macro\n\nDemonstrates usage of assert_ready_ok macro with a spawned future. Tests that a successful future poll returns Poll::Ready(Ok(..)). Requires futures_util and tokio_test crates."
              },
              {
                "language": "rust",
                "code": "use futures_util::StreamExt;\nuse std::time::Duration;\nuse tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}",
                "context": "okio-test/latest/src/tokio_test/stream_mock\n\nExample showing how to use StreamMockBuilder to create a stream that yields items with specific delays. Useful for testing async stream processing logic."
              },
              {
                "language": "Rust",
                "code": "impl<T: Future> Spawn<T> {\n    /// If `T` is a [`Future`] then poll it. This will handle pinning and the context\n    /// type for the future.\n    pub fn poll(&mut self) -> Poll<T::Output> {\n        let fut = self.future.as_mut();\n        self.task.enter(|cx| fut.poll(cx))\n    }\n}",
                "context": "_test/task\n\nThis snippet shows how to poll a future using the `Spawn` struct from the `tokio-test` crate. The Spawn struct allows easy polling of futures without needing to setup pinning or context."
              },
              {
                "language": "Rust",
                "code": "macro_rules! assert_elapsed {\n    ($start:expr, $dur:expr) => {{ \n        let elapsed = $start.elapsed();\n        // type ascription improves compiler error when wrong type is passed\n        let lower: std::time::Duration = $dur;\n\n        // Handles ms rounding\n        assert!(\n            elapsed >= lower && elapsed <= lower + std::time::Duration::from_millis(1),\n            \"actual = {:?}, expected = {:?}\",\n            elapsed,\n            lower\n        );\n    }};\n}",
                "context": "t a specified duration has elapsed since a given start instant, allowing for a 1ms buffer due to Tokio's timer resolution.  It is commonly used in asynchronous tests to verify time-based operations."
              },
              {
                "language": "rust",
                "code": "use std::sync::{Arc, Condvar, Mutex};\nuse std::task::{Context, RawWaker, RawWakerVTable, Waker};\nuse std::mem;\n\nconst IDLE: u8 = 0;\nconst WAKE: u8 = 1;\nconst SLEEP: u8 = 2;\n\nstruct MockTask {\n    waker: Arc<ThreadWaker>,\n}\n\nstruct ThreadWaker {\n    state: Mutex<u8>,\n    condvar: Condvar,\n}\n\nimpl MockTask {\n    fn new() -> Self {\n        Self {\n            waker: Arc::new(ThreadWaker::new()),\n        }\n    }\n\n    /// Runs the provided closure with a fresh context that references\n    /// this task's waker.\n    fn enter<F, R>(&self, f: F) -> R\n    where\n        F: FnMut(&mut Context<'_>) -> R,\n    {\n        self.waker.clear();\n        let waker = self.waker();\n        let mut cx = Context::from_waker(&waker);\n\n        f(&mut cx)\n    }\n\n    /// Returns `true` if the inner future has received a wake notification\n    /// since the last call to `enter`.\n    fn is_woken(&self) -> bool {\n        self.waker.is_woken()\n    }\n\n    /// Returns the number of references to the task waker\n    ///\n    /// The task itself holds a reference. The return value will never be zero.\n    fn waker_ref_count(&self) -> usize {\n        Arc::strong_count(&self.waker)\n    }\n\n    fn waker(&self) -> Waker {\n        unsafe {\n            let raw = to_raw(self.waker.clone());\n            Waker::from_raw(raw)\n        }\n    }\n}\n\nimpl Default for MockTask {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl ThreadWaker {\n    fn new() -> Self {\n        ThreadWaker {\n            state: Mutex::new(IDLE),\n            condvar: Condvar::new(),\n        }\n    }\n\n    /// Clears any previously received wakes, avoiding potential spurious\n    /// wake notifications. This should only be called immediately before running the\n    /// task.\n    fn clear(&self) {\n        *self.state.lock().unwrap() = IDLE;\n    }\n\n    fn is_woken(&self) -> bool {\n        match *self.state.lock().unwrap() {\n            IDLE => false,\n            WAKE => true,\n            _ => unreachable!(),\n        }\n    }\n\n    fn wake(&self) {\n        // First, try transitioning from IDLE -> NOTIFY, this does not require a lock.\n        let mut state = self.state.lock().unwrap();\n        let prev = *state;\n\n        if prev == WAKE {\n            return;\n        }\n\n        *state = WAKE;\n\n        if prev == IDLE {\n            return;\n        }\n\n        // The other half is sleeping, so we wake it up.\n        assert_eq!(prev, SLEEP);\n        self.condvar.notify_one();\n    }\n}\n\nstatic VTABLE: RawWakerVTable = RawWakerVTable::new(clone, wake, wake_by_ref, drop_waker);\n\nunsafe fn to_raw(waker: Arc<ThreadWaker>) -> RawWaker {\n    RawWaker::new(Arc::into_raw(waker) as *const (), &VTABLE)\n}\n\nunsafe fn from_raw(raw: *const ()) -> Arc<ThreadWaker> {\n    Arc::from_raw(raw as *const ThreadWaker)\n}\n\nunsafe fn clone(raw: *const ()) -> RawWaker {\n    let waker = from_raw(raw);\n\n    // Increment the ref count\n    mem::forget(waker.clone());\n\n    to_raw(waker)\n}\n\nunsafe fn wake(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n}\n\nunsafe fn wake_by_ref(raw: *const ()) {\n    let waker = from_raw(raw);\n    waker.wake();\n\n    // We don't actually own a reference to the unparker\n    mem::forget(waker);\n}\n\nunsafe fn drop_waker(raw: *const ()) {\n    let _ = from_raw(raw);\n}",
                "context": "r testing async code. Includes state management for wake notifications, thread synchronization using mutex and condition variables, and RawWaker VTable implementation for low-level waker operations."
              },
              {
                "language": "rust",
                "code": "use tokio_test::task;\n\nlet fut = async {};\n\nlet mut task = task::spawn(fut);\n\nassert!(task.poll().is_ready(), \"Task was not ready!\");",
                "context": "trates how to use the `tokio_test::task::spawn` function to create a mock task for a future and then poll it to check its readiness. This bypasses the need for manual pinning and context management."
              },
              {
                "language": "Rust",
                "code": "impl<T: Stream> Spawn<T> {\n    /// If `T` is a [`Stream`] then `poll_next` it. This will handle pinning and the context\n    /// type for the stream.\n    pub fn poll_next(&mut self) -> Poll<Option<T::Item>> {\n        let stream = self.future.as_mut();\n        self.task.enter(|cx| stream.poll_next(cx))\n    }\n}",
                "context": "io-test/latest/src/tokio_test/task\n\nIllustrates how to use `Spawn` to poll a stream and retrieve the next item. Like futures, the `Spawn` struct abstracts away the need to manage pinning or context."
              },
              {
                "language": "Rust",
                "code": "use futures_util::StreamExt;\nuse std::time::Duration;\n tokio_test::stream_mock::StreamMockBuilder;\n\nasync fn test_stream_mock_wait() {\n    let mut stream_mock = StreamMockBuilder::new()\n        .next(1)\n        .wait(Duration::from_millis(300))\n        .next(2)\n        .build();\n\n    assert_eq!(stream_mock.next().await, Some(1));\n    let start = std::time::Instant::now();\n    assert_eq!(stream_mock.next().await, Some(2));\n    let elapsed = start.elapsed();\n    assert!(elapsed >= Duration::from_millis(300));\n    assert_eq!(stream_mock.next().await, None);\n}",
                "context": "als using StreamMockBuilder. It demonstrates asynchronous consumption of the mock stream and asserts timing and value expectations. No external dependencies beyond futures_util and std are required."
              },
              {
                "language": "rust",
                "code": "#![warn(\n    missing_debug_implementations,\n    missing_docs,\n    rust_2018_idioms,\n    unreachable_pub\n)]\n#![doc(test(\n    no_crate_inject,\n    attr(deny(warnings, rust_2018_idioms), allow(dead_code, unused_variables))\n))]",
                "context": "crate. It enforces debug implementations, documentation, Rust 2018 idioms, and unreachable public items. It also configures test attributes to deny warnings and allow dead code and unused variables."
              },
              {
                "language": "Rust",
                "code": "pub struct StreamMockBuilder<T: Unpin> { /* private fields */ }",
                "context": "//docs.rs/tokio-test/latest/tokio_test/stream_mock/struct\n\nInitializes a new StreamMockBuilder instance. This builder is used to configure and create mock streams for testing asynchronous Rust code."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/yandex/yandex-taxi-testsuite",
        "packageName": "yandex-taxi-testsuite",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:46.377Z",
        "content": "### Python Example Web Service\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet shows a basic aiohttp web application that handles GET requests, demonstrating a simple 'Hello, World' type service.\n\n```Python\nimport asyncio\nfrom aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\napp = web.Application()\napp.router.add_get('/', handle)\napp.router.add_get('/{name}', handle)\n\nif __name__ == '__main__':\n    web.run_app(app, port=8080)\n```\n\n--------------------------------\n\n### Example Service Implementation (server.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file demonstrates a basic aiohttp web service that could serve as the chat storage backend. It defines a simple GET endpoint.\n\n```Python\nimport aiohttp.web\n\nasync def handle(request):\n    return aiohttp.web.Response(text=\"Hello, world\")\n\napp = aiohttp.web.Application()\napp.router.add_get('/', handle)\n\nif __name__ == '__main__':\n    aiohttp.web.run_app(app)\n```\n\n--------------------------------\n\n### Example Web Service Implementation (Python)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mongo/README.rst\n\nThis Python code defines a simple web application using aiohttp, handling basic GET requests. It serves as the core service for the chat storage backend example, demonstrating how to set up routes and handle requests.\n\n```Python\n# server.py\nfrom aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\napp = web.Application()\napp.router.add_get('/', handle)\napp.router.add_get('/{name}', handle)\n\nif __name__ == '__main__':\n    web.run_app(app, port=8080)\n```\n\n--------------------------------\n\n### Pytest Service Integration Tests\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet contains integration tests for the example aiohttp service, verifying its GET endpoints and response content using the aiohttp test client.\n\n```Python\nasync def test_hello(cli):\n    resp = await cli.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, Anonymous' in text\n\nasync def test_hello_name(cli):\n    resp = await cli.get('/World')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, World' in text\n```\n\n--------------------------------\n\n### Run tests for example services\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/README.rst\n\nCommands to navigate to the examples directory and run tests using `make`.\n\n```sh\ncd docs/examples && make\n```\n\n--------------------------------\n\n### Run all testsuite examples\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/README.rst\n\nCommands to execute all integration tests for the chat application examples, both locally using system pytest and within Docker containers for a consistent environment.\n\n```Shell\nmake runtests\n```\n\n```Shell\nmake docker-runtests\n```\n\n--------------------------------\n\n### Pytest Configuration and Fixtures (Python)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mongo/README.rst\n\nThis conftest.py file provides essential pytest fixtures for testing the aiohttp web application. It includes a client fixture for making requests to the app and a base test case class, simplifying test setup and execution.\n\n```Python\n# tests/conftest.py\nimport pytest\nfrom aiohttp.test_utils import TestClient, AioHTTPTestCase\nfrom server import app\n\n@pytest.fixture\nasync def cli(aiohttp_client):\n    return await aiohttp_client(app)\n\nclass MyTestCase(AioHTTPTestCase):\n    async def get_application(self):\n        return app\n```\n\n--------------------------------\n\n### Pytest Conftest Fixtures (tests/conftest.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis conftest.py file provides pytest fixtures for testing the example service. It sets up an aiohttp test client using the application defined in server.py.\n\n```Python\nimport pytest\n\n@pytest.fixture\ndef client(aiohttp_client):\n    from server import app\n    return aiohttp_client(app)\n```\n\n--------------------------------\n\n### Service Test Case (tests/test_service.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file contains a pytest asynchronous test case for the example service. It uses the 'client' fixture to make a GET request and asserts the response status and content.\n\n```Python\nasync def test_hello(client):\n    resp = await client.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, world' in text\n```\n\n--------------------------------\n\n### Python: Custom Data Setup for redis_store using pytest.mark.redis_store\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/redis.rst\n\nThis example shows how to use the `pytest.mark.redis_store` decorator to pre-populate the `redis_store` fixture with specific data before a test runs. It supports providing a list of Redis commands or specifying a file to load data from.\n\n```python\nimport pytest\n\npytest_plugins = [\n    'testsuite.pytest_plugin',\n    'testsuite.databases.redis.pytest_plugin',\n]\n\n@pytest.mark.redis_store(\n    ['set', 'foo', 'bar'],\n    ['hset', 'baz', 'quux', 'bat'],\n)\ndef test_redis_marker_store(redis_store):\n    assert redis_store.get('foo') == b'bar'\n    assert redis_store.hgetall('baz') == {b'quux': b'bat'}\n\n@pytest.mark.redis_store(file='use_redis_store_file')\ndef test_redis_store_file(redis_store):\n    assert redis_store.get('foo') == b'store'\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Python Example Web Service\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet shows a basic aiohttp web application that handles GET requests, demonstrating a simple 'Hello, World' type service.\n\n```Python\nimport asyncio\nfrom aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\napp = web.Application()\napp.router.add_get('/', handle)\napp.router.add_get('/{name}', handle)\n\nif __name__ == '__main__':\n    web.run_app(app, port=8080)\n```\n\n--------------------------------\n\n### Example Service Implementation (server.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file demonstrates a basic aiohttp web service that could serve as the chat storage backend. It defines a simple GET endpoint.\n\n```Python\nimport aiohttp.web\n\nasync def handle(request):\n    return aiohttp.web.Response(text=\"Hello, world\")\n\napp = aiohttp.web.Application()\napp.router.add_get('/', handle)\n\nif __name__ == '__main__':\n    aiohttp.web.run_app(app)\n```\n\n--------------------------------\n\n### Example Web Service Implementation (Python)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mongo/README.rst\n\nThis Python code defines a simple web application using aiohttp, handling basic GET requests. It serves as the core service for the chat storage backend example, demonstrating how to set up routes and handle requests.\n\n```Python",
            "codeBlocks": [
              {
                "language": "Python",
                "code": "import asyncio\nfrom aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\napp = web.Application()\napp.router.add_get('/', handle)\napp.router.add_get('/{name}', handle)\n\nif __name__ == '__main__':\n    web.run_app(app, port=8080)",
                "context": "stsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet shows a basic aiohttp web application that handles GET requests, demonstrating a simple 'Hello, World' type service."
              },
              {
                "language": "Python",
                "code": "import aiohttp.web\n\nasync def handle(request):\n    return aiohttp.web.Response(text=\"Hello, world\")\n\napp = aiohttp.web.Application()\napp.router.add_get('/', handle)\n\nif __name__ == '__main__':\n    aiohttp.web.run_app(app)",
                "context": "te/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file demonstrates a basic aiohttp web service that could serve as the chat storage backend. It defines a simple GET endpoint."
              }
            ]
          },
          {
            "title": "server.py",
            "type": "other",
            "content": "from aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\napp = web.Application()\napp.router.add_get('/', handle)\napp.router.add_get('/{name}', handle)\n\nif __name__ == '__main__':\n    web.run_app(app, port=8080)\n```\n\n--------------------------------\n\n### Pytest Service Integration Tests\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet contains integration tests for the example aiohttp service, verifying its GET endpoints and response content using the aiohttp test client.\n\n```Python\nasync def test_hello(cli):\n    resp = await cli.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, Anonymous' in text\n\nasync def test_hello_name(cli):\n    resp = await cli.get('/World')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, World' in text\n```\n\n--------------------------------\n\n### Run tests for example services\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/README.rst\n\nCommands to navigate to the examples directory and run tests using `make`.\n\n```sh\ncd docs/examples && make\n```\n\n--------------------------------\n\n### Run all testsuite examples\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/README.rst\n\nCommands to execute all integration tests for the chat application examples, both locally using system pytest and within Docker containers for a consistent environment.\n\n```Shell\nmake runtests\n```\n\n```Shell\nmake docker-runtests\n```\n\n--------------------------------\n\n### Pytest Configuration and Fixtures (Python)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mongo/README.rst\n\nThis conftest.py file provides essential pytest fixtures for testing the aiohttp web application. It includes a client fixture for making requests to the app and a base test case class, simplifying test setup and execution.\n\n```Python",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Pytest Service Integration Tests\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-postgres/README.rst\n\nThis snippet contains integration tests for the example aiohttp service, verifying its GET endpoints and response content using the aiohttp test client.",
                "context": "\nif __name__ == '__main__':\n    web.run_app(app, port=8080)"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Run tests for example services\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/README.rst\n\nCommands to navigate to the examples directory and run tests using `make`.",
                "context": "    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, World' in text"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Run all testsuite examples\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/README.rst\n\nCommands to execute all integration tests for the chat application examples, both locally using system pytest and within Docker containers for a consistent environment.",
                "context": "\n```sh\ncd docs/examples && make"
              },
              {
                "language": "text",
                "code": "",
                "context": "\n```Shell\nmake runtests"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Pytest Configuration and Fixtures (Python)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mongo/README.rst\n\nThis conftest.py file provides essential pytest fixtures for testing the aiohttp web application. It includes a client fixture for making requests to the app and a base test case class, simplifying test setup and execution.",
                "context": "\n```Shell\nmake docker-runtests"
              }
            ]
          },
          {
            "title": "tests/conftest.py",
            "type": "other",
            "content": "import pytest\nfrom aiohttp.test_utils import TestClient, AioHTTPTestCase\nfrom server import app\n\n@pytest.fixture\nasync def cli(aiohttp_client):\n    return await aiohttp_client(app)\n\nclass MyTestCase(AioHTTPTestCase):\n    async def get_application(self):\n        return app\n```\n\n--------------------------------\n\n### Pytest Conftest Fixtures (tests/conftest.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis conftest.py file provides pytest fixtures for testing the example service. It sets up an aiohttp test client using the application defined in server.py.\n\n```Python\nimport pytest\n\n@pytest.fixture\ndef client(aiohttp_client):\n    from server import app\n    return aiohttp_client(app)\n```\n\n--------------------------------\n\n### Service Test Case (tests/test_service.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file contains a pytest asynchronous test case for the example service. It uses the 'client' fixture to make a GET request and asserts the response status and content.\n\n```Python\nasync def test_hello(client):\n    resp = await client.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, world' in text\n```\n\n--------------------------------\n\n### Python: Custom Data Setup for redis_store using pytest.mark.redis_store\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/redis.rst\n\nThis example shows how to use the `pytest.mark.redis_store` decorator to pre-populate the `redis_store` fixture with specific data before a test runs. It supports providing a list of Redis commands or specifying a file to load data from.\n\n```python\nimport pytest\n\npytest_plugins = [\n    'testsuite.pytest_plugin',\n    'testsuite.databases.redis.pytest_plugin',\n]\n\n@pytest.mark.redis_store(\n    ['set', 'foo', 'bar'],\n    ['hset', 'baz', 'quux', 'bat'],\n)\ndef test_redis_marker_store(redis_store):\n    assert redis_store.get('foo') == b'bar'\n    assert redis_store.hgetall('baz') == {b'quux': b'bat'}\n\n@pytest.mark.redis_store(file='use_redis_store_file')\ndef test_redis_store_file(redis_store):\n    assert redis_store.get('foo') == b'store'\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Pytest Conftest Fixtures (tests/conftest.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis conftest.py file provides pytest fixtures for testing the example service. It sets up an aiohttp test client using the application defined in server.py.",
                "context": "class MyTestCase(AioHTTPTestCase):\n    async def get_application(self):\n        return app"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Service Test Case (tests/test_service.py)\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/examples/chat-storage-mysql/README.rst\n\nThis Python file contains a pytest asynchronous test case for the example service. It uses the 'client' fixture to make a GET request and asserts the response status and content.",
                "context": "def client(aiohttp_client):\n    from server import app\n    return aiohttp_client(app)"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Python: Custom Data Setup for redis_store using pytest.mark.redis_store\n\nSource: https://github.com/yandex/yandex-taxi-testsuite/blob/develop/docs/redis.rst\n\nThis example shows how to use the `pytest.mark.redis_store` decorator to pre-populate the `redis_store` fixture with specific data before a test runs. It supports providing a list of Redis commands or specifying a file to load data from.",
                "context": "    assert resp.status == 200\n    text = await resp.text()\n    assert 'Hello, world' in text"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/tibastral/program-test-boilerplate",
        "packageName": "program-test-boilerplate",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:49.845Z",
        "content": "### Start Lamdera Development Server for Visual Testing\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis command initiates the Lamdera development server, which is crucial for visually observing program-tests in action. Once the server is running, you can access the test runner in your browser to watch tests step-by-step and debug layout or rendering issues interactively.\n\n```Shell\nlamdera live\n```\n\n--------------------------------\n\n### Example End-to-End Test in Elm using Program-Test\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis Elm code snippet provides a typical example of an end-to-end test using the program-test framework. It demonstrates how to simulate user interactions like inputting text into a DOM element and clicking a button, followed by asserting the presence of specific text content in the rendered view. The test starts a frontend session and defines a sequence of client actions.\n\n```Elm\nhelloWorldTest : TF.EndToEndTest ToBackend FrontendMsg FrontendModel ToFrontend BackendMsg BackendModel\nhelloWorldTest =\n    TF.start \"Test Hello World\"\n        (Time.millisToPosix 0)\n        config\n        [ TF.connectFrontend\n            100\n            (sessionIdFromString \"session1\")\n            \"/\"\n            { width = 900, height = 800 }\n            (\\client ->\n                [ client.input 100 (Dom.id \"best-framework\") \"react\"\n                , client.click 100 (Dom.id \"save-the-world\")\n                , client.checkView\n                    100\n                    (Test.Html.Query.has\n                        [ Test.Html.Selector.text \"Thank you, Mario, but the slowness is in another framework\"\n                        , Test.Html.Selector.text \"Take that, react\"\n                        ]\n                    )\n                ]\n            )\n        ]\n```\n\n--------------------------------\n\n### Run Lamdera Program-Tests with elm-test-rs\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis shell command executes end-to-end tests written with program-test using the `elm-test-rs` runner. It specifies the Lamdera compiler to ensure compatibility with the Lamdera ecosystem. This is the standard way to run your program-tests from the command line.\n\n```Shell\nelm-test-rs --compiler lamdera\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Start Lamdera Development Server for Visual Testing\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis command initiates the Lamdera development server, which is crucial for visually observing program-tests in action. Once the server is running, you can access the test runner in your browser to watch tests step-by-step and debug layout or rendering issues interactively.\n\n```Shell\nlamdera live\n```\n\n--------------------------------\n\n### Example End-to-End Test in Elm using Program-Test\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis Elm code snippet provides a typical example of an end-to-end test using the program-test framework. It demonstrates how to simulate user interactions like inputting text into a DOM element and clicking a button, followed by asserting the presence of specific text content in the rendered view. The test starts a frontend session and defines a sequence of client actions.\n\n```Elm\nhelloWorldTest : TF.EndToEndTest ToBackend FrontendMsg FrontendModel ToFrontend BackendMsg BackendModel\nhelloWorldTest =\n    TF.start \"Test Hello World\"\n        (Time.millisToPosix 0)\n        config\n        [ TF.connectFrontend\n            100\n            (sessionIdFromString \"session1\")\n            \"/\"\n            { width = 900, height = 800 }\n            (\\client ->\n                [ client.input 100 (Dom.id \"best-framework\") \"react\"\n                , client.click 100 (Dom.id \"save-the-world\")\n                , client.checkView\n                    100\n                    (Test.Html.Query.has\n                        [ Test.Html.Selector.text \"Thank you, Mario, but the slowness is in another framework\"\n                        , Test.Html.Selector.text \"Take that, react\"\n                        ]\n                    )\n                ]\n            )\n        ]\n```\n\n--------------------------------\n\n### Run Lamdera Program-Tests with elm-test-rs\n\nSource: https://github.com/tibastral/program-test-boilerplate/blob/main/README.md\n\nThis shell command executes end-to-end tests written with program-test using the `elm-test-rs` runner. It specifies the Lamdera compiler to ensure compatibility with the Lamdera ecosystem. This is the standard way to run your program-tests from the command line.\n\n```Shell\nelm-test-rs --compiler lamdera\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
            "codeBlocks": [
              {
                "language": "Shell",
                "code": "lamdera live",
                "context": "visually observing program-tests in action. Once the server is running, you can access the test runner in your browser to watch tests step-by-step and debug layout or rendering issues interactively."
              },
              {
                "language": "Elm",
                "code": "helloWorldTest : TF.EndToEndTest ToBackend FrontendMsg FrontendModel ToFrontend BackendMsg BackendModel\nhelloWorldTest =\n    TF.start \"Test Hello World\"\n        (Time.millisToPosix 0)\n        config\n        [ TF.connectFrontend\n            100\n            (sessionIdFromString \"session1\")\n            \"/\"\n            { width = 900, height = 800 }\n            (\\client ->\n                [ client.input 100 (Dom.id \"best-framework\") \"react\"\n                , client.click 100 (Dom.id \"save-the-world\")\n                , client.checkView\n                    100\n                    (Test.Html.Query.has\n                        [ Test.Html.Selector.text \"Thank you, Mario, but the slowness is in another framework\"\n                        , Test.Html.Selector.text \"Take that, react\"\n                        ]\n                    )\n                ]\n            )\n        ]",
                "context": "to a DOM element and clicking a button, followed by asserting the presence of specific text content in the rendered view. The test starts a frontend session and defines a sequence of client actions."
              },
              {
                "language": "Shell",
                "code": "elm-test-rs --compiler lamdera",
                "context": "m-test using the `elm-test-rs` runner. It specifies the Lamdera compiler to ensure compatibility with the Lamdera ecosystem. This is the standard way to run your program-tests from the command line."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/mwgmorningwood/context7-test",
        "packageName": "context7-test",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:53.477Z",
        "content": "### Environment File Setup (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to copy the example environment file and instructions on how to update it.\n\n```bash\ncp .env.example .env\n```\n\n--------------------------------\n\n### Install Dependencies (npm)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using npm.\n\n```bash\nnpm install\n```\n\n--------------------------------\n\n### Install Dependencies (yarn)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using yarn.\n\n```bash\nyarn install\n```\n\n--------------------------------\n\n### Start Development Server (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to start the development server.\n\n```bash\nnpm run dev\n```\n\n--------------------------------\n\n### Example .env Configuration (env)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nAn example of the .env file content, showing configuration for database, API, authentication, external services, and feature flags.\n\n```env\n# Database Configuration\nDATABASE_URL=postgresql://username:password@localhost:5432/context7_test\n\n# API Configuration\nAPI_PORT=3000\nAPI_HOST=localhost\n\n# Authentication\nJWT_SECRET=your-super-secret-jwt-key\nSESSION_SECRET=your-session-secret\n\n# External Services\nREDIS_URL=redis://localhost:6379\nMONGODB_URL=mongodb://localhost:27017/context7\n\n# Feature Flags\nENABLE_ANALYTICS=true\nENABLE_DEBUG_MODE=false\n```\n\n--------------------------------\n\n### Verify Installations (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to check if Node.js, npm, Git, and Docker are installed and their versions.\n\n```bash\nnode --version\nnpm --version\ngit --version\ndocker --version\n```\n\n--------------------------------\n\n### Start PostgreSQL (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to start the PostgreSQL service, depending on the operating system (macOS with Homebrew or Linux systemd).\n\n```bash\nbrew services start postgresql\n# or\nsudo systemctl start postgresql\n```\n\n--------------------------------\n\n### Setup Development Environment with npm\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/tutorials/building-your-first-app.md\n\nInitializes a new Node.js project and installs core Context7 dependencies and development tools.\n\n```bash\nmkdir my-context7-app\ncd my-context7-app\nnpm init -y\nnpm install @context7/core @context7/auth @context7/ui\nnpm install --save-dev @context7/dev-tools typescript @types/node\n```\n\n--------------------------------\n\n### Database Migration (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to run database migrations using npm.\n\n```bash\nnpm run db:migrate\n```\n\n--------------------------------\n\n### Run Test Suite (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to execute the entire test suite using npm.\n\n```bash\nnpm test\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Environment File Setup (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to copy the example environment file and instructions on how to update it.\n\n```bash\ncp .env.example .env\n```\n\n--------------------------------\n\n### Install Dependencies (npm)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using npm.\n\n```bash\nnpm install\n```\n\n--------------------------------\n\n### Install Dependencies (yarn)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using yarn.\n\n```bash\nyarn install\n```\n\n--------------------------------\n\n### Start Development Server (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to start the development server.\n\n```bash\nnpm run dev\n```\n\n--------------------------------\n\n### Example .env Configuration (env)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nAn example of the .env file content, showing configuration for database, API, authentication, external services, and feature flags.\n\n```env",
            "codeBlocks": [
              {
                "language": "bash",
                "code": "cp .env.example .env",
                "context": "Source: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to copy the example environment file and instructions on how to update it."
              },
              {
                "language": "bash",
                "code": "npm install",
                "context": "Source: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using npm."
              },
              {
                "language": "bash",
                "code": "yarn install",
                "context": "Source: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to install project dependencies using yarn."
              },
              {
                "language": "bash",
                "code": "npm run dev",
                "context": "Source: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to start the development server."
              }
            ]
          },
          {
            "title": "Database Configuration",
            "type": "other",
            "content": "DATABASE_URL=postgresql://username:password@localhost:5432/context7_test",
            "codeBlocks": []
          },
          {
            "title": "API Configuration",
            "type": "api",
            "content": "API_PORT=3000\nAPI_HOST=localhost",
            "codeBlocks": []
          },
          {
            "title": "Authentication",
            "type": "other",
            "content": "JWT_SECRET=your-super-secret-jwt-key\nSESSION_SECRET=your-session-secret",
            "codeBlocks": []
          },
          {
            "title": "External Services",
            "type": "other",
            "content": "REDIS_URL=redis://localhost:6379\nMONGODB_URL=mongodb://localhost:27017/context7",
            "codeBlocks": []
          },
          {
            "title": "Feature Flags",
            "type": "other",
            "content": "ENABLE_ANALYTICS=true\nENABLE_DEBUG_MODE=false\n```\n\n--------------------------------\n\n### Verify Installations (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to check if Node.js, npm, Git, and Docker are installed and their versions.\n\n```bash\nnode --version\nnpm --version\ngit --version\ndocker --version\n```\n\n--------------------------------\n\n### Start PostgreSQL (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to start the PostgreSQL service, depending on the operating system (macOS with Homebrew or Linux systemd).\n\n```bash\nbrew services start postgresql",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Verify Installations (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to check if Node.js, npm, Git, and Docker are installed and their versions.",
                "context": "ENABLE_ANALYTICS=true\nENABLE_DEBUG_MODE=false"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Start PostgreSQL (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommands to start the PostgreSQL service, depending on the operating system (macOS with Homebrew or Linux systemd).",
                "context": "npm --version\ngit --version\ndocker --version"
              }
            ]
          },
          {
            "title": "or",
            "type": "other",
            "content": "sudo systemctl start postgresql\n```\n\n--------------------------------\n\n### Setup Development Environment with npm\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/tutorials/building-your-first-app.md\n\nInitializes a new Node.js project and installs core Context7 dependencies and development tools.\n\n```bash\nmkdir my-context7-app\ncd my-context7-app\nnpm init -y\nnpm install @context7/core @context7/auth @context7/ui\nnpm install --save-dev @context7/dev-tools typescript @types/node\n```\n\n--------------------------------\n\n### Database Migration (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to run database migrations using npm.\n\n```bash\nnpm run db:migrate\n```\n\n--------------------------------\n\n### Run Test Suite (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to execute the entire test suite using npm.\n\n```bash\nnpm test\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Setup Development Environment with npm\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/tutorials/building-your-first-app.md\n\nInitializes a new Node.js project and installs core Context7 dependencies and development tools.",
                "context": "sudo systemctl start postgresql"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Database Migration (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to run database migrations using npm.",
                "context": "npm init -y\nnpm install @context7/core @context7/auth @context7/ui\nnpm install --save-dev @context7/dev-tools typescript @types/node"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Run Test Suite (Bash)\n\nSource: https://github.com/mwgmorningwood/context7-test/blob/docs/docs/getting-started.md\n\nCommand to execute the entire test suite using npm.",
                "context": "\n```bash\nnpm run db:migrate"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/alibaba/testable-mock",
        "packageName": "testable-mock",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:16:57.035Z",
        "content": "### Use TestableMock\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nGuide on how to set up and start using TestableMock for Java mocking.\n\n```en-us\nThis section provides instructions on how to use TestableMock, covering initial setup and basic usage patterns.\n```\n\n--------------------------------\n\n### Generate Documentation Locally\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nServe the TestableMock documentation locally using the docsify tool. Ensure you have Node.js installed and have installed docsify globally via npm. This command will start a local server to view the documentation.\n\n```bash\ndocsify serve docs\n```\n\n--------------------------------\n\n### Upgrade Guide\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nInstructions and considerations for upgrading to newer versions of TestableMock.\n\n```en-us\nThis guide provides steps and important notes for migrating from older versions of TestableMock to the latest release.\n```\n\n--------------------------------\n\n### Generate Documentation with Docsify\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command serves the documentation generated by Docsify. Ensure Node.js and Docsify are installed globally (`npm install -g docsify`).\n\n```bash\ndocsify serve docs\n```\n\n--------------------------------\n\n### Build TestableMock Plugin from Source\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/use-intellij-plugin.md\n\nInstructions to clone the repository, build the plugin using Gradle, and locate the packaged zip file for installation.\n\n```Bash\ngit clone https://github.com/zcbbpo/testable-idea\ncd testable-idea\n./gradlew clean build\n```\n\n--------------------------------\n\n### Use IntelliJ Plugin\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nGuide on integrating and using the TestableMock IntelliJ plugin for enhanced development experience.\n\n```en-us\nThis documentation covers the installation and usage of the IntelliJ plugin for TestableMock, providing IDE support.\n```\n\n--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command cleans the project and installs all modules using Maven. It requires JDK 1.6+ and Maven 3+.\n\n```bash\nmvn clean install\n```\n\n--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nBuild the TestableMock project using Maven. This command cleans the project, installs dependencies, and builds the artifacts. The main project requires JDK 1.6+ and Maven 3+, while the demo subprojects require JDK 1.8+.\n\n```bash\nmvn clean install\n```\n\n--------------------------------\n\n### OmniAccessor Get and Set Examples\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/omni-constructor.md\n\nDemonstrates how to retrieve and assign values to object members using OmniAccessor's get and set methods with path expressions. Handles complex object structures and array indexing.\n\n```java\nOmniAccessor.get(parent, \"{GrandChild}/content\");\nOmniAccessor.set(parent, \"children[2]/*/value\", 100);\n```\n\n--------------------------------\n\n### TestableMock Mock Scanning Log Example\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/troubleshooting.md\n\nAn example of the log content generated by TestableMock, showing found test classes, mock classes, source classes, and mocked method invocations with line numbers.\n\n```text\n[INFO] Start at Mon Jan 00 00:00:00 CST 0000\n... ...\n[INFO] Found test class com/alibaba/testable/demo/basic/DemoMockTest\n[INFO]   Found 6 test cases\n[INFO] Found mock class com/alibaba/testable/demo/basic/DemoMockTest$Mock\n[INFO]   Found 8 mock methods\n[INFO] Found source class com/alibaba/testable/demo/basic/DemoMock\n[INFO]   Found method <init>\n[INFO]   Found method newFunc\n[INFO]     Line 19, mock method \"createBlackBox\" used\n[INFO]   Found method outerFunc\n[INFO]     Line 27, mock method \"innerFunc\" used\n[INFO]     Line 27, mock method \"staticFunc\" used\n[INFO]   Found method commonFunc\n[INFO]     Line 34, mock method \"trim\" used\n[INFO]     Line 34, mock method \"sub\" used\n[INFO]     Line 34, mock method \"startsWith\" used\n... ...\n[INFO] Completed at Mon Jan 00 00:00:00 CST 0000\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Use TestableMock\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nGuide on how to set up and start using TestableMock for Java mocking.\n\n```en-us\nThis section provides instructions on how to use TestableMock, covering initial setup and basic usage patterns.\n```\n\n--------------------------------\n\n### Generate Documentation Locally\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nServe the TestableMock documentation locally using the docsify tool. Ensure you have Node.js installed and have installed docsify globally via npm. This command will start a local server to view the documentation.\n\n```bash\ndocsify serve docs\n```\n\n--------------------------------\n\n### Upgrade Guide\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nInstructions and considerations for upgrading to newer versions of TestableMock.\n\n```en-us\nThis guide provides steps and important notes for migrating from older versions of TestableMock to the latest release.\n```\n\n--------------------------------\n\n### Generate Documentation with Docsify\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command serves the documentation generated by Docsify. Ensure Node.js and Docsify are installed globally (`npm install -g docsify`).\n\n```bash\ndocsify serve docs\n```\n\n--------------------------------\n\n### Build TestableMock Plugin from Source\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/use-intellij-plugin.md\n\nInstructions to clone the repository, build the plugin using Gradle, and locate the packaged zip file for installation.\n\n```Bash\ngit clone https://github.com/zcbbpo/testable-idea\ncd testable-idea\n./gradlew clean build\n```\n\n--------------------------------\n\n### Use IntelliJ Plugin\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nGuide on integrating and using the TestableMock IntelliJ plugin for enhanced development experience.\n\n```en-us\nThis documentation covers the installation and usage of the IntelliJ plugin for TestableMock, providing IDE support.\n```\n\n--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command cleans the project and installs all modules using Maven. It requires JDK 1.6+ and Maven 3+.\n\n```bash\nmvn clean install\n```\n\n--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nBuild the TestableMock project using Maven. This command cleans the project, installs dependencies, and builds the artifacts. The main project requires JDK 1.6+ and Maven 3+, while the demo subprojects require JDK 1.8+.\n\n```bash\nmvn clean install\n```\n\n--------------------------------\n\n### OmniAccessor Get and Set Examples\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/omni-constructor.md\n\nDemonstrates how to retrieve and assign values to object members using OmniAccessor's get and set methods with path expressions. Handles complex object structures and array indexing.\n\n```java\nOmniAccessor.get(parent, \"{GrandChild}/content\");\nOmniAccessor.set(parent, \"children[2]/*/value\", 100);\n```\n\n--------------------------------\n\n### TestableMock Mock Scanning Log Example\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/troubleshooting.md\n\nAn example of the log content generated by TestableMock, showing found test classes, mock classes, source classes, and mocked method invocations with line numbers.\n\n```text\n[INFO] Start at Mon Jan 00 00:00:00 CST 0000\n... ...\n[INFO] Found test class com/alibaba/testable/demo/basic/DemoMockTest\n[INFO]   Found 6 test cases\n[INFO] Found mock class com/alibaba/testable/demo/basic/DemoMockTest$Mock\n[INFO]   Found 8 mock methods\n[INFO] Found source class com/alibaba/testable/demo/basic/DemoMock\n[INFO]   Found method <init>\n[INFO]   Found method newFunc\n[INFO]     Line 19, mock method \"createBlackBox\" used\n[INFO]   Found method outerFunc\n[INFO]     Line 27, mock method \"innerFunc\" used\n[INFO]     Line 27, mock method \"staticFunc\" used\n[INFO]   Found method commonFunc\n[INFO]     Line 34, mock method \"trim\" used\n[INFO]     Line 34, mock method \"sub\" used\n[INFO]     Line 34, mock method \"startsWith\" used\n... ...\n[INFO] Completed at Mon Jan 00 00:00:00 CST 0000\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Generate Documentation Locally\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nServe the TestableMock documentation locally using the docsify tool. Ensure you have Node.js installed and have installed docsify globally via npm. This command will start a local server to view the documentation.",
                "context": "\n```en-us\nThis section provides instructions on how to use TestableMock, covering initial setup and basic usage patterns."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Upgrade Guide\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nInstructions and considerations for upgrading to newer versions of TestableMock.",
                "context": "\n```bash\ndocsify serve docs"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Generate Documentation with Docsify\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command serves the documentation generated by Docsify. Ensure Node.js and Docsify are installed globally (`npm install -g docsify`).",
                "context": "\n```en-us\nThis guide provides steps and important notes for migrating from older versions of TestableMock to the latest release."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Build TestableMock Plugin from Source\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/use-intellij-plugin.md\n\nInstructions to clone the repository, build the plugin using Gradle, and locate the packaged zip file for installation.",
                "context": "\n```bash\ndocsify serve docs"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Use IntelliJ Plugin\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/sidebar.md\n\nGuide on integrating and using the TestableMock IntelliJ plugin for enhanced development experience.",
                "context": "git clone https://github.com/zcbbpo/testable-idea\ncd testable-idea\n./gradlew clean build"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README_EN.md\n\nThis command cleans the project and installs all modules using Maven. It requires JDK 1.6+ and Maven 3+.",
                "context": "\n```en-us\nThis documentation covers the installation and usage of the IntelliJ plugin for TestableMock, providing IDE support."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Build Project with Maven\n\nSource: https://github.com/alibaba/testable-mock/blob/master/README.md\n\nBuild the TestableMock project using Maven. This command cleans the project, installs dependencies, and builds the artifacts. The main project requires JDK 1.6+ and Maven 3+, while the demo subprojects require JDK 1.8+.",
                "context": "\n```bash\nmvn clean install"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### OmniAccessor Get and Set Examples\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/omni-constructor.md\n\nDemonstrates how to retrieve and assign values to object members using OmniAccessor's get and set methods with path expressions. Handles complex object structures and array indexing.",
                "context": "\n```bash\nmvn clean install"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### TestableMock Mock Scanning Log Example\n\nSource: https://github.com/alibaba/testable-mock/blob/master/docs/en-us/doc/troubleshooting.md\n\nAn example of the log content generated by TestableMock, showing found test classes, mock classes, source classes, and mocked method invocations with line numbers.",
                "context": "```java\nOmniAccessor.get(parent, \"{GrandChild}/content\");\nOmniAccessor.set(parent, \"children[2]/*/value\", 100);"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/teemtee/tmt",
        "packageName": "tmt",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:00.673Z",
        "content": "### Install tmt Core Functionality\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls the main tmt package with core functionality using dnf. This is the first step to getting started with tmt.\n\n```shell\nsudo dnf install -y tmt\n```\n\n--------------------------------\n\n### Install All tmt Functionality and Check Help\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls all available tmt functionality and then uses the help command to list all supported provision methods. This provides a comprehensive overview of tmt's capabilities.\n\n```shell\nsudo dnf install tmt+all\ntmt run provision --help\n```\n\n--------------------------------\n\n### Install tmt\n\nSource: https://github.com/teemtee/tmt/blob/main/examples/manual/full.md\n\nInstalls the Test Management Tool (tmt) with all functionalities using dnf package manager.\n\n```bash\nsudo dnf install -y tmt+all\n```\n\n--------------------------------\n\n### Initialize tmt Project and Create Example Plan\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new tmt project with a mini template and opens the example plan file for customization. This sets up a basic project structure for testing.\n\n```shell\ntmt init --template mini\nvim plans/example.fmf\n```\n\n--------------------------------\n\n### Create and Initialize Git Repository\n\nSource: https://github.com/teemtee/tmt/blob/main/examples/manual/full.md\n\nCreates a temporary directory, initializes it as a new git repository, and navigates into it. This is a common setup step for version-controlled projects.\n\n```bash\ncd $(mktemp -d)\ngit init\n```\n\n--------------------------------\n\n### Install tmt using tldr\n\nSource: https://github.com/teemtee/tmt/blob/main/README.rst\n\nThis command provides a quick way to get started with the tmt tool. It's a convenient shortcut for users who want to quickly access basic usage information.\n\n```shell\ntldr tmt\n```\n\n--------------------------------\n\n### Define a TMT Plan with Container Provision and Package Install\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nAn example of a TMT plan configuration. It specifies using a container for provisioning and installing the 'wget' package before executing tests.\n\n```yaml\nprovision:\n    how: container\n    image: fedora:33\nprepare:\n    how: install\n    package: wget\nexecute:\n    how: tmt\n    script: wget http://example.org/\n```\n\n--------------------------------\n\n### Initialize TMT Project with Full Template\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new TMT project using the 'full' template, which sets up a complete example including a test, a plan, and a story.\n\n```shell\ntmt init -t full\n```\n\n--------------------------------\n\n### Install tmt with Virtual Machine Provision and Run\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls tmt with virtual machine provision support and runs tests using the virtual provision method with libvirt. This is for scenarios where container environments are insufficient.\n\n```shell\nsudo dnf install -y tmt+provision-virtual\ntmt run -a provision -h virtual\n```\n\n--------------------------------\n\n### Install tmt with Container Provision and Run\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls tmt with container provision support and runs tests using the container provision method. This is useful for keeping the environment clean.\n\n```shell\nsudo dnf install -y tmt+provision-container\ntmt run -a provision -h container\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install tmt Core Functionality\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls the main tmt package with core functionality using dnf. This is the first step to getting started with tmt.\n\n```shell\nsudo dnf install -y tmt\n```\n\n--------------------------------\n\n### Install All tmt Functionality and Check Help\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls all available tmt functionality and then uses the help command to list all supported provision methods. This provides a comprehensive overview of tmt's capabilities.\n\n```shell\nsudo dnf install tmt+all\ntmt run provision --help\n```\n\n--------------------------------\n\n### Install tmt\n\nSource: https://github.com/teemtee/tmt/blob/main/examples/manual/full.md\n\nInstalls the Test Management Tool (tmt) with all functionalities using dnf package manager.\n\n```bash\nsudo dnf install -y tmt+all\n```\n\n--------------------------------\n\n### Initialize tmt Project and Create Example Plan\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new tmt project with a mini template and opens the example plan file for customization. This sets up a basic project structure for testing.\n\n```shell\ntmt init --template mini\nvim plans/example.fmf\n```\n\n--------------------------------\n\n### Create and Initialize Git Repository\n\nSource: https://github.com/teemtee/tmt/blob/main/examples/manual/full.md\n\nCreates a temporary directory, initializes it as a new git repository, and navigates into it. This is a common setup step for version-controlled projects.\n\n```bash\ncd $(mktemp -d)\ngit init\n```\n\n--------------------------------\n\n### Install tmt using tldr\n\nSource: https://github.com/teemtee/tmt/blob/main/README.rst\n\nThis command provides a quick way to get started with the tmt tool. It's a convenient shortcut for users who want to quickly access basic usage information.\n\n```shell\ntldr tmt\n```\n\n--------------------------------\n\n### Define a TMT Plan with Container Provision and Package Install\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nAn example of a TMT plan configuration. It specifies using a container for provisioning and installing the 'wget' package before executing tests.\n\n```yaml\nprovision:\n    how: container\n    image: fedora:33\nprepare:\n    how: install\n    package: wget\nexecute:\n    how: tmt\n    script: wget http://example.org/\n```\n\n--------------------------------\n\n### Initialize TMT Project with Full Template\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new TMT project using the 'full' template, which sets up a complete example including a test, a plan, and a story.\n\n```shell\ntmt init -t full\n```\n\n--------------------------------\n\n### Install tmt with Virtual Machine Provision and Run\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls tmt with virtual machine provision support and runs tests using the virtual provision method with libvirt. This is for scenarios where container environments are insufficient.\n\n```shell\nsudo dnf install -y tmt+provision-virtual\ntmt run -a provision -h virtual\n```\n\n--------------------------------\n\n### Install tmt with Container Provision and Run\n\nSource: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls tmt with container provision support and runs tests using the container provision method. This is useful for keeping the environment clean.\n\n```shell\nsudo dnf install -y tmt+provision-container\ntmt run -a provision -h container\n```",
            "codeBlocks": [
              {
                "language": "shell",
                "code": "sudo dnf install -y tmt",
                "context": "Source: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls the main tmt package with core functionality using dnf. This is the first step to getting started with tmt."
              },
              {
                "language": "shell",
                "code": "sudo dnf install tmt+all\ntmt run provision --help",
                "context": "ob/main/docs/guide.rst\n\nInstalls all available tmt functionality and then uses the help command to list all supported provision methods. This provides a comprehensive overview of tmt's capabilities."
              },
              {
                "language": "bash",
                "code": "sudo dnf install -y tmt+all",
                "context": "Source: https://github.com/teemtee/tmt/blob/main/examples/manual/full.md\n\nInstalls the Test Management Tool (tmt) with all functionalities using dnf package manager."
              },
              {
                "language": "shell",
                "code": "tmt init --template mini\nvim plans/example.fmf",
                "context": "ub.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new tmt project with a mini template and opens the example plan file for customization. This sets up a basic project structure for testing."
              },
              {
                "language": "bash",
                "code": "cd $(mktemp -d)\ngit init",
                "context": "mtee/tmt/blob/main/examples/manual/full.md\n\nCreates a temporary directory, initializes it as a new git repository, and navigates into it. This is a common setup step for version-controlled projects."
              },
              {
                "language": "shell",
                "code": "tldr tmt",
                "context": "hub.com/teemtee/tmt/blob/main/README.rst\n\nThis command provides a quick way to get started with the tmt tool. It's a convenient shortcut for users who want to quickly access basic usage information."
              },
              {
                "language": "yaml",
                "code": "provision:\n    how: container\n    image: fedora:33\nprepare:\n    how: install\n    package: wget\nexecute:\n    how: tmt\n    script: wget http://example.org/",
                "context": "s://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nAn example of a TMT plan configuration. It specifies using a container for provisioning and installing the 'wget' package before executing tests."
              },
              {
                "language": "shell",
                "code": "tmt init -t full",
                "context": "Source: https://github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInitializes a new TMT project using the 'full' template, which sets up a complete example including a test, a plan, and a story."
              },
              {
                "language": "shell",
                "code": "sudo dnf install -y tmt+provision-virtual\ntmt run -a provision -h virtual",
                "context": "cs/guide.rst\n\nInstalls tmt with virtual machine provision support and runs tests using the virtual provision method with libvirt. This is for scenarios where container environments are insufficient."
              },
              {
                "language": "shell",
                "code": "sudo dnf install -y tmt+provision-container\ntmt run -a provision -h container",
                "context": "/github.com/teemtee/tmt/blob/main/docs/guide.rst\n\nInstalls tmt with container provision support and runs tests using the container provision method. This is useful for keeping the environment clean."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/testably/testably.abstractions",
        "packageName": "testably.abstractions",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:04.358Z",
        "content": "### Dependency Injection Setup\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nShows how to register file system abstractions like IFileSystem, IRandomSystem, and ITimeSystem using Microsoft.Extensions.DependencyInjection. This sets up the necessary services for testing environments.\n\n```csharp\nServiceProvider services = new ServiceCollection()\n    .AddSingleton<IFileSystem, FileSystem>()\n    .AddSingleton<IRandomSystem, RandomSystem>()\n    .AddSingleton<ITimeSystem, TimeSystem>()\n    .BuildServiceProvider();\n```\n\n--------------------------------\n\n### NuGet Package Installation\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nProvides the command-line instructions to install the Testably.Abstractions and Testably.Abstractions.Testing NuGet packages using the .NET CLI.\n\n```powershell\ndotnet add package Testably.Abstractions\ndotnet add package Testably.Abstractions.Testing\n```\n\n--------------------------------\n\n### ZipFileHelper Class API\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/ZipFile/README.md\n\nDocumentation for the ZipFileHelper class, detailing its methods for zip archive operations.\n\n```APIDOC\nZipFileHelper:\n  Description: Provides utility methods for creating and extracting zip archives.\n\n  Methods:\n    Stream CreateZipFromDirectory(string directory)\n      Description: Creates a zip archive from the specified directory.\n      Parameters:\n        directory (string): The path to the directory to compress.\n      Returns:\n        Stream: A stream containing the zip archive data.\n      Dependencies:\n        System.IO.Compression\n        IFileSystem\n\n    void ExtractZipToDirectory(Stream stream, string directory)\n      Description: Extracts the contents of a zip archive from a stream to the specified directory.\n      Parameters:\n        stream (Stream): The stream containing the zip archive data.\n        directory (string): The path to the directory where the archive contents will be extracted.\n      Dependencies:\n        IFileSystem\n```\n\n--------------------------------\n\n### Initialize File System\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nIllustrates initializing a mock file system in a specified directory. It supports creating subdirectories, files within subdirectories, and top-level files, allowing for complex test file structures.\n\n```csharp\nfileSystem.InitializeIn(\"current-directory\")\n    .WithASubdirectory()\n    .WithSubdirectory(\"foo\").Initialized(s => s\n        .WithAFile())\n    .WithFile(\"bar.txt\");\n```\n\n--------------------------------\n\n### Configure File System Drive\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nDemonstrates how to register custom drives for the file system, which is useful for simulating multiple drives on Windows or network shares. It allows setting the total size for a drive.\n\n```csharp\nfileSystem.WithDrive(@\"D:\", drive => drive.SetTotalSize(1024));\n```\n\n--------------------------------\n\n### Initialize MockFileSystem with Fluent API\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nInitializes a MockFileSystem using a fluent API for a more chained and readable setup of directories and files. This approach allows for building the file system structure step-by-step.\n\n```csharp\nvar fileSystem = new MockFileSystem();\nfileSystem.Initialize()\n\t.WithSubdirectory(\"foo\").Initialized(d => d\n\t\t.WithSubdirectory(\"bar\")\n\t\t.WithFile(\"bar.txt\"))\n\t.WithFile(\"foo.txt\").Which(f => f.HasStringContent(\"some file content\"));\n```\n\n--------------------------------\n\n### IRandomProvider Interface\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Tests/Api/Testably.Abstractions.Api.Tests/Expected/Testably.Abstractions.Testing_net6.0.txt\n\nDefines an interface for obtaining random number generation capabilities, including methods to get a GUID and a seeded random number generator.\n\n```csharp\nnamespace Testably.Abstractions.Testing.RandomSystem\n{\n    public interface IRandomProvider\n    {\n        System.Guid GetGuid();\n        Testably.Abstractions.RandomSystem.IRandom GetRandom(int seed = -1);\n    }\n}\n```\n\n--------------------------------\n\n### Create Zip from Directory\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/ZipFile/README.md\n\nCreates a zip file stream from all files and sub-directories within a given directory using the IFileSystem. The resulting stream can be saved to a file.\n\n```csharp\nusing (Stream zipStream = ZipFileHelper.CreateZipFromDirectory(directory))\n{\n    using FileSystemStream fileStream = FileSystem.File.Create(\"test.zip\");\n    zipStream.CopyTo(fileStream);\n}\n```\n\n--------------------------------\n\n### Initialize MockFileSystem with Directory/File Descriptions\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nInitializes a MockFileSystem with a predefined directory and file structure using DirectoryDescription and FileDescription objects. This method allows for declarative setup of the file system state for testing.\n\n```csharp\nvar fileSystem = new MockFileSystem();\nfileSystem.Initialize().With(\n    new DirectoryDescription(\"foo\",\n        new DirectoryDescription(\"bar\"),\n        new FileDescription(\"bar.txt\")),\n    new FileDescription(\"foo.txt\", \"some file content\"));\n```\n\n--------------------------------\n\n### MockRandomSystem Class\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Tests/Api/Testably.Abstractions.Api.Tests/Expected/Testably.Abstractions.Testing_net6.0.txt\n\nA mock implementation of the IRandomSystem interface for testing. It allows injecting custom random providers and provides access to GUID, random factory, and the provider itself.\n\n```APIDOC\nMockRandomSystem:\n  Constructors:\n    MockRandomSystem()\n      - Initializes a new instance of the MockRandomSystem.\n\n    MockRandomSystem(randomProvider: IRandomProvider)\n      - Initializes a new instance with a specified random provider.\n      - Parameters:\n        - randomProvider: The IRandomProvider to use.\n\n  Properties:\n    Guid: IGuid\n      - Gets the GUID generation service.\n\n    Random: IRandomFactory\n      - Gets the random number generation factory.\n\n    RandomProvider: IRandomProvider\n      - Gets the configured random provider.\n\n  Methods:\n    ToString(): string\n      - Returns a string representation of the MockRandomSystem.\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Dependency Injection Setup\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nShows how to register file system abstractions like IFileSystem, IRandomSystem, and ITimeSystem using Microsoft.Extensions.DependencyInjection. This sets up the necessary services for testing environments.\n\n```csharp\nServiceProvider services = new ServiceCollection()\n    .AddSingleton<IFileSystem, FileSystem>()\n    .AddSingleton<IRandomSystem, RandomSystem>()\n    .AddSingleton<ITimeSystem, TimeSystem>()\n    .BuildServiceProvider();\n```\n\n--------------------------------\n\n### NuGet Package Installation\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nProvides the command-line instructions to install the Testably.Abstractions and Testably.Abstractions.Testing NuGet packages using the .NET CLI.\n\n```powershell\ndotnet add package Testably.Abstractions\ndotnet add package Testably.Abstractions.Testing\n```\n\n--------------------------------\n\n### ZipFileHelper Class API\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/ZipFile/README.md\n\nDocumentation for the ZipFileHelper class, detailing its methods for zip archive operations.\n\n```APIDOC\nZipFileHelper:\n  Description: Provides utility methods for creating and extracting zip archives.\n\n  Methods:\n    Stream CreateZipFromDirectory(string directory)\n      Description: Creates a zip archive from the specified directory.\n      Parameters:\n        directory (string): The path to the directory to compress.\n      Returns:\n        Stream: A stream containing the zip archive data.\n      Dependencies:\n        System.IO.Compression\n        IFileSystem\n\n    void ExtractZipToDirectory(Stream stream, string directory)\n      Description: Extracts the contents of a zip archive from a stream to the specified directory.\n      Parameters:\n        stream (Stream): The stream containing the zip archive data.\n        directory (string): The path to the directory where the archive contents will be extracted.\n      Dependencies:\n        IFileSystem\n```\n\n--------------------------------\n\n### Initialize File System\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nIllustrates initializing a mock file system in a specified directory. It supports creating subdirectories, files within subdirectories, and top-level files, allowing for complex test file structures.\n\n```csharp\nfileSystem.InitializeIn(\"current-directory\")\n    .WithASubdirectory()\n    .WithSubdirectory(\"foo\").Initialized(s => s\n        .WithAFile())\n    .WithFile(\"bar.txt\");\n```\n\n--------------------------------\n\n### Configure File System Drive\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/Configuration/README.md\n\nDemonstrates how to register custom drives for the file system, which is useful for simulating multiple drives on Windows or network shares. It allows setting the total size for a drive.\n\n```csharp\nfileSystem.WithDrive(@\"D:\", drive => drive.SetTotalSize(1024));\n```\n\n--------------------------------\n\n### Initialize MockFileSystem with Fluent API\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nInitializes a MockFileSystem using a fluent API for a more chained and readable setup of directories and files. This approach allows for building the file system structure step-by-step.\n\n```csharp\nvar fileSystem = new MockFileSystem();\nfileSystem.Initialize()\n\t.WithSubdirectory(\"foo\").Initialized(d => d\n\t\t.WithSubdirectory(\"bar\")\n\t\t.WithFile(\"bar.txt\"))\n\t.WithFile(\"foo.txt\").Which(f => f.HasStringContent(\"some file content\"));\n```\n\n--------------------------------\n\n### IRandomProvider Interface\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Tests/Api/Testably.Abstractions.Api.Tests/Expected/Testably.Abstractions.Testing_net6.0.txt\n\nDefines an interface for obtaining random number generation capabilities, including methods to get a GUID and a seeded random number generator.\n\n```csharp\nnamespace Testably.Abstractions.Testing.RandomSystem\n{\n    public interface IRandomProvider\n    {\n        System.Guid GetGuid();\n        Testably.Abstractions.RandomSystem.IRandom GetRandom(int seed = -1);\n    }\n}\n```\n\n--------------------------------\n\n### Create Zip from Directory\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Examples/ZipFile/README.md\n\nCreates a zip file stream from all files and sub-directories within a given directory using the IFileSystem. The resulting stream can be saved to a file.\n\n```csharp\nusing (Stream zipStream = ZipFileHelper.CreateZipFromDirectory(directory))\n{\n    using FileSystemStream fileStream = FileSystem.File.Create(\"test.zip\");\n    zipStream.CopyTo(fileStream);\n}\n```\n\n--------------------------------\n\n### Initialize MockFileSystem with Directory/File Descriptions\n\nSource: https://github.com/testably/testably.abstractions/blob/main/README.md\n\nInitializes a MockFileSystem with a predefined directory and file structure using DirectoryDescription and FileDescription objects. This method allows for declarative setup of the file system state for testing.\n\n```csharp\nvar fileSystem = new MockFileSystem();\nfileSystem.Initialize().With(\n    new DirectoryDescription(\"foo\",\n        new DirectoryDescription(\"bar\"),\n        new FileDescription(\"bar.txt\")),\n    new FileDescription(\"foo.txt\", \"some file content\"));\n```\n\n--------------------------------\n\n### MockRandomSystem Class\n\nSource: https://github.com/testably/testably.abstractions/blob/main/Tests/Api/Testably.Abstractions.Api.Tests/Expected/Testably.Abstractions.Testing_net6.0.txt\n\nA mock implementation of the IRandomSystem interface for testing. It allows injecting custom random providers and provides access to GUID, random factory, and the provider itself.\n\n```APIDOC\nMockRandomSystem:\n  Constructors:\n    MockRandomSystem()\n      - Initializes a new instance of the MockRandomSystem.\n\n    MockRandomSystem(randomProvider: IRandomProvider)\n      - Initializes a new instance with a specified random provider.\n      - Parameters:\n        - randomProvider: The IRandomProvider to use.\n\n  Properties:\n    Guid: IGuid\n      - Gets the GUID generation service.\n\n    Random: IRandomFactory\n      - Gets the random number generation factory.\n\n    RandomProvider: IRandomProvider\n      - Gets the configured random provider.\n\n  Methods:\n    ToString(): string\n      - Returns a string representation of the MockRandomSystem.\n```",
            "codeBlocks": [
              {
                "language": "csharp",
                "code": "ServiceProvider services = new ServiceCollection()\n    .AddSingleton<IFileSystem, FileSystem>()\n    .AddSingleton<IRandomSystem, RandomSystem>()\n    .AddSingleton<ITimeSystem, TimeSystem>()\n    .BuildServiceProvider();",
                "context": "ow to register file system abstractions like IFileSystem, IRandomSystem, and ITimeSystem using Microsoft.Extensions.DependencyInjection. This sets up the necessary services for testing environments."
              },
              {
                "language": "powershell",
                "code": "dotnet add package Testably.Abstractions\ndotnet add package Testably.Abstractions.Testing",
                "context": "m/testably/testably.abstractions/blob/main/README.md\n\nProvides the command-line instructions to install the Testably.Abstractions and Testably.Abstractions.Testing NuGet packages using the .NET CLI."
              },
              {
                "language": "APIDOC",
                "code": "ZipFileHelper:\n  Description: Provides utility methods for creating and extracting zip archives.\n\n  Methods:\n    Stream CreateZipFromDirectory(string directory)\n      Description: Creates a zip archive from the specified directory.\n      Parameters:\n        directory (string): The path to the directory to compress.\n      Returns:\n        Stream: A stream containing the zip archive data.\n      Dependencies:\n        System.IO.Compression\n        IFileSystem\n\n    void ExtractZipToDirectory(Stream stream, string directory)\n      Description: Extracts the contents of a zip archive from a stream to the specified directory.\n      Parameters:\n        stream (Stream): The stream containing the zip archive data.\n        directory (string): The path to the directory where the archive contents will be extracted.\n      Dependencies:\n        IFileSystem",
                "context": "Source: https://github.com/testably/testably.abstractions/blob/main/Examples/ZipFile/README.md\n\nDocumentation for the ZipFileHelper class, detailing its methods for zip archive operations."
              },
              {
                "language": "csharp",
                "code": "fileSystem.InitializeIn(\"current-directory\")\n    .WithASubdirectory()\n    .WithSubdirectory(\"foo\").Initialized(s => s\n        .WithAFile())\n    .WithFile(\"bar.txt\");",
                "context": "llustrates initializing a mock file system in a specified directory. It supports creating subdirectories, files within subdirectories, and top-level files, allowing for complex test file structures."
              },
              {
                "language": "csharp",
                "code": "fileSystem.WithDrive(@\"D:\", drive => drive.SetTotalSize(1024));",
                "context": "/README.md\n\nDemonstrates how to register custom drives for the file system, which is useful for simulating multiple drives on Windows or network shares. It allows setting the total size for a drive."
              },
              {
                "language": "csharp",
                "code": "var fileSystem = new MockFileSystem();\nfileSystem.Initialize()\n\t.WithSubdirectory(\"foo\").Initialized(d => d\n\t\t.WithSubdirectory(\"bar\")\n\t\t.WithFile(\"bar.txt\"))\n\t.WithFile(\"foo.txt\").Which(f => f.HasStringContent(\"some file content\"));",
                "context": "n/README.md\n\nInitializes a MockFileSystem using a fluent API for a more chained and readable setup of directories and files. This approach allows for building the file system structure step-by-step."
              },
              {
                "language": "csharp",
                "code": "namespace Testably.Abstractions.Testing.RandomSystem\n{\n    public interface IRandomProvider\n    {\n        System.Guid GetGuid();\n        Testably.Abstractions.RandomSystem.IRandom GetRandom(int seed = -1);\n    }\n}",
                "context": "sts/Expected/Testably.Abstractions.Testing_net6.0.txt\n\nDefines an interface for obtaining random number generation capabilities, including methods to get a GUID and a seeded random number generator."
              },
              {
                "language": "csharp",
                "code": "using (Stream zipStream = ZipFileHelper.CreateZipFromDirectory(directory))\n{\n    using FileSystemStream fileStream = FileSystem.File.Create(\"test.zip\");\n    zipStream.CopyTo(fileStream);\n}",
                "context": "ctions/blob/main/Examples/ZipFile/README.md\n\nCreates a zip file stream from all files and sub-directories within a given directory using the IFileSystem. The resulting stream can be saved to a file."
              },
              {
                "language": "csharp",
                "code": "var fileSystem = new MockFileSystem();\nfileSystem.Initialize().With(\n    new DirectoryDescription(\"foo\",\n        new DirectoryDescription(\"bar\"),\n        new FileDescription(\"bar.txt\")),\n    new FileDescription(\"foo.txt\", \"some file content\"));",
                "context": "a MockFileSystem with a predefined directory and file structure using DirectoryDescription and FileDescription objects. This method allows for declarative setup of the file system state for testing."
              },
              {
                "language": "APIDOC",
                "code": "MockRandomSystem:\n  Constructors:\n    MockRandomSystem()\n      - Initializes a new instance of the MockRandomSystem.\n\n    MockRandomSystem(randomProvider: IRandomProvider)\n      - Initializes a new instance with a specified random provider.\n      - Parameters:\n        - randomProvider: The IRandomProvider to use.\n\n  Properties:\n    Guid: IGuid\n      - Gets the GUID generation service.\n\n    Random: IRandomFactory\n      - Gets the random number generation factory.\n\n    RandomProvider: IRandomProvider\n      - Gets the configured random provider.\n\n  Methods:\n    ToString(): string\n      - Returns a string representation of the MockRandomSystem.",
                "context": "esting_net6.0.txt\n\nA mock implementation of the IRandomSystem interface for testing. It allows injecting custom random providers and provides access to GUID, random factory, and the provider itself."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/super3001/testfwk_arkxtest",
        "packageName": "testfwk_arkxtest",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:08.020Z",
        "content": "### Mocking Functions with afterReturn in JsUnit\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nDemonstrates how to mock a function within a class using JsUnit's MockKit and the afterReturn API. This example shows the setup, mocking, and assertion process. It requires importing MockKit and when from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### JsUnit Basic Process Support Example\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nDemonstrates the basic structure of a JsUnit test suite using describe, it, and expect. It includes example assertions and asynchronous API calls. This snippet requires the '@ohos/hypium' and '@ohos.bundle' modules.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Define Test Suite and Lifecycle Hooks with describe() - JavaScript\n\nSource: https://context7.com/super3001/testfwk_arkxtest/llms.txt\n\nDefines a test suite using the `describe` function and includes lifecycle hooks (`beforeAll`, `beforeEach`, `afterEach`, `afterAll`) for setup and teardown operations in JavaScript. It also demonstrates basic assertions using `expect` and asynchronous operations with `it`.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    // Runs once before all tests\n    beforeAll(function () {\n      console.info('Test suite starting')\n    })\n\n    // Runs before each test\n    beforeEach(function () {\n      console.info('Test case starting')\n    })\n\n    // Runs after each test\n    afterEach(function () {\n      console.info('Test case completed')\n    })\n\n    // Runs once after all tests\n    afterAll(function () {\n      console.info('Test suite completed')\n    })\n\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with Regex Argument Matcher\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nThis example demonstrates mocking a class method using a regular expression to match arguments. The mocked function returns '1' if the input string matches the provided regex. It includes setup for MockKit, class definition, mocking, and assertions. Dependencies include MockKit, ArgumentMatchers, and Hypium testing utilities.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set a regular expression to match, for example, /123456/.\n            when(mockfunc)(ArgumentMatchers.matchRegexs(/123456/)).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string, for example, '1234567898', is passed in.\n            expect(claser.method_1('1234567898')).assertEqual('1'); // The operation is successful.\n            // The string '1234567898' matches the regular expression /123456/.\n\n            // The operation fails if '1234' is passed in.\n            //expect(claser.method_1('1234').assertEqual('1'); // The operation fails because '1234' does not match the regular expression /123456/.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Mock Function with Any String Argument Matcher\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method to accept any string argument. This example demonstrates the use of `ArgumentMatchers.anyString` for specific string matching. The code snippet is incomplete, but it sets up the mocking scenario using `MockKit` and `when` from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n\n```\n\n--------------------------------\n\n### Verifying Function Execution At Least a Certain Number of Times\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nThis example illustrates how to use MockKit's `verify` and `atLeast` methods to ensure a mocked function is called a minimum number of times. It follows a similar setup to the `times` verification, including mocking a class method, executing it multiple times, and then verifying that a specific method was called at least once.\n\n```javascript\nimport { describe, expect, it, MockKit, when } from '@ohos/hypium'\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_verify_atLeast', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Define the class to be mocked.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n            }\n\n            // 3. Create an object of the class.\n            let claser = new ClassName();\n            // 4. Mock a function, for example, method_1, of the object.\n            let func_1 = mocker.mockFunc(claser, claser.method_1);\n            // 5. Set the expected value to be returned by the mocked function.\n            when(func_1)('123').afterReturn('4');\n            // 6. Execute the function several times, with parameters set as follows:\n            claser.method_1('123', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            // The verify method is used here to check if method_1 was called at least once.\n            mocker.verify('method_1').atLeast(1);\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Data-Driven Testing Setup with Hypium.setData() in JavaScript\n\nSource: https://context7.com/super3001/testfwk_arkxtest/llms.txt\n\nConfigures parameterized tests by loading data from a JSON file using Hypium.setData(). This enables tests to run multiple times with different input parameters defined in the 'data.json' configuration. It's crucial for efficient testing of scenarios with varying data sets.\n\n```json\n// data.json\n{\n  \"suites\": [{\n    \"describe\": [\"actsAbilityTest\"],\n    \"stress\": 2,\n    \"params\": {\n      \"suiteParams1\": \"suiteParams001\",\n      \"suiteParams2\": \"suiteParams002\"\n    },\n    \"items\": [{\n      \"it\": \"testDataDriverAsync\",\n      \"stress\": 2,\n      \"params\": [{\n        \"name\": \"tom\",\n        \"value\": 5\n      }, {\n        \"name\": \"jerry\",\n        \"value\": 4\n      }]\n    }, {\n      \"it\": \"testDataDriver\",\n      \"stress\": 3\n    }]\n  }]\n}\n```\n\n```javascript\n// app.js - Test initialization\nimport AbilityDelegatorRegistry from '@ohos.application.abilityDelegatorRegistry'\nimport { Hypium } from '@ohos/hypium'\nimport testsuite from '../test/List.test'\nimport data from '../test/data.json'\n\n// Configure data-driven parameters\nHypium.setData(data)\nHypium.hypiumTest(abilityDelegator, abilityDelegatorArguments, testsuite)\n\n// Test implementation\nimport { describe, it, expect } from '@ohos/hypium'\n\nexport default function abilityTest() {\n  describe('actsAbilityTest', function () {\n    // Test executes 2 times (stress: 2) with 2 different param sets\n    it('testDataDriverAsync', 0, async function (done, data) {\n      console.info('Testing with name: ' + data.name)\n      console.info('Testing with value: ' + data.value)\n      expect(data.value).assertLarger(0)\n      done()\n    })\n\n    // Test executes 3 times (stress: 3) without params\n    it('testDataDriver', 0, function () {\n      console.info('Stress test execution')\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with Any Argument Matcher and Specific Return Value\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method to accept any argument (except undefined and null) and return a specific value. This example utilizes `ArgumentMatchers.any` for flexible argument handling and `afterReturn('1')` to set the return value. It requires importing `ArgumentMatchers` from '@ohos/hypium'. Assertions demonstrate successful calls with various data types.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set the parameter matcher and expected return value as required.\n            when(mockfunc)(ArgumentMatchers.any).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a number (for example '123') is passed in.\n            expect(claser.method_1(123)).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a Boolean value (for example 'true') is passed in.\n            expect(claser.method_1(true)).assertEqual('1');// The operation is successful.\n\n            // The operation fails if an empty value is passed in.\n            //expect(claser.method_1()).assertEqual('1');// The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Asserting Component Existence with UiDriver in JavaScript\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nIllustrates how to assert the existence of a UI component using UiDriver and the assertComponentExist method in JavaScript. The example demonstrates creating a UiDriver, checking for the presence of a component identified by its text, and handling potential exceptions within a try-finally block. This function is crucial for validating UI state during tests.\n\n```javascript\nimport {BY,UiDriver,UiComponent} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('UiTestDemo', function() {\n    it('Uitest_demo0', 0, async function(done) {\n      try{\n        // create UiDriver\n        let driver = await UiDriver.create()\n        // assert text 'hello' exists on current Ui\n        await assertComponentExist(BY.text('hello'))\n      } finally {\n        done()\n      }\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with No Return Value using afterReturnNothing\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method and configures it to return undefined after being called with a specific argument. This example demonstrates the use of `afterReturnNothing()` and asserts that the mocked method returns undefined when called with the expected argument. It requires importing MockKit and related functions from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n\n            // 4. Set the action to be performed when the test case ends. For example, set it to afterReturnNothing(), which returns no value.\n            when(mockfunc)('test').afterReturnNothing();\n\n            // 5. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            // The mocked claser.method_1 does not return '888888'. Instead, afterReturnNothing() takes effect, that is, no value is returned.\n            expect(claser.method_1('test')).assertUndefined(); // The operation is successful.\n\n            // The operation fails if '123' is passed in.\n            // expect(method_1(123)).assertUndefined();// The operation fails.\n        });\n    });\n}\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Mocking Functions with afterReturn in JsUnit\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nDemonstrates how to mock a function within a class using JsUnit's MockKit and the afterReturn API. This example shows the setup, mocking, and assertion process. It requires importing MockKit and when from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### JsUnit Basic Process Support Example\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nDemonstrates the basic structure of a JsUnit test suite using describe, it, and expect. It includes example assertions and asynchronous API calls. This snippet requires the '@ohos/hypium' and '@ohos.bundle' modules.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Define Test Suite and Lifecycle Hooks with describe() - JavaScript\n\nSource: https://context7.com/super3001/testfwk_arkxtest/llms.txt\n\nDefines a test suite using the `describe` function and includes lifecycle hooks (`beforeAll`, `beforeEach`, `afterEach`, `afterAll`) for setup and teardown operations in JavaScript. It also demonstrates basic assertions using `expect` and asynchronous operations with `it`.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    // Runs once before all tests\n    beforeAll(function () {\n      console.info('Test suite starting')\n    })\n\n    // Runs before each test\n    beforeEach(function () {\n      console.info('Test case starting')\n    })\n\n    // Runs after each test\n    afterEach(function () {\n      console.info('Test case completed')\n    })\n\n    // Runs once after all tests\n    afterAll(function () {\n      console.info('Test suite completed')\n    })\n\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with Regex Argument Matcher\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nThis example demonstrates mocking a class method using a regular expression to match arguments. The mocked function returns '1' if the input string matches the provided regex. It includes setup for MockKit, class definition, mocking, and assertions. Dependencies include MockKit, ArgumentMatchers, and Hypium testing utilities.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set a regular expression to match, for example, /123456/.\n            when(mockfunc)(ArgumentMatchers.matchRegexs(/123456/)).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string, for example, '1234567898', is passed in.\n            expect(claser.method_1('1234567898')).assertEqual('1'); // The operation is successful.\n            // The string '1234567898' matches the regular expression /123456/.\n\n            // The operation fails if '1234' is passed in.\n            //expect(claser.method_1('1234').assertEqual('1'); // The operation fails because '1234' does not match the regular expression /123456/.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Mock Function with Any String Argument Matcher\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method to accept any string argument. This example demonstrates the use of `ArgumentMatchers.anyString` for specific string matching. The code snippet is incomplete, but it sets up the mocking scenario using `MockKit` and `when` from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n\n```\n\n--------------------------------\n\n### Verifying Function Execution At Least a Certain Number of Times\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nThis example illustrates how to use MockKit's `verify` and `atLeast` methods to ensure a mocked function is called a minimum number of times. It follows a similar setup to the `times` verification, including mocking a class method, executing it multiple times, and then verifying that a specific method was called at least once.\n\n```javascript\nimport { describe, expect, it, MockKit, when } from '@ohos/hypium'\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_verify_atLeast', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Define the class to be mocked.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n            }\n\n            // 3. Create an object of the class.\n            let claser = new ClassName();\n            // 4. Mock a function, for example, method_1, of the object.\n            let func_1 = mocker.mockFunc(claser, claser.method_1);\n            // 5. Set the expected value to be returned by the mocked function.\n            when(func_1)('123').afterReturn('4');\n            // 6. Execute the function several times, with parameters set as follows:\n            claser.method_1('123', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            // The verify method is used here to check if method_1 was called at least once.\n            mocker.verify('method_1').atLeast(1);\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Data-Driven Testing Setup with Hypium.setData() in JavaScript\n\nSource: https://context7.com/super3001/testfwk_arkxtest/llms.txt\n\nConfigures parameterized tests by loading data from a JSON file using Hypium.setData(). This enables tests to run multiple times with different input parameters defined in the 'data.json' configuration. It's crucial for efficient testing of scenarios with varying data sets.\n\n```json\n// data.json\n{\n  \"suites\": [{\n    \"describe\": [\"actsAbilityTest\"],\n    \"stress\": 2,\n    \"params\": {\n      \"suiteParams1\": \"suiteParams001\",\n      \"suiteParams2\": \"suiteParams002\"\n    },\n    \"items\": [{\n      \"it\": \"testDataDriverAsync\",\n      \"stress\": 2,\n      \"params\": [{\n        \"name\": \"tom\",\n        \"value\": 5\n      }, {\n        \"name\": \"jerry\",\n        \"value\": 4\n      }]\n    }, {\n      \"it\": \"testDataDriver\",\n      \"stress\": 3\n    }]\n  }]\n}\n```\n\n```javascript\n// app.js - Test initialization\nimport AbilityDelegatorRegistry from '@ohos.application.abilityDelegatorRegistry'\nimport { Hypium } from '@ohos/hypium'\nimport testsuite from '../test/List.test'\nimport data from '../test/data.json'\n\n// Configure data-driven parameters\nHypium.setData(data)\nHypium.hypiumTest(abilityDelegator, abilityDelegatorArguments, testsuite)\n\n// Test implementation\nimport { describe, it, expect } from '@ohos/hypium'\n\nexport default function abilityTest() {\n  describe('actsAbilityTest', function () {\n    // Test executes 2 times (stress: 2) with 2 different param sets\n    it('testDataDriverAsync', 0, async function (done, data) {\n      console.info('Testing with name: ' + data.name)\n      console.info('Testing with value: ' + data.value)\n      expect(data.value).assertLarger(0)\n      done()\n    })\n\n    // Test executes 3 times (stress: 3) without params\n    it('testDataDriver', 0, function () {\n      console.info('Stress test execution')\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with Any Argument Matcher and Specific Return Value\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method to accept any argument (except undefined and null) and return a specific value. This example utilizes `ArgumentMatchers.any` for flexible argument handling and `afterReturn('1')` to set the return value. It requires importing `ArgumentMatchers` from '@ohos/hypium'. Assertions demonstrate successful calls with various data types.\n\n```javascript\nimport {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set the parameter matcher and expected return value as required.\n            when(mockfunc)(ArgumentMatchers.any).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a number (for example '123') is passed in.\n            expect(claser.method_1(123)).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a Boolean value (for example 'true') is passed in.\n            expect(claser.method_1(true)).assertEqual('1');// The operation is successful.\n\n            // The operation fails if an empty value is passed in.\n            //expect(claser.method_1()).assertEqual('1');// The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Asserting Component Existence with UiDriver in JavaScript\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nIllustrates how to assert the existence of a UI component using UiDriver and the assertComponentExist method in JavaScript. The example demonstrates creating a UiDriver, checking for the presence of a component identified by its text, and handling potential exceptions within a try-finally block. This function is crucial for validating UI state during tests.\n\n```javascript\nimport {BY,UiDriver,UiComponent} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('UiTestDemo', function() {\n    it('Uitest_demo0', 0, async function(done) {\n      try{\n        // create UiDriver\n        let driver = await UiDriver.create()\n        // assert text 'hello' exists on current Ui\n        await assertComponentExist(BY.text('hello'))\n      } finally {\n        done()\n      }\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mock Function with No Return Value using afterReturnNothing\n\nSource: https://github.com/super3001/testfwk_arkxtest/blob/master/README_en.md\n\nMocks a class method and configures it to return undefined after being called with a specific argument. This example demonstrates the use of `afterReturnNothing()` and asserts that the mocked method returns undefined when called with the expected argument. It requires importing MockKit and related functions from '@ohos/hypium'.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n\n            // 4. Set the action to be performed when the test case ends. For example, set it to afterReturnNothing(), which returns no value.\n            when(mockfunc)('test').afterReturnNothing();\n\n            // 5. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            // The mocked claser.method_1 does not return '888888'. Instead, afterReturnNothing() takes effect, that is, no value is returned.\n            expect(claser.method_1('test')).assertUndefined(); // The operation is successful.\n\n            // The operation fails if '123' is passed in.\n            // expect(method_1(123)).assertUndefined();// The operation fails.\n        });\n    });\n}\n```",
            "codeBlocks": [
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}",
                "context": "ck a function within a class using JsUnit's MockKit and the afterReturn API. This example shows the setup, mocking, and assertion process. It requires importing MockKit and when from '@ohos/hypium'."
              },
              {
                "language": "javascript",
                "code": "import { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}",
                "context": "basic structure of a JsUnit test suite using describe, it, and expect. It includes example assertions and asynchronous API calls. This snippet requires the '@ohos/hypium' and '@ohos.bundle' modules."
              },
              {
                "language": "javascript",
                "code": "import { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    // Runs once before all tests\n    beforeAll(function () {\n      console.info('Test suite starting')\n    })\n\n    // Runs before each test\n    beforeEach(function () {\n      console.info('Test case starting')\n    })\n\n    // Runs after each test\n    afterEach(function () {\n      console.info('Test case completed')\n    })\n\n    // Runs once after all tests\n    afterAll(function () {\n      console.info('Test suite completed')\n    })\n\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}",
                "context": "ooks (`beforeAll`, `beforeEach`, `afterEach`, `afterAll`) for setup and teardown operations in JavaScript. It also demonstrates basic assertions using `expect` and asynchronous operations with `it`."
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set a regular expression to match, for example, /123456/.\n            when(mockfunc)(ArgumentMatchers.matchRegexs(/123456/)).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string, for example, '1234567898', is passed in.\n            expect(claser.method_1('1234567898')).assertEqual('1'); // The operation is successful.\n            // The string '1234567898' matches the regular expression /123456/.\n\n            // The operation fails if '1234' is passed in.\n            //expect(claser.method_1('1234').assertEqual('1'); // The operation fails because '1234' does not match the regular expression /123456/.\n        });\n    });\n}",
                "context": "f the input string matches the provided regex. It includes setup for MockKit, class definition, mocking, and assertions. Dependencies include MockKit, ArgumentMatchers, and Hypium testing utilities."
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.",
                "context": "demonstrates the use of `ArgumentMatchers.anyString` for specific string matching. The code snippet is incomplete, but it sets up the mocking scenario using `MockKit` and `when` from '@ohos/hypium'."
              },
              {
                "language": "javascript",
                "code": "import { describe, expect, it, MockKit, when } from '@ohos/hypium'\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_verify_atLeast', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Define the class to be mocked.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n            }\n\n            // 3. Create an object of the class.\n            let claser = new ClassName();\n            // 4. Mock a function, for example, method_1, of the object.\n            let func_1 = mocker.mockFunc(claser, claser.method_1);\n            // 5. Set the expected value to be returned by the mocked function.\n            when(func_1)('123').afterReturn('4');\n            // 6. Execute the function several times, with parameters set as follows:\n            claser.method_1('123', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            // The verify method is used here to check if method_1 was called at least once.\n            mocker.verify('method_1').atLeast(1);\n        });\n    });\n}",
                "context": "r of times. It follows a similar setup to the `times` verification, including mocking a class method, executing it multiple times, and then verifying that a specific method was called at least once."
              },
              {
                "language": "json",
                "code": "// data.json\n{\n  \"suites\": [{\n    \"describe\": [\"actsAbilityTest\"],\n    \"stress\": 2,\n    \"params\": {\n      \"suiteParams1\": \"suiteParams001\",\n      \"suiteParams2\": \"suiteParams002\"\n    },\n    \"items\": [{\n      \"it\": \"testDataDriverAsync\",\n      \"stress\": 2,\n      \"params\": [{\n        \"name\": \"tom\",\n        \"value\": 5\n      }, {\n        \"name\": \"jerry\",\n        \"value\": 4\n      }]\n    }, {\n      \"it\": \"testDataDriver\",\n      \"stress\": 3\n    }]\n  }]\n}",
                "context": ".setData(). This enables tests to run multiple times with different input parameters defined in the 'data.json' configuration. It's crucial for efficient testing of scenarios with varying data sets."
              },
              {
                "language": "javascript",
                "code": "// app.js - Test initialization\nimport AbilityDelegatorRegistry from '@ohos.application.abilityDelegatorRegistry'\nimport { Hypium } from '@ohos/hypium'\nimport testsuite from '../test/List.test'\nimport data from '../test/data.json'\n\n// Configure data-driven parameters\nHypium.setData(data)\nHypium.hypiumTest(abilityDelegator, abilityDelegatorArguments, testsuite)\n\n// Test implementation\nimport { describe, it, expect } from '@ohos/hypium'\n\nexport default function abilityTest() {\n  describe('actsAbilityTest', function () {\n    // Test executes 2 times (stress: 2) with 2 different param sets\n    it('testDataDriverAsync', 0, async function (done, data) {\n      console.info('Testing with name: ' + data.name)\n      console.info('Testing with value: ' + data.value)\n      expect(data.value).assertLarger(0)\n      done()\n    })\n\n    // Test executes 3 times (stress: 3) without params\n    it('testDataDriver', 0, function () {\n      console.info('Stress test execution')\n    })\n  })\n}",
                "context": "  }]\n}\n```"
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when, ArgumentMatchers} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            // Set the parameter matcher and expected return value as required.\n            when(mockfunc)(ArgumentMatchers.any).afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if a string is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a number (for example '123') is passed in.\n            expect(claser.method_1(123)).assertEqual('1'); // The operation is successful.\n            // The operation is successful if a Boolean value (for example 'true') is passed in.\n            expect(claser.method_1(true)).assertEqual('1');// The operation is successful.\n\n            // The operation fails if an empty value is passed in.\n            //expect(claser.method_1()).assertEqual('1');// The operation fails.\n        });\n    });\n}",
                "context": "xible argument handling and `afterReturn('1')` to set the return value. It requires importing `ArgumentMatchers` from '@ohos/hypium'. Assertions demonstrate successful calls with various data types."
              },
              {
                "language": "javascript",
                "code": "import {BY,UiDriver,UiComponent} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('UiTestDemo', function() {\n    it('Uitest_demo0', 0, async function(done) {\n      try{\n        // create UiDriver\n        let driver = await UiDriver.create()\n        // assert text 'hello' exists on current Ui\n        await assertComponentExist(BY.text('hello'))\n      } finally {\n        done()\n      }\n    })\n  })\n}",
                "context": "iDriver, checking for the presence of a component identified by its text, and handling potential exceptions within a try-finally block. This function is crucial for validating UI state during tests."
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n\n            // 4. Set the action to be performed when the test case ends. For example, set it to afterReturnNothing(), which returns no value.\n            when(mockfunc)('test').afterReturnNothing();\n\n            // 5. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            // The mocked claser.method_1 does not return '888888'. Instead, afterReturnNothing() takes effect, that is, no value is returned.\n            expect(claser.method_1('test')).assertUndefined(); // The operation is successful.\n\n            // The operation fails if '123' is passed in.\n            // expect(method_1(123)).assertUndefined();// The operation fails.\n        });\n    });\n}",
                "context": "he use of `afterReturnNothing()` and asserts that the mocked method returns undefined when called with the expected argument. It requires importing MockKit and related functions from '@ohos/hypium'."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/x-tabdeveloping/turftopic",
        "packageName": "turftopic",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:12.019Z",
        "content": "### Install Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/index.md\n\nInstalls the turftopic library using pip. This is the first step to using Turftopic in your Python projects.\n\n```bash\npip install turftopic\n```\n\n--------------------------------\n\n### Basic KeyNMF Model Usage with Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/index.md\n\nDemonstrates how to use the KeyNMF model from Turftopic. It fetches the 20Newsgroups dataset, trains a KeyNMF model, and prints the topics. This example assumes familiarity with scikit-learn.\n\n```python\nfrom turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()\n```\n\n--------------------------------\n\n### Install Development Dependencies with Pip\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/CONTRIBUTING.md\n\nInstalls all necessary development dependencies for the Turftopic project. This command assumes you have pip installed and are in the project's root directory.\n\n```console\npip install turftopic[dev]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/ideologies.md\n\nInstalls the Turftopic library along with Plotly for visualization and the 'datasets' library for fetching data from Hugging Face Hub. This is a prerequisite for running the tutorial.\n\n```bash\npip install datasets plotly pandas turftopic\n\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/reviews.md\n\nInstalls the turftopic library along with necessary packages like SpaCy, Plotly, and Pandas. It also downloads a small English language model for SpaCy.\n\n```shell\npip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/arxiv_ml.md\n\nInstalls the necessary Python libraries for the project, including `datasets`, `plotly`, and `turftopic` with optional dependencies for UMAP-based clustering and datamapplot.\n\n```bash\npip install datasets plotly turftopic[umap-learn, datamapplot]\n```\n\n--------------------------------\n\n### Run Full Test Suite with Pytest\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/CONTRIBUTING.md\n\nExecutes the entire test suite for the Turftopic project using pytest. Ensure you have pytest installed and are in the project's root directory.\n\n```console\npytest tests/\n```\n\n--------------------------------\n\n### Install Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/README.md\n\nInstalls the turftopic library from PyPI. Includes optional dependencies for specific functionalities like CTMs (using Pyro) or clustering models (using UMAP).\n\n```bash\npip install turftopic\n\n```\n\n```bash\npip install \"turftopic[pyro-ppl]\"\n\n```\n\n```bash\npip install \"turftopic[umap-learn]\"\n\n```\n\n--------------------------------\n\n### Install MTEB and Initialize KeyNMF with MTEB Encoder\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/multimodal.md\n\nInstalls the MTEB library and initializes the KeyNMF model using an encoder compatible with the MTEB multimodal encoder interface. This enables topic modeling on multimodal data. Requires installation of MTEB.\n\n```bash\npip install \"mteb<2.0.0\"\n```\n\n```python\nfrom turftopic import KeyNMF\nimport mteb\n\nencoder = mteb.get_model(\"kakaobrain/align-base\")\n\nmultimodal_keynmf = KeyNMF(10, encoder=\"clip-ViT-B-32\")\n```\n\n--------------------------------\n\n### Install Turftopic with Datamapplot (Bash)\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/model_interpretation.md\n\nInstalls the turftopic library with the datamapplot extra, enabling interactive cluster visualizations. This command is run in the terminal.\n\n```bash\npip install turftopic[datamapplot]\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/index.md\n\nInstalls the turftopic library using pip. This is the first step to using Turftopic in your Python projects.\n\n```bash\npip install turftopic\n```\n\n--------------------------------\n\n### Basic KeyNMF Model Usage with Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/index.md\n\nDemonstrates how to use the KeyNMF model from Turftopic. It fetches the 20Newsgroups dataset, trains a KeyNMF model, and prints the topics. This example assumes familiarity with scikit-learn.\n\n```python\nfrom turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()\n```\n\n--------------------------------\n\n### Install Development Dependencies with Pip\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/CONTRIBUTING.md\n\nInstalls all necessary development dependencies for the Turftopic project. This command assumes you have pip installed and are in the project's root directory.\n\n```console\npip install turftopic[dev]\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/ideologies.md\n\nInstalls the Turftopic library along with Plotly for visualization and the 'datasets' library for fetching data from Hugging Face Hub. This is a prerequisite for running the tutorial.\n\n```bash\npip install datasets plotly pandas turftopic\n\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/reviews.md\n\nInstalls the turftopic library along with necessary packages like SpaCy, Plotly, and Pandas. It also downloads a small English language model for SpaCy.\n\n```shell\npip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm\n```\n\n--------------------------------\n\n### Install Turftopic and Dependencies\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/tutorials/arxiv_ml.md\n\nInstalls the necessary Python libraries for the project, including `datasets`, `plotly`, and `turftopic` with optional dependencies for UMAP-based clustering and datamapplot.\n\n```bash\npip install datasets plotly turftopic[umap-learn, datamapplot]\n```\n\n--------------------------------\n\n### Run Full Test Suite with Pytest\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/CONTRIBUTING.md\n\nExecutes the entire test suite for the Turftopic project using pytest. Ensure you have pytest installed and are in the project's root directory.\n\n```console\npytest tests/\n```\n\n--------------------------------\n\n### Install Turftopic\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/README.md\n\nInstalls the turftopic library from PyPI. Includes optional dependencies for specific functionalities like CTMs (using Pyro) or clustering models (using UMAP).\n\n```bash\npip install turftopic\n\n```\n\n```bash\npip install \"turftopic[pyro-ppl]\"\n\n```\n\n```bash\npip install \"turftopic[umap-learn]\"\n\n```\n\n--------------------------------\n\n### Install MTEB and Initialize KeyNMF with MTEB Encoder\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/multimodal.md\n\nInstalls the MTEB library and initializes the KeyNMF model using an encoder compatible with the MTEB multimodal encoder interface. This enables topic modeling on multimodal data. Requires installation of MTEB.\n\n```bash\npip install \"mteb<2.0.0\"\n```\n\n```python\nfrom turftopic import KeyNMF\nimport mteb\n\nencoder = mteb.get_model(\"kakaobrain/align-base\")\n\nmultimodal_keynmf = KeyNMF(10, encoder=\"clip-ViT-B-32\")\n```\n\n--------------------------------\n\n### Install Turftopic with Datamapplot (Bash)\n\nSource: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/model_interpretation.md\n\nInstalls the turftopic library with the datamapplot extra, enabling interactive cluster visualizations. This command is run in the terminal.\n\n```bash\npip install turftopic[datamapplot]\n```",
            "codeBlocks": [
              {
                "language": "bash",
                "code": "pip install turftopic",
                "context": "Source: https://github.com/x-tabdeveloping/turftopic/blob/main/docs/index.md\n\nInstalls the turftopic library using pip. This is the first step to using Turftopic in your Python projects."
              },
              {
                "language": "python",
                "code": "from turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorspus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()",
                "context": "ex.md\n\nDemonstrates how to use the KeyNMF model from Turftopic. It fetches the 20Newsgroups dataset, trains a KeyNMF model, and prints the topics. This example assumes familiarity with scikit-learn."
              },
              {
                "language": "console",
                "code": "pip install turftopic[dev]",
                "context": "g/turftopic/blob/main/CONTRIBUTING.md\n\nInstalls all necessary development dependencies for the Turftopic project. This command assumes you have pip installed and are in the project's root directory."
              },
              {
                "language": "bash",
                "code": "pip install datasets plotly pandas turftopic",
                "context": "ideologies.md\n\nInstalls the Turftopic library along with Plotly for visualization and the 'datasets' library for fetching data from Hugging Face Hub. This is a prerequisite for running the tutorial."
              },
              {
                "language": "shell",
                "code": "pip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm",
                "context": "urftopic/blob/main/docs/tutorials/reviews.md\n\nInstalls the turftopic library along with necessary packages like SpaCy, Plotly, and Pandas. It also downloads a small English language model for SpaCy."
              },
              {
                "language": "bash",
                "code": "pip install datasets plotly turftopic[umap-learn, datamapplot]",
                "context": "/tutorials/arxiv_ml.md\n\nInstalls the necessary Python libraries for the project, including `datasets`, `plotly`, and `turftopic` with optional dependencies for UMAP-based clustering and datamapplot."
              },
              {
                "language": "console",
                "code": "pytest tests/",
                "context": "/x-tabdeveloping/turftopic/blob/main/CONTRIBUTING.md\n\nExecutes the entire test suite for the Turftopic project using pytest. Ensure you have pytest installed and are in the project's root directory."
              },
              {
                "language": "bash",
                "code": "pip install turftopic",
                "context": "eloping/turftopic/blob/main/README.md\n\nInstalls the turftopic library from PyPI. Includes optional dependencies for specific functionalities like CTMs (using Pyro) or clustering models (using UMAP)."
              },
              {
                "language": "bash",
                "code": "pip install \"turftopic[pyro-ppl]\"",
                "context": "pip install turftopic\n\n```"
              },
              {
                "language": "bash",
                "code": "pip install \"turftopic[umap-learn]\"",
                "context": "pip install \"turftopic[pyro-ppl]\"\n\n```"
              },
              {
                "language": "bash",
                "code": "pip install \"mteb<2.0.0\"",
                "context": "e MTEB library and initializes the KeyNMF model using an encoder compatible with the MTEB multimodal encoder interface. This enables topic modeling on multimodal data. Requires installation of MTEB."
              },
              {
                "language": "python",
                "code": "from turftopic import KeyNMF\nimport mteb\n\nencoder = mteb.get_model(\"kakaobrain/align-base\")\n\nmultimodal_keynmf = KeyNMF(10, encoder=\"clip-ViT-B-32\")",
                "context": "```bash\npip install \"mteb<2.0.0\"\n```"
              },
              {
                "language": "bash",
                "code": "pip install turftopic[datamapplot]",
                "context": "eloping/turftopic/blob/main/docs/model_interpretation.md\n\nInstalls the turftopic library with the datamapplot extra, enabling interactive cluster visualizations. This command is run in the terminal."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/ethereum/execution-spec-tests",
        "packageName": "execution-spec-tests",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:16.182Z",
        "content": "### Install 'uv' via curl\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nInstalls the latest version of 'uv' using a curl command. This method is recommended as it allows 'uv' to self-update. It may also download a compatible Python version if not found locally.\n\n```console\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n--------------------------------\n\n### Install 'uv' via pip\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nInstalls 'uv' using pip. This method requires an existing Python installation and does not support self-updating.\n\n```console\npip install uv\n```\n\n--------------------------------\n\n### Install Execution-Spec-Tests Dependencies (All Platforms)\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nClones the execution-spec-tests repository and installs its dependencies using 'uv'. It specifically configures Python 3.12 as the project's Python version and synchronizes all dependencies, including extras.\n\n```console\ngit clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.12\nuv python pin 3.12\nuv sync --all-extras\n```\n\n--------------------------------\n\n### View Generated Ethereum Test Fixture File\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/filling_tests/getting_started.md\n\nThis command displays the beginning of a generated JSON fixture file for an Ethereum state test. It uses the `head` command to show the initial lines of the file, allowing for a quick verification of the fixture's content and structure.\n\n```console\nhead fixtures/state_tests/shanghai/eip3855_push0/push0/push0_contract_during_call_contexts.json\n```\n\n--------------------------------\n\n### Setup Commands for Project Environment\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/CLAUDE.md\n\nInstalls project dependencies and sets up pre-commit hooks. Ensures the development environment is correctly configured.\n\n```bash\nuv sync --all-extras\nuvx pre-commit install\n```\n\n--------------------------------\n\n### Install Dependencies\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/dev/docs.md\n\nInstalls all necessary dependencies for the project, including extras. This command is typically run once to set up the development environment.\n\n```console\nuv sync --all-extras\n\n```\n\n--------------------------------\n\n### Installing Markdownlint CLI\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/code_standards_details.md\n\nCommands to install markdownlint-cli2 globally, which is required for the markdownlint tox environment. It suggests using apt for nodejs installation and npm for installing a specific version of markdownlint-cli2.\n\n```bash\nsudo apt install nodejs\nsudo npm install -g markdownlint-cli2@0.17.2\n\n```\n\n--------------------------------\n\n### Build Solidity (solc) from source on Ubuntu ARM64\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation_troubleshooting.md\n\nProvides a comprehensive set of commands to clone, build, and install the Solidity compiler (solc) from source on Ubuntu 24.04.2 LTS ARM64. This is a solution for 'solc' installation issues and 'failed to compile yul source' errors on ARM platforms when using the ethereum/execution-spec-tests.\n\n```bash\ngit clone --branch v0.8.28 --depth 1 https://github.com/ethereum/solidity.git\ncd solidity && mkdir build && cd build\nsudo apt install build-essential libboost-all-dev z3\ncmake ..\nmake\nmv $HOME/Documents/execution-spec-tests/.venv/bin/solc $HOME/Documents/execution-spec-tests/.venv/bin/solc-x86-64\ncp ./solc/solc $HOME/Documents/execution-spec-tests/.venv/bin/\nchmod +x $HOME/Documents/execution-spec-tests/.venv/bin/solc\n```\n\n--------------------------------\n\n### Install CA certificates on macOS\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation_troubleshooting.md\n\nExecutes a Python script on macOS to install necessary CA certificates, addressing potential SSL certificate verification failures during the setup or use of the ethereum/execution-spec-tests project. This command assumes Python 3.11 is installed.\n\n```bash\n/Applications/Python\\ 3.11/Install\\ Certificates.command\n```\n\n--------------------------------\n\n### Install Execution Spec Tests Dependencies (Console)\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/README.md\n\nInstalls the execution-spec-tests project and its dependencies using 'uv'. This involves cloning the repository, installing Python 3.11, pinning the version, and syncing all extras.\n\n```console\ngit clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.11\nuv python pin 3.11\nuv sync --all-extras\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install 'uv' via curl\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nInstalls the latest version of 'uv' using a curl command. This method is recommended as it allows 'uv' to self-update. It may also download a compatible Python version if not found locally.\n\n```console\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n--------------------------------\n\n### Install 'uv' via pip\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nInstalls 'uv' using pip. This method requires an existing Python installation and does not support self-updating.\n\n```console\npip install uv\n```\n\n--------------------------------\n\n### Install Execution-Spec-Tests Dependencies (All Platforms)\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nClones the execution-spec-tests repository and installs its dependencies using 'uv'. It specifically configures Python 3.12 as the project's Python version and synchronizes all dependencies, including extras.\n\n```console\ngit clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.12\nuv python pin 3.12\nuv sync --all-extras\n```\n\n--------------------------------\n\n### View Generated Ethereum Test Fixture File\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/filling_tests/getting_started.md\n\nThis command displays the beginning of a generated JSON fixture file for an Ethereum state test. It uses the `head` command to show the initial lines of the file, allowing for a quick verification of the fixture's content and structure.\n\n```console\nhead fixtures/state_tests/shanghai/eip3855_push0/push0/push0_contract_during_call_contexts.json\n```\n\n--------------------------------\n\n### Setup Commands for Project Environment\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/CLAUDE.md\n\nInstalls project dependencies and sets up pre-commit hooks. Ensures the development environment is correctly configured.\n\n```bash\nuv sync --all-extras\nuvx pre-commit install\n```\n\n--------------------------------\n\n### Install Dependencies\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/dev/docs.md\n\nInstalls all necessary dependencies for the project, including extras. This command is typically run once to set up the development environment.\n\n```console\nuv sync --all-extras\n\n```\n\n--------------------------------\n\n### Installing Markdownlint CLI\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/code_standards_details.md\n\nCommands to install markdownlint-cli2 globally, which is required for the markdownlint tox environment. It suggests using apt for nodejs installation and npm for installing a specific version of markdownlint-cli2.\n\n```bash\nsudo apt install nodejs\nsudo npm install -g markdownlint-cli2@0.17.2\n\n```\n\n--------------------------------\n\n### Build Solidity (solc) from source on Ubuntu ARM64\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation_troubleshooting.md\n\nProvides a comprehensive set of commands to clone, build, and install the Solidity compiler (solc) from source on Ubuntu 24.04.2 LTS ARM64. This is a solution for 'solc' installation issues and 'failed to compile yul source' errors on ARM platforms when using the ethereum/execution-spec-tests.\n\n```bash\ngit clone --branch v0.8.28 --depth 1 https://github.com/ethereum/solidity.git\ncd solidity && mkdir build && cd build\nsudo apt install build-essential libboost-all-dev z3\ncmake ..\nmake\nmv $HOME/Documents/execution-spec-tests/.venv/bin/solc $HOME/Documents/execution-spec-tests/.venv/bin/solc-x86-64\ncp ./solc/solc $HOME/Documents/execution-spec-tests/.venv/bin/\nchmod +x $HOME/Documents/execution-spec-tests/.venv/bin/solc\n```\n\n--------------------------------\n\n### Install CA certificates on macOS\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation_troubleshooting.md\n\nExecutes a Python script on macOS to install necessary CA certificates, addressing potential SSL certificate verification failures during the setup or use of the ethereum/execution-spec-tests project. This command assumes Python 3.11 is installed.\n\n```bash\n/Applications/Python\\ 3.11/Install\\ Certificates.command\n```\n\n--------------------------------\n\n### Install Execution Spec Tests Dependencies (Console)\n\nSource: https://github.com/ethereum/execution-spec-tests/blob/main/README.md\n\nInstalls the execution-spec-tests project and its dependencies using 'uv'. This involves cloning the repository, installing Python 3.11, pinning the version, and syncing all extras.\n\n```console\ngit clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.11\nuv python pin 3.11\nuv sync --all-extras\n```",
            "codeBlocks": [
              {
                "language": "console",
                "code": "curl -LsSf https://astral.sh/uv/install.sh | sh",
                "context": "tion.md\n\nInstalls the latest version of 'uv' using a curl command. This method is recommended as it allows 'uv' to self-update. It may also download a compatible Python version if not found locally."
              },
              {
                "language": "console",
                "code": "pip install uv",
                "context": "ub.com/ethereum/execution-spec-tests/blob/main/docs/getting_started/installation.md\n\nInstalls 'uv' using pip. This method requires an existing Python installation and does not support self-updating."
              },
              {
                "language": "console",
                "code": "git clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.12\nuv python pin 3.12\nuv sync --all-extras",
                "context": "execution-spec-tests repository and installs its dependencies using 'uv'. It specifically configures Python 3.12 as the project's Python version and synchronizes all dependencies, including extras."
              },
              {
                "language": "console",
                "code": "head fixtures/state_tests/shanghai/eip3855_push0/push0/push0_contract_during_call_contexts.json",
                "context": "a generated JSON fixture file for an Ethereum state test. It uses the `head` command to show the initial lines of the file, allowing for a quick verification of the fixture's content and structure."
              },
              {
                "language": "bash",
                "code": "uv sync --all-extras\nuvx pre-commit install",
                "context": "Source: https://github.com/ethereum/execution-spec-tests/blob/main/CLAUDE.md\n\nInstalls project dependencies and sets up pre-commit hooks. Ensures the development environment is correctly configured."
              },
              {
                "language": "console",
                "code": "uv sync --all-extras",
                "context": "reum/execution-spec-tests/blob/main/docs/dev/docs.md\n\nInstalls all necessary dependencies for the project, including extras. This command is typically run once to set up the development environment."
              },
              {
                "language": "bash",
                "code": "sudo apt install nodejs\nsudo npm install -g markdownlint-cli2@0.17.2",
                "context": "tall markdownlint-cli2 globally, which is required for the markdownlint tox environment. It suggests using apt for nodejs installation and npm for installing a specific version of markdownlint-cli2."
              },
              {
                "language": "bash",
                "code": "git clone --branch v0.8.28 --depth 1 https://github.com/ethereum/solidity.git\ncd solidity && mkdir build && cd build\nsudo apt install build-essential libboost-all-dev z3\ncmake ..\nmake\nmv $HOME/Documents/execution-spec-tests/.venv/bin/solc $HOME/Documents/execution-spec-tests/.venv/bin/solc-x86-64\ncp ./solc/solc $HOME/Documents/execution-spec-tests/.venv/bin/\nchmod +x $HOME/Documents/execution-spec-tests/.venv/bin/solc",
                "context": "c) from source on Ubuntu 24.04.2 LTS ARM64. This is a solution for 'solc' installation issues and 'failed to compile yul source' errors on ARM platforms when using the ethereum/execution-spec-tests."
              },
              {
                "language": "bash",
                "code": "/Applications/Python\\ 3.11/Install\\ Certificates.command",
                "context": "ssary CA certificates, addressing potential SSL certificate verification failures during the setup or use of the ethereum/execution-spec-tests project. This command assumes Python 3.11 is installed."
              },
              {
                "language": "console",
                "code": "git clone https://github.com/ethereum/execution-spec-tests\ncd execution-spec-tests\nuv python install 3.11\nuv python pin 3.11\nuv sync --all-extras",
                "context": "/main/README.md\n\nInstalls the execution-spec-tests project and its dependencies using 'uv'. This involves cloning the repository, installing Python 3.11, pinning the version, and syncing all extras."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/test-prof/test-prof",
        "packageName": "test-prof",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:19.931Z",
        "content": "### Configure TestProf Global Settings (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nDemonstrates how to configure global settings for TestProf using `TestProf.configure`. Includes examples for setting output directory, enabling timestamps, enabling color output, and specifying a logger.\n\n```ruby\nTestProf.configure do |config|\n  # the directory to put artifacts (reports) in ('tmp/test_prof' by default)\n  config.output_dir = \"tmp/test_prof\"\n\n  # use unique filenames for reports (by simply appending current timestamp)\n  config.timestamps = true\n\n  # color output\n  config.color = true\n\n  # where to write logs (defaults)\n  config.output = $stdout\n\n  # alternatively, you can specify a custom logger instance\n  config.logger = MyLogger.new\nend\n```\n\n--------------------------------\n\n### Add test-prof Gem to Gemfile (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nAdds the `test-prof` gem to the `:test` group in the Gemfile, specifying a version constraint. This is the first step for installation.\n\n```ruby\ngroup :test do\n  gem \"test-prof\", \"~> 1.0\"\nend\n```\n\n--------------------------------\n\n### Equivalent Setup for Basic let_it_be using before_all and let in Ruby\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/let_it_be.md\n\nThis code shows the equivalent setup using `before_all` and `let` hooks for a basic `let_it_be` call (like `let_it_be(:user) { create(:user) }`). It initializes a user before the context and then defines a `let` helper to find that user by ID within examples.\n\n```Ruby\nbefore_all { @user = create(:user) }\nlet(:user) { User.find(@user.id) }\n```\n\n--------------------------------\n\n### Using RSpec `before(:each)` for Setup (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nDemonstrates using RSpec's `before(:each)` hook to create test data before each example. This approach ensures test isolation but can be slow due to repeated setup.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before(:each) do\n    @paul = create(:beatle, name: \"Paul\")\n    @ringo = create(:beatle, name: \"Ringo\")\n    @george = create(:beatle, name: \"George\")\n    @john = create(:beatle, name: \"John\")\n  end\n\n  # and about 15 examples here\nend\n```\n\n--------------------------------\n\n### Using RSpec `before(:all)` for Setup (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nShows how to use RSpec's `before(:all)` hook to create test data once before the entire example group. While faster than `before(:each)`, it requires manual database cleanup afterward.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before(:all) do\n    @paul = create(:beatle, name: \"Paul\")\n    # ...\n  end\n\n  # ...\nend\n```\n\n--------------------------------\n\n### Profile RSpec Setup Hooks with RSpecDissect (Shell)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/playbook.md\n\nRun RSpec tests with RD_PROF=1 to use RSpecDissect, which measures the time spent in `let` and `before` hooks compared to the actual example execution time.\n\n```sh\nRD_PROF=1 bin/rspec\n```\n\n--------------------------------\n\n### Example MemoryProf Output\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/profilers/memory_prof.md\n\nIllustrates the typical output format of MemoryProf, showing top groups and examples by RSS memory usage with their respective memory increases and percentages.\n\n```sh\n[TEST PROF INFO] MemoryProf results\n\nFinal RSS: 673KB\n\nTop 5 groups (by RSS):\n\nAnswersController (./spec/controllers/answers_controller_spec.rb:3)  +80KB (13.50%)\nQuestionsController (./spec/controllers/questions_controller_spec.rb:3)  +32KB  (9.08%)\nCommentsController (./spec/controllers/comments_controller_spec.rb:3)  +16KB (3.27%)\n\nTop 5 examples (by RSS):\n\ndestroys question (./spec/controllers/questions_controller_spec.rb:38)  +144KB (24.38%)\nchange comments count (./spec/controllers/comments_controller_spec.rb:7)  +120KB (20.00%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +90KB (16.36%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +64KB (12.86%)\nfails (./spec/shared_examples/controllers/invalid_examples.rb:3)  +32KB (5.00%)\n```\n\n--------------------------------\n\n### Generate TestProf Reports with Custom Suffixes (Shell)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nShows how to use the `TEST_PROF_REPORT` environment variable to add a custom suffix to generated TestProf report filenames. This is useful for comparing reports from different test runs or configurations.\n\n```sh\n# Generate first report using `-with-bootsnap` suffix\n$ TEST_STACK_PROF=boot TEST_PROF_REPORT=with-bootsnap bundle exec rake\n$ #=> StackProf report generated: tmp/test_prof/stack-prof-report-wall-raw-boot-with-bootsnap.dump\n\n# Assume that you disabled bootsnap and want to generate a new report\n$ TEST_STACK_PROF=boot TEST_PROF_REPORT=no-bootsnap bundle exec rake\n$ #=> StackProf report generated: tmp/test_prof/stack-prof-report-wall-raw-boot-no-bootsnap.dump\n```\n\n--------------------------------\n\n### Example Factory Cascade (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/profilers/factory_prof.md\n\nProvides a Ruby code example demonstrating how nested factory definitions and a create call result in a sequence of record creations, illustrating the concept of a factory cascade relevant to the FactoryFlame graph explanation.\n\n```ruby\nfactory :comment do\n  answer\n  author\nend\n\nfactory :answer do\n  question\n  author\nend\n\nfactory :question do\n  author\nend\n\ncreate(:comment) #=> creates 5 records\n\n# And the corresponding stack is:\n# [:comment, :answer, :question, :author, :author, :author]\n```\n\n--------------------------------\n\n### Using TestProf `before_all` in RSpec (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nIllustrates using TestProf's `before_all` hook in RSpec. This hook wraps the entire example group in a transaction, providing fast setup like `before(:all)` but with automatic rollback, simplifying database cleanup.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before_all do\n    @paul = create(:beatle, name: \"Paul\")\n    # ...\n  end\n\n  # ...\nend\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Configure TestProf Global Settings (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nDemonstrates how to configure global settings for TestProf using `TestProf.configure`. Includes examples for setting output directory, enabling timestamps, enabling color output, and specifying a logger.\n\n```ruby\nTestProf.configure do |config|\n  # the directory to put artifacts (reports) in ('tmp/test_prof' by default)\n  config.output_dir = \"tmp/test_prof\"\n\n  # use unique filenames for reports (by simply appending current timestamp)\n  config.timestamps = true\n\n  # color output\n  config.color = true\n\n  # where to write logs (defaults)\n  config.output = $stdout\n\n  # alternatively, you can specify a custom logger instance\n  config.logger = MyLogger.new\nend\n```\n\n--------------------------------\n\n### Add test-prof Gem to Gemfile (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nAdds the `test-prof` gem to the `:test` group in the Gemfile, specifying a version constraint. This is the first step for installation.\n\n```ruby\ngroup :test do\n  gem \"test-prof\", \"~> 1.0\"\nend\n```\n\n--------------------------------\n\n### Equivalent Setup for Basic let_it_be using before_all and let in Ruby\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/let_it_be.md\n\nThis code shows the equivalent setup using `before_all` and `let` hooks for a basic `let_it_be` call (like `let_it_be(:user) { create(:user) }`). It initializes a user before the context and then defines a `let` helper to find that user by ID within examples.\n\n```Ruby\nbefore_all { @user = create(:user) }\nlet(:user) { User.find(@user.id) }\n```\n\n--------------------------------\n\n### Using RSpec `before(:each)` for Setup (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nDemonstrates using RSpec's `before(:each)` hook to create test data before each example. This approach ensures test isolation but can be slow due to repeated setup.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before(:each) do\n    @paul = create(:beatle, name: \"Paul\")\n    @ringo = create(:beatle, name: \"Ringo\")\n    @george = create(:beatle, name: \"George\")\n    @john = create(:beatle, name: \"John\")\n  end\n\n  # and about 15 examples here\nend\n```\n\n--------------------------------\n\n### Using RSpec `before(:all)` for Setup (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nShows how to use RSpec's `before(:all)` hook to create test data once before the entire example group. While faster than `before(:each)`, it requires manual database cleanup afterward.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before(:all) do\n    @paul = create(:beatle, name: \"Paul\")\n    # ...\n  end\n\n  # ...\nend\n```\n\n--------------------------------\n\n### Profile RSpec Setup Hooks with RSpecDissect (Shell)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/playbook.md\n\nRun RSpec tests with RD_PROF=1 to use RSpecDissect, which measures the time spent in `let` and `before` hooks compared to the actual example execution time.\n\n```sh\nRD_PROF=1 bin/rspec\n```\n\n--------------------------------\n\n### Example MemoryProf Output\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/profilers/memory_prof.md\n\nIllustrates the typical output format of MemoryProf, showing top groups and examples by RSS memory usage with their respective memory increases and percentages.\n\n```sh\n[TEST PROF INFO] MemoryProf results\n\nFinal RSS: 673KB\n\nTop 5 groups (by RSS):\n\nAnswersController (./spec/controllers/answers_controller_spec.rb:3)  +80KB (13.50%)\nQuestionsController (./spec/controllers/questions_controller_spec.rb:3)  +32KB  (9.08%)\nCommentsController (./spec/controllers/comments_controller_spec.rb:3)  +16KB (3.27%)\n\nTop 5 examples (by RSS):\n\ndestroys question (./spec/controllers/questions_controller_spec.rb:38)  +144KB (24.38%)\nchange comments count (./spec/controllers/comments_controller_spec.rb:7)  +120KB (20.00%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +90KB (16.36%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +64KB (12.86%)\nfails (./spec/shared_examples/controllers/invalid_examples.rb:3)  +32KB (5.00%)\n```\n\n--------------------------------\n\n### Generate TestProf Reports with Custom Suffixes (Shell)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nShows how to use the `TEST_PROF_REPORT` environment variable to add a custom suffix to generated TestProf report filenames. This is useful for comparing reports from different test runs or configurations.\n\n```sh",
            "codeBlocks": [
              {
                "language": "ruby",
                "code": "TestProf.configure do |config|\n  # the directory to put artifacts (reports) in ('tmp/test_prof' by default)\n  config.output_dir = \"tmp/test_prof\"\n\n  # use unique filenames for reports (by simply appending current timestamp)\n  config.timestamps = true\n\n  # color output\n  config.color = true\n\n  # where to write logs (defaults)\n  config.output = $stdout\n\n  # alternatively, you can specify a custom logger instance\n  config.logger = MyLogger.new\nend",
                "context": "strates how to configure global settings for TestProf using `TestProf.configure`. Includes examples for setting output directory, enabling timestamps, enabling color output, and specifying a logger."
              },
              {
                "language": "ruby",
                "code": "group :test do\n  gem \"test-prof\", \"~> 1.0\"\nend",
                "context": "b.com/test-prof/test-prof/blob/master/docs/getting_started.md\n\nAdds the `test-prof` gem to the `:test` group in the Gemfile, specifying a version constraint. This is the first step for installation."
              },
              {
                "language": "Ruby",
                "code": "before_all { @user = create(:user) }\nlet(:user) { User.find(@user.id) }",
                "context": "let` hooks for a basic `let_it_be` call (like `let_it_be(:user) { create(:user) }`). It initializes a user before the context and then defines a `let` helper to find that user by ID within examples."
              },
              {
                "language": "ruby",
                "code": "describe BeatleWeightedSearchQuery do\n  before(:each) do\n    @paul = create(:beatle, name: \"Paul\")\n    @ringo = create(:beatle, name: \"Ringo\")\n    @george = create(:beatle, name: \"George\")\n    @john = create(:beatle, name: \"John\")\n  end\n\n  # and about 15 examples here\nend",
                "context": "aster/docs/recipes/before_all.md\n\nDemonstrates using RSpec's `before(:each)` hook to create test data before each example. This approach ensures test isolation but can be slow due to repeated setup."
              },
              {
                "language": "ruby",
                "code": "describe BeatleWeightedSearchQuery do\n  before(:all) do\n    @paul = create(:beatle, name: \"Paul\")\n    # ...\n  end\n\n  # ...\nend",
                "context": "efore_all.md\n\nShows how to use RSpec's `before(:all)` hook to create test data once before the entire example group. While faster than `before(:each)`, it requires manual database cleanup afterward."
              },
              {
                "language": "sh",
                "code": "RD_PROF=1 bin/rspec",
                "context": "f/test-prof/blob/master/docs/playbook.md\n\nRun RSpec tests with RD_PROF=1 to use RSpecDissect, which measures the time spent in `let` and `before` hooks compared to the actual example execution time."
              },
              {
                "language": "sh",
                "code": "[TEST PROF INFO] MemoryProf results\n\nFinal RSS: 673KB\n\nTop 5 groups (by RSS):\n\nAnswersController (./spec/controllers/answers_controller_spec.rb:3)  +80KB (13.50%)\nQuestionsController (./spec/controllers/questions_controller_spec.rb:3)  +32KB  (9.08%)\nCommentsController (./spec/controllers/comments_controller_spec.rb:3)  +16KB (3.27%)\n\nTop 5 examples (by RSS):\n\ndestroys question (./spec/controllers/questions_controller_spec.rb:38)  +144KB (24.38%)\nchange comments count (./spec/controllers/comments_controller_spec.rb:7)  +120KB (20.00%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +90KB (16.36%)\nchange Votes count (./spec/shared_examples/controllers/voted_examples.rb:23)  +64KB (12.86%)\nfails (./spec/shared_examples/controllers/invalid_examples.rb:3)  +32KB (5.00%)",
                "context": "master/docs/profilers/memory_prof.md\n\nIllustrates the typical output format of MemoryProf, showing top groups and examples by RSS memory usage with their respective memory increases and percentages."
              }
            ]
          },
          {
            "title": "Generate first report using `-with-bootsnap` suffix",
            "type": "other",
            "content": "$ TEST_STACK_PROF=boot TEST_PROF_REPORT=with-bootsnap bundle exec rake\n$ #=> StackProf report generated: tmp/test_prof/stack-prof-report-wall-raw-boot-with-bootsnap.dump",
            "codeBlocks": []
          },
          {
            "title": "Assume that you disabled bootsnap and want to generate a new report",
            "type": "other",
            "content": "$ TEST_STACK_PROF=boot TEST_PROF_REPORT=no-bootsnap bundle exec rake\n$ #=> StackProf report generated: tmp/test_prof/stack-prof-report-wall-raw-boot-no-bootsnap.dump\n```\n\n--------------------------------\n\n### Example Factory Cascade (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/profilers/factory_prof.md\n\nProvides a Ruby code example demonstrating how nested factory definitions and a create call result in a sequence of record creations, illustrating the concept of a factory cascade relevant to the FactoryFlame graph explanation.\n\n```ruby\nfactory :comment do\n  answer\n  author\nend\n\nfactory :answer do\n  question\n  author\nend\n\nfactory :question do\n  author\nend\n\ncreate(:comment) #=> creates 5 records",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Example Factory Cascade (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/profilers/factory_prof.md\n\nProvides a Ruby code example demonstrating how nested factory definitions and a create call result in a sequence of record creations, illustrating the concept of a factory cascade relevant to the FactoryFlame graph explanation.",
                "context": "$ TEST_STACK_PROF=boot TEST_PROF_REPORT=no-bootsnap bundle exec rake\n$ #=> StackProf report generated: tmp/test_prof/stack-prof-report-wall-raw-boot-no-bootsnap.dump"
              }
            ]
          },
          {
            "title": "[:comment, :answer, :question, :author, :author, :author]",
            "type": "other",
            "content": "```\n\n--------------------------------\n\n### Using TestProf `before_all` in RSpec (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nIllustrates using TestProf's `before_all` hook in RSpec. This hook wraps the entire example group in a transaction, providing fast setup like `before(:all)` but with automatic rollback, simplifying database cleanup.\n\n```ruby\ndescribe BeatleWeightedSearchQuery do\n  before_all do\n    @paul = create(:beatle, name: \"Paul\")\n    # ...\n  end\n\n  # ...\nend\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Using TestProf `before_all` in RSpec (Ruby)\n\nSource: https://github.com/test-prof/test-prof/blob/master/docs/recipes/before_all.md\n\nIllustrates using TestProf's `before_all` hook in RSpec. This hook wraps the entire example group in a transaction, providing fast setup like `before(:all)` but with automatic rollback, simplifying database cleanup.",
                "context": ""
              }
            ]
          }
        ]
      },
      {
        "packageId": "/clojure/test.check",
        "packageName": "test.check",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:23.620Z",
        "content": "### Running JVM Tests with Leiningen\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis command executes the JVM-based tests for the test.check project using Leiningen. It's a prerequisite for comprehensive testing and ensures the core Clojure functionality is working correctly.\n\n```Shell\nlein test\n```\n\n--------------------------------\n\n### Generating Optional Integers (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example demonstrates how to create a generator that can produce either a small integer or `nil` using `gen/one-of`. It combines `gen/small-integer` with `gen/return nil` to achieve this optionality.\n\n```Clojure\n(def int-or-nil (gen/one-of [gen/small-integer (gen/return nil)]))\n(gen/sample int-or-nil)\n;; => (nil 0 -2 nil nil 3 nil nil 4 2)\n```\n\n--------------------------------\n\n### Generating Integers within a Range (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example shows how to create a generator that produces integers inclusively between 5 and 9 using `gen/choose`. It then samples the generator to demonstrate the range of values produced.\n\n```Clojure\n(def five-through-nine (gen/choose 5 9))\n(gen/sample five-through-nine)\n;; => (6 5 9 5 7 7 6 9 7 9)\n```\n\n--------------------------------\n\n### Generating a Vector and an Element from It (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis advanced example uses `gen/bind` to create a generator where the second part depends on the first. It first generates a non-empty vector of small integers, then uses that vector to generate a tuple containing the vector itself and a random element chosen from it. This highlights `gen/bind`'s ability to create a new generator based on the value produced by another.\n\n```Clojure\n(def vector-and-elem (gen/bind (gen/not-empty (gen/vector gen/small-integer))\n                               #(gen/tuple (gen/return %) (gen/elements %))))\n(gen/sample vector-and-elem)\n;; =>([[-1] -1]\n;; => [[0] 0]\n;; => [[-1 -1] -1]\n;; => [[2 0 -2] 2]\n;; => [[0 1 1] 0]\n;; => [[-2 -3 -1 1] -1]\n;; => [[-1 2 -5] -5]\n;; => [[5 -7 -3 7] 5]\n;; => [[-1 2 2] 2]\n;; => [[-8 7 -3 -2 -6] -3])\n```\n\n--------------------------------\n\n### Running ClojureScript Tests with Leiningen\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis command compiles and runs the ClojureScript tests for test.check using Leiningen's cljsbuild plugin. It's essential for verifying the ClojureScript-specific parts of the library and requires Node.js for execution.\n\n```Shell\nlein cljsbuild once\n```\n\n--------------------------------\n\n### Sampling a Specified Number of Small Integers in Clojure\n\nSource: https://github.com/clojure/test.check/blob/master/doc/intro.md\n\nThis example extends basic sampling by specifying the number of values to generate. It uses `gen/sample` with `gen/small-integer` to produce 20 random integer values, illustrating how to control the quantity of generated data.\n\n```Clojure\n(gen/sample gen/small-integer 20)\n;; => (0 1 1 0 2 -4 0 5 -7 -8 4 5 3 11 -9 -4 6 -5 -3 0)\n```\n\n--------------------------------\n\n### Generating Sorted Sequences of Integers (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example shows how to generate a vector of small integers and then ensure it's sorted by applying the `sort` function using `gen/fmap`. This is a common pattern for generating structured data that adheres to specific properties.\n\n```Clojure\n;; apply the sort function to each generated vector\n(def sorted-vec (gen/fmap sort (gen/vector gen/small-integer)))\n(gen/sample sorted-vec)\n;; => (() (-1) (-2 -2) (-1 2 3) (-1 2 4) (-3 2 3 3 4) (1)\n;; => (-4 0 1 3 4 6) (-5 -4 -1 0 2 8) (1)\n```\n\n--------------------------------\n\n### Requiring Clojure Test.Check Generators\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis snippet demonstrates the standard way to require the `clojure.test.check.generators` namespace, aliasing it as `gen` for convenient use throughout the code. This setup is a prerequisite for all subsequent generator examples.\n\n```Clojure\n(require '[clojure.test.check.generators :as gen])\n```\n\n--------------------------------\n\n### Generating Even Positive Integers with Fmap (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example illustrates how to transform the output of one generator using `gen/fmap`. It takes natural numbers from `gen/nat` and applies a function to multiply them by 2, resulting in a generator for even, positive integers.\n\n```Clojure\n(def even-and-positive (gen/fmap #(* 2 %) gen/nat))\n(gen/sample even-and-positive 20)\n;; => (0 0 2 0 8 6 4 12 4 18 10 0 8 2 16 16 6 4 10 4)\n```\n\n--------------------------------\n\n### Running Self-Hosted ClojureScript Tests\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis script executes the self-hosted ClojureScript tests for test.check. These tests are crucial for ensuring compatibility and functionality in environments where ClojureScript runs without a JVM, typically requiring Node.js for execution.\n\n```Shell\nscript/test-self-host\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Running JVM Tests with Leiningen\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis command executes the JVM-based tests for the test.check project using Leiningen. It's a prerequisite for comprehensive testing and ensures the core Clojure functionality is working correctly.\n\n```Shell\nlein test\n```\n\n--------------------------------\n\n### Generating Optional Integers (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example demonstrates how to create a generator that can produce either a small integer or `nil` using `gen/one-of`. It combines `gen/small-integer` with `gen/return nil` to achieve this optionality.\n\n```Clojure\n(def int-or-nil (gen/one-of [gen/small-integer (gen/return nil)]))\n(gen/sample int-or-nil)\n;; => (nil 0 -2 nil nil 3 nil nil 4 2)\n```\n\n--------------------------------\n\n### Generating Integers within a Range (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example shows how to create a generator that produces integers inclusively between 5 and 9 using `gen/choose`. It then samples the generator to demonstrate the range of values produced.\n\n```Clojure\n(def five-through-nine (gen/choose 5 9))\n(gen/sample five-through-nine)\n;; => (6 5 9 5 7 7 6 9 7 9)\n```\n\n--------------------------------\n\n### Generating a Vector and an Element from It (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis advanced example uses `gen/bind` to create a generator where the second part depends on the first. It first generates a non-empty vector of small integers, then uses that vector to generate a tuple containing the vector itself and a random element chosen from it. This highlights `gen/bind`'s ability to create a new generator based on the value produced by another.\n\n```Clojure\n(def vector-and-elem (gen/bind (gen/not-empty (gen/vector gen/small-integer))\n                               #(gen/tuple (gen/return %) (gen/elements %))))\n(gen/sample vector-and-elem)\n;; =>([[-1] -1]\n;; => [[0] 0]\n;; => [[-1 -1] -1]\n;; => [[2 0 -2] 2]\n;; => [[0 1 1] 0]\n;; => [[-2 -3 -1 1] -1]\n;; => [[-1 2 -5] -5]\n;; => [[5 -7 -3 7] 5]\n;; => [[-1 2 2] 2]\n;; => [[-8 7 -3 -2 -6] -3])\n```\n\n--------------------------------\n\n### Running ClojureScript Tests with Leiningen\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis command compiles and runs the ClojureScript tests for test.check using Leiningen's cljsbuild plugin. It's essential for verifying the ClojureScript-specific parts of the library and requires Node.js for execution.\n\n```Shell\nlein cljsbuild once\n```\n\n--------------------------------\n\n### Sampling a Specified Number of Small Integers in Clojure\n\nSource: https://github.com/clojure/test.check/blob/master/doc/intro.md\n\nThis example extends basic sampling by specifying the number of values to generate. It uses `gen/sample` with `gen/small-integer` to produce 20 random integer values, illustrating how to control the quantity of generated data.\n\n```Clojure\n(gen/sample gen/small-integer 20)\n;; => (0 1 1 0 2 -4 0 5 -7 -8 4 5 3 11 -9 -4 6 -5 -3 0)\n```\n\n--------------------------------\n\n### Generating Sorted Sequences of Integers (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example shows how to generate a vector of small integers and then ensure it's sorted by applying the `sort` function using `gen/fmap`. This is a common pattern for generating structured data that adheres to specific properties.\n\n```Clojure\n;; apply the sort function to each generated vector\n(def sorted-vec (gen/fmap sort (gen/vector gen/small-integer)))\n(gen/sample sorted-vec)\n;; => (() (-1) (-2 -2) (-1 2 3) (-1 2 4) (-3 2 3 3 4) (1)\n;; => (-4 0 1 3 4 6) (-5 -4 -1 0 2 8) (1)\n```\n\n--------------------------------\n\n### Requiring Clojure Test.Check Generators\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis snippet demonstrates the standard way to require the `clojure.test.check.generators` namespace, aliasing it as `gen` for convenient use throughout the code. This setup is a prerequisite for all subsequent generator examples.\n\n```Clojure\n(require '[clojure.test.check.generators :as gen])\n```\n\n--------------------------------\n\n### Generating Even Positive Integers with Fmap (Clojure)\n\nSource: https://github.com/clojure/test.check/blob/master/doc/generator-examples.md\n\nThis example illustrates how to transform the output of one generator using `gen/fmap`. It takes natural numbers from `gen/nat` and applies a function to multiply them by 2, resulting in a generator for even, positive integers.\n\n```Clojure\n(def even-and-positive (gen/fmap #(* 2 %) gen/nat))\n(gen/sample even-and-positive 20)\n;; => (0 0 2 0 8 6 4 12 4 18 10 0 8 2 16 16 6 4 10 4)\n```\n\n--------------------------------\n\n### Running Self-Hosted ClojureScript Tests\n\nSource: https://github.com/clojure/test.check/blob/master/doc/development.md\n\nThis script executes the self-hosted ClojureScript tests for test.check. These tests are crucial for ensuring compatibility and functionality in environments where ClojureScript runs without a JVM, typically requiring Node.js for execution.\n\n```Shell\nscript/test-self-host\n```",
            "codeBlocks": [
              {
                "language": "Shell",
                "code": "lein test",
                "context": "This command executes the JVM-based tests for the test.check project using Leiningen. It's a prerequisite for comprehensive testing and ensures the core Clojure functionality is working correctly."
              },
              {
                "language": "Clojure",
                "code": "(def int-or-nil (gen/one-of [gen/small-integer (gen/return nil)]))\n(gen/sample int-or-nil)\n;; => (nil 0 -2 nil nil 3 nil nil 4 2)",
                "context": "example demonstrates how to create a generator that can produce either a small integer or `nil` using `gen/one-of`. It combines `gen/small-integer` with `gen/return nil` to achieve this optionality."
              },
              {
                "language": "Clojure",
                "code": "(def five-through-nine (gen/choose 5 9))\n(gen/sample five-through-nine)\n;; => (6 5 9 5 7 7 6 9 7 9)",
                "context": "les.md\n\nThis example shows how to create a generator that produces integers inclusively between 5 and 9 using `gen/choose`. It then samples the generator to demonstrate the range of values produced."
              },
              {
                "language": "Clojure",
                "code": "(def vector-and-elem (gen/bind (gen/not-empty (gen/vector gen/small-integer))\n                               #(gen/tuple (gen/return %) (gen/elements %))))\n(gen/sample vector-and-elem)\n;; =>([[-1] -1]\n;; => [[0] 0]\n;; => [[-1 -1] -1]\n;; => [[2 0 -2] 2]\n;; => [[0 1 1] 0]\n;; => [[-2 -3 -1 1] -1]\n;; => [[-1 2 -5] -5]\n;; => [[5 -7 -3 7] 5]\n;; => [[-1 2 2] 2]\n;; => [[-8 7 -3 -2 -6] -3])",
                "context": "at vector to generate a tuple containing the vector itself and a random element chosen from it. This highlights `gen/bind`'s ability to create a new generator based on the value produced by another."
              },
              {
                "language": "Shell",
                "code": "lein cljsbuild once",
                "context": "s and runs the ClojureScript tests for test.check using Leiningen's cljsbuild plugin. It's essential for verifying the ClojureScript-specific parts of the library and requires Node.js for execution."
              },
              {
                "language": "Clojure",
                "code": "(gen/sample gen/small-integer 20)\n;; => (0 1 1 0 2 -4 0 5 -7 -8 4 5 3 11 -9 -4 6 -5 -3 0)",
                "context": "ampling by specifying the number of values to generate. It uses `gen/sample` with `gen/small-integer` to produce 20 random integer values, illustrating how to control the quantity of generated data."
              },
              {
                "language": "Clojure",
                "code": ";; apply the sort function to each generated vector\n(def sorted-vec (gen/fmap sort (gen/vector gen/small-integer)))\n(gen/sample sorted-vec)\n;; => (() (-1) (-2 -2) (-1 2 3) (-1 2 4) (-3 2 3 3 4) (1)\n;; => (-4 0 1 3 4 6) (-5 -4 -1 0 2 8) (1)",
                "context": "a vector of small integers and then ensure it's sorted by applying the `sort` function using `gen/fmap`. This is a common pattern for generating structured data that adheres to specific properties."
              },
              {
                "language": "Clojure",
                "code": "(require '[clojure.test.check.generators :as gen])",
                "context": "tandard way to require the `clojure.test.check.generators` namespace, aliasing it as `gen` for convenient use throughout the code. This setup is a prerequisite for all subsequent generator examples."
              },
              {
                "language": "Clojure",
                "code": "(def even-and-positive (gen/fmap #(* 2 %) gen/nat))\n(gen/sample even-and-positive 20)\n;; => (0 0 2 0 8 6 4 12 4 18 10 0 8 2 16 16 6 4 10 4)",
                "context": "to transform the output of one generator using `gen/fmap`. It takes natural numbers from `gen/nat` and applies a function to multiply them by 2, resulting in a generator for even, positive integers."
              },
              {
                "language": "Shell",
                "code": "script/test-self-host",
                "context": "reScript tests for test.check. These tests are crucial for ensuring compatibility and functionality in environments where ClojureScript runs without a JVM, typically requiring Node.js for execution."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/elifnurdeniz/context7-test",
        "packageName": "context7-test",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:27.189Z",
        "content": "No code documentation available for this library. Try mode='info' for guides and tutorials.",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "No code documentation available for this library. Try mode='info' for guides and tutorials.",
            "codeBlocks": []
          }
        ]
      },
      {
        "packageId": "/super3001/arkui-xtest-corpus",
        "packageName": "arkui-xtest-corpus",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:30.753Z",
        "content": "### Define Test Suite using describe in TypeScript\n\nSource: https://context7.com/super3001/arkui-xtest-corpus/llms.txt\n\nThe `describe` function creates a test suite to group related test cases. It supports setup and teardown hooks like `beforeAll` and `afterAll` for managing test state. This example demonstrates basic setup, teardown, and assertion within a suite.\n\n```typescript\nimport { describe, it, expect, beforeAll, afterAll } from '@ohos/hypium';\n\nexport default function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    beforeAll(function () {\n      // Setup code runs once before all tests\n      console.info('test suite setup');\n    });\n\n    afterAll(function () {\n      // Cleanup code runs once after all tests\n      console.info('test suite teardown');\n    });\n\n    it('testCase001', 0, function () {\n      let result = 2 + 2;\n      expect(result).assertEqual(4);\n    });\n\n    it('testCase002', 0, function () {\n      let text = 'hello';\n      expect(text).assertContain('ell');\n    });\n  });\n}\n```\n\n--------------------------------\n\n### Mocking Functions with JsUnit (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nDemonstrates how to use JsUnit's MockKit to mock a class method and define its return value using `afterReturn`. This example shows the setup, mocking process, and assertion for a mocked function.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### JsUnit Basic Process Support Example (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis JavaScript code demonstrates the basic process support of JsUnit for writing and executing test cases. It utilizes `describe` to define a test suite and `it` to specify individual test cases. The example also shows the use of `expect` for assertions and an asynchronous test case involving `getBundleInfo` from the '@ohos.bundle' module.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Basic UiTest Example (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nDemonstrates a basic UI test case using UiTest. It initializes a UiDriver, finds a component by its text attribute, simulates a click on the component, and asserts that the component's text has changed. This example highlights the asynchronous nature of UiDriver and UiComponent APIs, requiring the use of 'await'.\n\n```javascript\nimport {describe, beforeAll, beforeEach, afterEach, afterAll, it, expect} from '@ohos/hypium'\nimport {BY, UiDriver, UiComponent, MatchPattern} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('uiTestDemo', function() {\n    it('uitest_demo0', 0, async function() {\n      // create UiDriver\n      let driver = await UiDriver.create()\n      // find component by text\n      let button = await driver.findComponent(BY.text('hello').enabled(true))\n      // click component\n      await button.click()\n      // get and assert component text\n      let content = await button.getText()\n      expect(content).assertEquals('clicked!')\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mocking Asynchronous Functions\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nProvides an example of how to mock asynchronous functions using MockKit. This snippet is intended to show the setup for handling promises or other asynchronous operations, though the specific implementation details for async mocking are not fully detailed in the provided text.\n\n```javascript\n// Example 10: Mock asynchronous functions.\n\n```\n\n--------------------------------\n\n### TypeScript - Test Setup and Teardown Hooks\n\nSource: https://context7.com/super3001/arkui-xtest-corpus/llms.txt\n\nDefines `beforeEach` and `afterEach` hooks for running code before and after each test case within a suite. These are useful for initializing test state or cleaning up resources. Dependencies include '@ohos/hypium'. They take callback functions as arguments.\n\n```typescript\nimport { describe, beforeEach, afterEach, it, expect } from '@ohos/hypium';\n\nexport default function hookTest() {\n  describe('HookTest', function () {\n    let testData: number;\n\n    beforeEach(function () {\n      // Runs before each test case\n      testData = 0;\n      console.info('beforeEach: testData initialized to 0');\n    });\n\n    afterEach(function () {\n      // Runs after each test case\n      console.info('afterEach: testData was ' + testData);\n      testData = -1;\n    });\n\n    it('test_increment', 0, function () {\n      testData += 1;\n      expect(testData).assertEqual(1);\n    });\n\n    it('test_double', 0, function () {\n      testData = 10;\n      testData *= 2;\n      expect(testData).assertEqual(20);\n    });\n  });\n}\n```\n\n--------------------------------\n\n### Mocking a System API with afterReturn\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis example illustrates how to mock a system API, specifically 'app.getInfo()', using MockKit. It demonstrates setting a return value for the mocked function and then asserting that the mocked function returns the expected value.\n\n```javascript\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_systemApi', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Mock the app.getInfo() function.\n            let mockf = mocker.mockFunc(app, app.getInfo);\n            when(mockf)('test').afterReturn('1');\n            // The operation is successful.\n            expect(app.getInfo('test')).assertEqual('1');\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Install arkXtest Dependency in oh-package.json5\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis JSON snippet shows how to add the '@ohos/hypium' dependency to your project's oh-package.json5 file to use the arkXtest framework. After adding the dependency, run 'ohpm install' to fetch the package.\n\n```json\n{\n  \"dependencies\": {\n    \"@ohos/hypium\": \"^1.0.0\"\n  }\n}\n```\n\n--------------------------------\n\n### Verify Mocked Function Calls (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nIllustrates how to mock multiple methods of a class and then use `mocker.verify()` to check if these methods were called with specific arguments and with a certain frequency. This example highlights verification of call counts.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos.hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n\n                method_2(...arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 and method_2 of the ClassName class.\n            mocker.mockFunc(claser, claser.method_1);\n            mocker.mockFunc(claser, claser.method_2);\n\n            // 4. Call the following methods.\n            claser.method_1('abc', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            claser.method_2('111');\n            claser.method_2('111', '222');\n\n            //5. Verify the mocked functions.\n            mocker.verify('method_1', []).atLeast(3); // The result is \"failed\".\n            // Verify whether 'method_1' with an empty parameter list was executed at least three times.\n            // The result is \"failed\" because 'method_1' with an empty parameter list was executed only twice in Step 4.\n            //mocker.verify('method_2',['111']).once(); // The result is \"success\".\n            //mocker.verify('method_2',['111',,'222']).once(); // The result is \"success\".\n        });\n    });\n}\n```\n\n--------------------------------\n\n### UiWindow Class\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThe UiWindow class represents a window object, providing APIs to get window attributes, drag windows, and adjust window sizes.\n\n```APIDOC\n## UiWindow Class\n\n### Description\nThe `UiWindow` class provides functionalities to manage and interact with application windows. You can obtain window properties, perform window manipulation actions like dragging, and resize windows.\n\n### Methods\n\n*   **`getAttributes(): Promise<object>`**\n    *   **Description**: Retrieves the attributes of the window.\n    *   **Returns**: A `Promise` that resolves to an object containing the window's attributes.\n\n*   **`drag(x1: number, y1: number, x2: number, y2: number): Promise<void>`**\n    *   **Description**: Drags the window from a starting point (x1, y1) to an ending point (x2, y2).\n    *   **Parameters**:\n        *   `x1` (number) - Required - The starting x-coordinate.\n        *   `y1` (number) - Required - The starting y-coordinate.\n        *   `x2` (number) - Required - The ending x-coordinate.\n        *   `y2` (number) - Required - The ending y-coordinate.\n\n*   **`resize(width: number, height: number): Promise<void>`**\n    *   **Description**: Resizes the window to the specified width and height.\n    *   **Parameters**:\n        *   `width` (number) - Required - The target width of the window.\n        *   `height` (number) - Required - The target height of the window.\n\n### Usage Example\n\n```javascript\nimport { UiDriver } from '@ohos.uitest';\n\n// Assuming 'driver' is an initialized UiDriver object\n// const driver = await UiDriver.create();\n\n// Define a filter to find a specific window (e.g., by title)\n// const windowFilter = { title: 'My App Window' };\n\n// Find the window\n// const myWindow = await driver.findWindow(windowFilter);\n\n// Interact with the window\n// await myWindow.drag(100, 100, 200, 200);\n// await myWindow.resize(800, 600);\n// const attributes = await myWindow.getAttributes();\n```\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Define Test Suite using describe in TypeScript\n\nSource: https://context7.com/super3001/arkui-xtest-corpus/llms.txt\n\nThe `describe` function creates a test suite to group related test cases. It supports setup and teardown hooks like `beforeAll` and `afterAll` for managing test state. This example demonstrates basic setup, teardown, and assertion within a suite.\n\n```typescript\nimport { describe, it, expect, beforeAll, afterAll } from '@ohos/hypium';\n\nexport default function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    beforeAll(function () {\n      // Setup code runs once before all tests\n      console.info('test suite setup');\n    });\n\n    afterAll(function () {\n      // Cleanup code runs once after all tests\n      console.info('test suite teardown');\n    });\n\n    it('testCase001', 0, function () {\n      let result = 2 + 2;\n      expect(result).assertEqual(4);\n    });\n\n    it('testCase002', 0, function () {\n      let text = 'hello';\n      expect(text).assertContain('ell');\n    });\n  });\n}\n```\n\n--------------------------------\n\n### Mocking Functions with JsUnit (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nDemonstrates how to use JsUnit's MockKit to mock a class method and define its return value using `afterReturn`. This example shows the setup, mocking process, and assertion for a mocked function.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}\n```\n\n--------------------------------\n\n### JsUnit Basic Process Support Example (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis JavaScript code demonstrates the basic process support of JsUnit for writing and executing test cases. It utilizes `describe` to define a test suite and `it` to specify individual test cases. The example also shows the use of `expect` for assertions and an asynchronous test case involving `getBundleInfo` from the '@ohos.bundle' module.\n\n```javascript\nimport { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Basic UiTest Example (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nDemonstrates a basic UI test case using UiTest. It initializes a UiDriver, finds a component by its text attribute, simulates a click on the component, and asserts that the component's text has changed. This example highlights the asynchronous nature of UiDriver and UiComponent APIs, requiring the use of 'await'.\n\n```javascript\nimport {describe, beforeAll, beforeEach, afterEach, afterAll, it, expect} from '@ohos/hypium'\nimport {BY, UiDriver, UiComponent, MatchPattern} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('uiTestDemo', function() {\n    it('uitest_demo0', 0, async function() {\n      // create UiDriver\n      let driver = await UiDriver.create()\n      // find component by text\n      let button = await driver.findComponent(BY.text('hello').enabled(true))\n      // click component\n      await button.click()\n      // get and assert component text\n      let content = await button.getText()\n      expect(content).assertEquals('clicked!')\n    })\n  })\n}\n```\n\n--------------------------------\n\n### Mocking Asynchronous Functions\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nProvides an example of how to mock asynchronous functions using MockKit. This snippet is intended to show the setup for handling promises or other asynchronous operations, though the specific implementation details for async mocking are not fully detailed in the provided text.\n\n```javascript\n// Example 10: Mock asynchronous functions.\n\n```\n\n--------------------------------\n\n### TypeScript - Test Setup and Teardown Hooks\n\nSource: https://context7.com/super3001/arkui-xtest-corpus/llms.txt\n\nDefines `beforeEach` and `afterEach` hooks for running code before and after each test case within a suite. These are useful for initializing test state or cleaning up resources. Dependencies include '@ohos/hypium'. They take callback functions as arguments.\n\n```typescript\nimport { describe, beforeEach, afterEach, it, expect } from '@ohos/hypium';\n\nexport default function hookTest() {\n  describe('HookTest', function () {\n    let testData: number;\n\n    beforeEach(function () {\n      // Runs before each test case\n      testData = 0;\n      console.info('beforeEach: testData initialized to 0');\n    });\n\n    afterEach(function () {\n      // Runs after each test case\n      console.info('afterEach: testData was ' + testData);\n      testData = -1;\n    });\n\n    it('test_increment', 0, function () {\n      testData += 1;\n      expect(testData).assertEqual(1);\n    });\n\n    it('test_double', 0, function () {\n      testData = 10;\n      testData *= 2;\n      expect(testData).assertEqual(20);\n    });\n  });\n}\n```\n\n--------------------------------\n\n### Mocking a System API with afterReturn\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis example illustrates how to mock a system API, specifically 'app.getInfo()', using MockKit. It demonstrates setting a return value for the mocked function and then asserting that the mocked function returns the expected value.\n\n```javascript\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_systemApi', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Mock the app.getInfo() function.\n            let mockf = mocker.mockFunc(app, app.getInfo);\n            when(mockf)('test').afterReturn('1');\n            // The operation is successful.\n            expect(app.getInfo('test')).assertEqual('1');\n        });\n    });\n}\n```\n\n--------------------------------\n\n### Install arkXtest Dependency in oh-package.json5\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThis JSON snippet shows how to add the '@ohos/hypium' dependency to your project's oh-package.json5 file to use the arkXtest framework. After adding the dependency, run 'ohpm install' to fetch the package.\n\n```json\n{\n  \"dependencies\": {\n    \"@ohos/hypium\": \"^1.0.0\"\n  }\n}\n```\n\n--------------------------------\n\n### Verify Mocked Function Calls (JavaScript)\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nIllustrates how to mock multiple methods of a class and then use `mocker.verify()` to check if these methods were called with specific arguments and with a certain frequency. This example highlights verification of call counts.\n\n```javascript\nimport {describe, expect, it, MockKit, when} from '@ohos.hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n\n                method_2(...arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 and method_2 of the ClassName class.\n            mocker.mockFunc(claser, claser.method_1);\n            mocker.mockFunc(claser, claser.method_2);\n\n            // 4. Call the following methods.\n            claser.method_1('abc', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            claser.method_2('111');\n            claser.method_2('111', '222');\n\n            //5. Verify the mocked functions.\n            mocker.verify('method_1', []).atLeast(3); // The result is \"failed\".\n            // Verify whether 'method_1' with an empty parameter list was executed at least three times.\n            // The result is \"failed\" because 'method_1' with an empty parameter list was executed only twice in Step 4.\n            //mocker.verify('method_2',['111']).once(); // The result is \"success\".\n            //mocker.verify('method_2',['111',,'222']).once(); // The result is \"success\".\n        });\n    });\n}\n```\n\n--------------------------------\n\n### UiWindow Class\n\nSource: https://github.com/super3001/arkui-xtest-corpus/blob/main/README.md\n\nThe UiWindow class represents a window object, providing APIs to get window attributes, drag windows, and adjust window sizes.\n\n```APIDOC",
            "codeBlocks": [
              {
                "language": "typescript",
                "code": "import { describe, it, expect, beforeAll, afterAll } from '@ohos/hypium';\n\nexport default function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    beforeAll(function () {\n      // Setup code runs once before all tests\n      console.info('test suite setup');\n    });\n\n    afterAll(function () {\n      // Cleanup code runs once after all tests\n      console.info('test suite teardown');\n    });\n\n    it('testCase001', 0, function () {\n      let result = 2 + 2;\n      expect(result).assertEqual(4);\n    });\n\n    it('testCase002', 0, function () {\n      let text = 'hello';\n      expect(text).assertContain('ell');\n    });\n  });\n}",
                "context": "group related test cases. It supports setup and teardown hooks like `beforeAll` and `afterAll` for managing test state. This example demonstrates basic setup, teardown, and assertion within a suite."
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when} from '@ohos/hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(arg) {\n                    return '888888';\n                }\n\n                method_2(arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 of the ClassName class.\n            let mockfunc = mocker.mockFunc(claser, claser.method_1);\n            when(mockfunc)('test').afterReturn('1');\n\n            // 4. Assert whether the mocked function is implemented as expected.\n            // The operation is successful if 'test' is passed in.\n            expect(claser.method_1('test')).assertEqual('1'); // The operation is successful.\n\n            // The operation fails if 'abc' is passed in.\n            //expect(claser.method_1('abc')).assertEqual('1'); // The operation fails.\n        });\n    });\n}",
                "context": "Demonstrates how to use JsUnit's MockKit to mock a class method and define its return value using `afterReturn`. This example shows the setup, mocking process, and assertion for a mocked function."
              },
              {
                "language": "javascript",
                "code": "import { describe, beforeAll, beforeEach, afterEach, afterAll, it, expect } from '@ohos/hypium'\nimport demo from '@ohos.bundle'\n\nexport default async function abilityTest() {\n  describe('ActsAbilityTest', function () {\n    it('String_assertContain_success', 0, function () {\n      let a = 'abc'\n      let b = 'b'\n      expect(a).assertContain(b)\n      expect(a).assertEqual(a)\n    })\n    it('getBundleInfo_0100', 0, async function () {\n      const NAME1 = \"com.example.MyApplicationStage\"\n      await demo.getBundleInfo(NAME1,\n        demo.BundleFlag.GET_BUNDLE_WITH_ABILITIES | demo.BundleFlag.GET_BUNDLE_WITH_REQUESTED_PERMISSION)\n        .then((value) => {\n          console.info(value.appId)\n        })\n        .catch((err) => {\n          console.info(err.code)\n        })\n    })\n  })\n}",
                "context": "est suite and `it` to specify individual test cases. The example also shows the use of `expect` for assertions and an asynchronous test case involving `getBundleInfo` from the '@ohos.bundle' module."
              },
              {
                "language": "javascript",
                "code": "import {describe, beforeAll, beforeEach, afterEach, afterAll, it, expect} from '@ohos/hypium'\nimport {BY, UiDriver, UiComponent, MatchPattern} from '@ohos.uitest'\n\nexport default async function abilityTest() {\n  describe('uiTestDemo', function() {\n    it('uitest_demo0', 0, async function() {\n      // create UiDriver\n      let driver = await UiDriver.create()\n      // find component by text\n      let button = await driver.findComponent(BY.text('hello').enabled(true))\n      // click component\n      await button.click()\n      // get and assert component text\n      let content = await button.getText()\n      expect(content).assertEquals('clicked!')\n    })\n  })\n}",
                "context": "simulates a click on the component, and asserts that the component's text has changed. This example highlights the asynchronous nature of UiDriver and UiComponent APIs, requiring the use of 'await'."
              },
              {
                "language": "javascript",
                "code": "// Example 10: Mock asynchronous functions.",
                "context": "nippet is intended to show the setup for handling promises or other asynchronous operations, though the specific implementation details for async mocking are not fully detailed in the provided text."
              },
              {
                "language": "typescript",
                "code": "import { describe, beforeEach, afterEach, it, expect } from '@ohos/hypium';\n\nexport default function hookTest() {\n  describe('HookTest', function () {\n    let testData: number;\n\n    beforeEach(function () {\n      // Runs before each test case\n      testData = 0;\n      console.info('beforeEach: testData initialized to 0');\n    });\n\n    afterEach(function () {\n      // Runs after each test case\n      console.info('afterEach: testData was ' + testData);\n      testData = -1;\n    });\n\n    it('test_increment', 0, function () {\n      testData += 1;\n      expect(testData).assertEqual(1);\n    });\n\n    it('test_double', 0, function () {\n      testData = 10;\n      testData *= 2;\n      expect(testData).assertEqual(20);\n    });\n  });\n}",
                "context": "before and after each test case within a suite. These are useful for initializing test state or cleaning up resources. Dependencies include '@ohos/hypium'. They take callback functions as arguments."
              },
              {
                "language": "javascript",
                "code": "export default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('test_systemApi', 0, function () {\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n            // 2. Mock the app.getInfo() function.\n            let mockf = mocker.mockFunc(app, app.getInfo);\n            when(mockf)('test').afterReturn('1');\n            // The operation is successful.\n            expect(app.getInfo('test')).assertEqual('1');\n        });\n    });\n}",
                "context": "mock a system API, specifically 'app.getInfo()', using MockKit. It demonstrates setting a return value for the mocked function and then asserting that the mocked function returns the expected value."
              },
              {
                "language": "json",
                "code": "{\n  \"dependencies\": {\n    \"@ohos/hypium\": \"^1.0.0\"\n  }\n}",
                "context": "ON snippet shows how to add the '@ohos/hypium' dependency to your project's oh-package.json5 file to use the arkXtest framework. After adding the dependency, run 'ohpm install' to fetch the package."
              },
              {
                "language": "javascript",
                "code": "import {describe, expect, it, MockKit, when} from '@ohos.hypium';\n\nexport default function ActsAbilityTest() {\n    describe('ActsAbilityTest', function () {\n        it('testMockfunc', 0, function () {\n            console.info(\"it1 begin\");\n\n            // 1. Create a MockKit object.\n            let mocker = new MockKit();\n\n            // 2. Define the ClassName class, which contains two functions, and then create a claser object.\n            class ClassName {\n                constructor() {\n                }\n\n                method_1(...arg) {\n                    return '888888';\n                }\n\n                method_2(...arg) {\n                    return '999999';\n                }\n            }\n\n            let claser = new ClassName();\n\n            // 3. Mock method_1 and method_2 of the ClassName class.\n            mocker.mockFunc(claser, claser.method_1);\n            mocker.mockFunc(claser, claser.method_2);\n\n            // 4. Call the following methods.\n            claser.method_1('abc', 'ppp');\n            claser.method_1('abc');\n            claser.method_1('xyz');\n            claser.method_1();\n            claser.method_1('abc', 'xxx', 'yyy');\n            claser.method_1();\n            claser.method_2('111');\n            claser.method_2('111', '222');\n\n            //5. Verify the mocked functions.\n            mocker.verify('method_1', []).atLeast(3); // The result is \"failed\".\n            // Verify whether 'method_1' with an empty parameter list was executed at least three times.\n            // The result is \"failed\" because 'method_1' with an empty parameter list was executed only twice in Step 4.\n            //mocker.verify('method_2',['111']).once(); // The result is \"success\".\n            //mocker.verify('method_2',['111',,'222']).once(); // The result is \"success\".\n        });\n    });\n}",
                "context": "ple methods of a class and then use `mocker.verify()` to check if these methods were called with specific arguments and with a certain frequency. This example highlights verification of call counts."
              }
            ]
          },
          {
            "title": "UiWindow Class",
            "type": "other",
            "content": "### Description\nThe `UiWindow` class provides functionalities to manage and interact with application windows. You can obtain window properties, perform window manipulation actions like dragging, and resize windows.\n\n### Methods\n\n*   **`getAttributes(): Promise<object>`**\n    *   **Description**: Retrieves the attributes of the window.\n    *   **Returns**: A `Promise` that resolves to an object containing the window's attributes.\n\n*   **`drag(x1: number, y1: number, x2: number, y2: number): Promise<void>`**\n    *   **Description**: Drags the window from a starting point (x1, y1) to an ending point (x2, y2).\n    *   **Parameters**:\n        *   `x1` (number) - Required - The starting x-coordinate.\n        *   `y1` (number) - Required - The starting y-coordinate.\n        *   `x2` (number) - Required - The ending x-coordinate.\n        *   `y2` (number) - Required - The ending y-coordinate.\n\n*   **`resize(width: number, height: number): Promise<void>`**\n    *   **Description**: Resizes the window to the specified width and height.\n    *   **Parameters**:\n        *   `width` (number) - Required - The target width of the window.\n        *   `height` (number) - Required - The target height of the window.\n\n### Usage Example\n\n```javascript\nimport { UiDriver } from '@ohos.uitest';\n\n// Assuming 'driver' is an initialized UiDriver object\n// const driver = await UiDriver.create();\n\n// Define a filter to find a specific window (e.g., by title)\n// const windowFilter = { title: 'My App Window' };\n\n// Find the window\n// const myWindow = await driver.findWindow(windowFilter);\n\n// Interact with the window\n// await myWindow.drag(100, 100, 200, 200);\n// await myWindow.resize(800, 600);\n// const attributes = await myWindow.getAttributes();\n```\n```",
            "codeBlocks": [
              {
                "language": "javascript",
                "code": "import { UiDriver } from '@ohos.uitest';\n\n// Assuming 'driver' is an initialized UiDriver object\n// const driver = await UiDriver.create();\n\n// Define a filter to find a specific window (e.g., by title)\n// const windowFilter = { title: 'My App Window' };\n\n// Find the window\n// const myWindow = await driver.findWindow(windowFilter);\n\n// Interact with the window\n// await myWindow.drag(100, 100, 200, 200);\n// await myWindow.resize(800, 600);\n// const attributes = await myWindow.getAttributes();",
                "context": "        *   `height` (number) - Required - The target height of the window.\n\n### Usage Example"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/r-lib/testthat",
        "packageName": "testthat",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:34.298Z",
        "content": "### APIDOC: `testthat` Setup and Teardown Hooks\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nDocumentation for functions and file prefixes that enable setup and teardown operations for test files.\n\n```APIDOC\nsetup(): Function run at the start of each test file, useful for initializing state.\n```\n\n```APIDOC\nteardown(): Function run at the end of each test file, useful for cleaning up state.\n```\n\n```APIDOC\ntest/setup files: Files starting with `setup` in the `test/` directory are run before tests (unlike `helpers`, not run in `devtools::load_all()`).\n```\n\n```APIDOC\ntest/teardown files: Files starting with `teardown` in the `test/` directory are run after all tests are completed.\n```\n\n--------------------------------\n\n### Install testthat R Package\n\nSource: https://github.com/r-lib/testthat/blob/main/README.md\n\nInstructions for installing the testthat package, either the released version from CRAN or the development version from GitHub using the pak package.\n\n```R\n# Install the released version from CRAN\ninstall.packages(\"testthat\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"r-lib/testthat\")\n```\n\n--------------------------------\n\n### R Package `ieegio` Example Execution Error\n\nSource: https://github.com/r-lib/testthat/blob/main/revdep/problems.md\n\nOutput from `R CMD check` showing an error during the execution of examples for the `ieegio` package. The error occurred in `ieegio-Ex.R` and is related to the inability to find the `conda` binary, suggesting issues with `Miniconda` or `Conda Environments` installation during the check.\n\n```R Error Log\nRunning examples in ieegio-Ex.R failed\nThe error most likely occurred in:\n\n> ### Name: imaging-volume\n> ### Title: Read and write volume data\n> ### Aliases: imaging-volume read_volume write_volume io_read_mgz\n> ###   io_write_mgz io_write_mgz.ieegio_volume io_write_mgz.ieegio_mgh\n> ###   io_write_mgz.nifti io_write_mgz.niftiImage\n> ###   io_write_mgz.ants.core.ants_image.ANTsImage io_write_mgz.array\n> ###   io_read_nii io_write_nii io_write_nii.ieegio_nifti\n...\n+ # clean up\n+ unlink(f)\n+ unlink(f2)\n+ \n+ }\n<simpleWarning in check_forbidden_install(\"Miniconda\"): cannot install Miniconda during R CMD check>\nWarning in check_forbidden_install(\"Conda Environments\") :\n  cannot install Conda Environments during R CMD check\nError: Unable to find conda binary. Is Anaconda installed?\nExecution halted\n```\n\n--------------------------------\n\n### SummaryReporter Update\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\n`SummaryReporter` now records the file start time, not just the context start, which improves compatibility with modern testing styles that do not rely on `context()`.\n\n```APIDOC\nSummaryReporter:\n  Description: A reporter that provides a summary.\n  Improvements:\n    - Records file start, not just context start.\n    - More compatible with modern style which does not use `context()`.\n```\n\n--------------------------------\n\n### Perform Basic Arithmetic in R Console\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-verify-output.txt\n\nDemonstrates a simple addition operation within an R console session, including the input prompt, the code, and the resulting output.\n\n```R\n> # Output\n> 1 + 2\n[1] 3\n```\n\n--------------------------------\n\n### Informative Hints for Accepting testthat Snapshots\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/_snaps/snapshot.md\n\nProvides examples of the helpful hints generated by `testthat` for accepting or reviewing snapshot changes, including RStudio IDE links. These hints streamline the snapshot management workflow.\n\n```R\nCode:\n  cat(snapshot_accept_hint(\"_default\", \"bar.R\", reset_output = FALSE))\nOutput:\n  * Run  ]8;;ide:run:testthat::snapshot\n```\n\n--------------------------------\n\n### testthat New Utility Functions\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThis section highlights new functions that simplify accessing package examples and improve mocking capabilities. `local_mock()` offers a more streamlined approach to mocking compared to `with_mock()`.\n\n```APIDOC\ntestthat_examples():\n  Description: Provides easy access to new test files bundled with the package.\n\ntestthat_example():\n  Description: Provides easy access to new test files bundled with the package.\n\nlocal_mock():\n  Description: Allows mocking a function without requiring additional indentation.\n  Comparison: Offers a more convenient alternative to `with_mock()`.\n```\n\n--------------------------------\n\n### Generate a Sequence of Numbers in R\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-parallel/snap/tests/testthat/_snaps/snap-1.md\n\nThis snippet demonstrates how to generate a sequence of integers from a starting number to an ending number in R using the colon operator. The output shows the resulting vector of numbers.\n\n```R\n1:10\n```\n\n--------------------------------\n\n### test_file() Fix for Reporter Context Handling\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_file()` function now only calls `Reporter$end_context()` if a context was previously started, resolving an error in `TeamcityReporter` and improving robustness.\n\n```APIDOC\ntest_file(path)\n  Behavior: Calls Reporter$end_context() conditionally, fixing TeamcityReporter error.\n```\n\n--------------------------------\n\n### Minor Fixes and Documentation for test_examples()\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_examples()` function now returns results invisibly, no longer assumes examples contain tests, and its documentation clarifies that it's not intended for routine use. These changes improve its behavior and usage guidance.\n\n```APIDOC\ntest_examples()\n  Returns: Invisibly.\n  Usage: Not intended for routine use.\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### APIDOC: `testthat` Setup and Teardown Hooks\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nDocumentation for functions and file prefixes that enable setup and teardown operations for test files.\n\n```APIDOC\nsetup(): Function run at the start of each test file, useful for initializing state.\n```\n\n```APIDOC\nteardown(): Function run at the end of each test file, useful for cleaning up state.\n```\n\n```APIDOC\ntest/setup files: Files starting with `setup` in the `test/` directory are run before tests (unlike `helpers`, not run in `devtools::load_all()`).\n```\n\n```APIDOC\ntest/teardown files: Files starting with `teardown` in the `test/` directory are run after all tests are completed.\n```\n\n--------------------------------\n\n### Install testthat R Package\n\nSource: https://github.com/r-lib/testthat/blob/main/README.md\n\nInstructions for installing the testthat package, either the released version from CRAN or the development version from GitHub using the pak package.\n\n```R",
            "codeBlocks": [
              {
                "language": "APIDOC",
                "code": "setup(): Function run at the start of each test file, useful for initializing state.",
                "context": "Source: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nDocumentation for functions and file prefixes that enable setup and teardown operations for test files."
              },
              {
                "language": "APIDOC",
                "code": "teardown(): Function run at the end of each test file, useful for cleaning up state.",
                "context": "```APIDOC\nsetup(): Function run at the start of each test file, useful for initializing state.\n```"
              },
              {
                "language": "APIDOC",
                "code": "test/setup files: Files starting with `setup` in the `test/` directory are run before tests (unlike `helpers`, not run in `devtools::load_all()`).",
                "context": "```APIDOC\nteardown(): Function run at the end of each test file, useful for cleaning up state.\n```"
              },
              {
                "language": "APIDOC",
                "code": "test/teardown files: Files starting with `teardown` in the `test/` directory are run after all tests are completed.",
                "context": "```APIDOC\ntest/setup files: Files starting with `setup` in the `test/` directory are run before tests (unlike `helpers`, not run in `devtools::load_all()`).\n```"
              }
            ]
          },
          {
            "title": "Install the released version from CRAN",
            "type": "other",
            "content": "install.packages(\"testthat\")",
            "codeBlocks": []
          },
          {
            "title": "install.packages(\"pak\")",
            "type": "other",
            "content": "pak::pak(\"r-lib/testthat\")\n```\n\n--------------------------------\n\n### R Package `ieegio` Example Execution Error\n\nSource: https://github.com/r-lib/testthat/blob/main/revdep/problems.md\n\nOutput from `R CMD check` showing an error during the execution of examples for the `ieegio` package. The error occurred in `ieegio-Ex.R` and is related to the inability to find the `conda` binary, suggesting issues with `Miniconda` or `Conda Environments` installation during the check.\n\n```R Error Log\nRunning examples in ieegio-Ex.R failed\nThe error most likely occurred in:\n\n> ### Name: imaging-volume\n> ### Title: Read and write volume data\n> ### Aliases: imaging-volume read_volume write_volume io_read_mgz\n> ###   io_write_mgz io_write_mgz.ieegio_volume io_write_mgz.ieegio_mgh\n> ###   io_write_mgz.nifti io_write_mgz.niftiImage\n> ###   io_write_mgz.ants.core.ants_image.ANTsImage io_write_mgz.array\n> ###   io_read_nii io_write_nii io_write_nii.ieegio_nifti\n...\n+ # clean up\n+ unlink(f)\n+ unlink(f2)\n+ \n+ }\n<simpleWarning in check_forbidden_install(\"Miniconda\"): cannot install Miniconda during R CMD check>\nWarning in check_forbidden_install(\"Conda Environments\") :\n  cannot install Conda Environments during R CMD check\nError: Unable to find conda binary. Is Anaconda installed?\nExecution halted\n```\n\n--------------------------------\n\n### SummaryReporter Update\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\n`SummaryReporter` now records the file start time, not just the context start, which improves compatibility with modern testing styles that do not rely on `context()`.\n\n```APIDOC\nSummaryReporter:\n  Description: A reporter that provides a summary.\n  Improvements:\n    - Records file start, not just context start.\n    - More compatible with modern style which does not use `context()`.\n```\n\n--------------------------------\n\n### Perform Basic Arithmetic in R Console\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-verify-output.txt\n\nDemonstrates a simple addition operation within an R console session, including the input prompt, the code, and the resulting output.\n\n```R\n> # Output\n> 1 + 2\n[1] 3\n```\n\n--------------------------------\n\n### Informative Hints for Accepting testthat Snapshots\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/_snaps/snapshot.md\n\nProvides examples of the helpful hints generated by `testthat` for accepting or reviewing snapshot changes, including RStudio IDE links. These hints streamline the snapshot management workflow.\n\n```R\nCode:\n  cat(snapshot_accept_hint(\"_default\", \"bar.R\", reset_output = FALSE))\nOutput:\n  * Run  ]8;;ide:run:testthat::snapshot\n```\n\n--------------------------------\n\n### testthat New Utility Functions\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThis section highlights new functions that simplify accessing package examples and improve mocking capabilities. `local_mock()` offers a more streamlined approach to mocking compared to `with_mock()`.\n\n```APIDOC\ntestthat_examples():\n  Description: Provides easy access to new test files bundled with the package.\n\ntestthat_example():\n  Description: Provides easy access to new test files bundled with the package.\n\nlocal_mock():\n  Description: Allows mocking a function without requiring additional indentation.\n  Comparison: Offers a more convenient alternative to `with_mock()`.\n```\n\n--------------------------------\n\n### Generate a Sequence of Numbers in R\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-parallel/snap/tests/testthat/_snaps/snap-1.md\n\nThis snippet demonstrates how to generate a sequence of integers from a starting number to an ending number in R using the colon operator. The output shows the resulting vector of numbers.\n\n```R\n1:10\n```\n\n--------------------------------\n\n### test_file() Fix for Reporter Context Handling\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_file()` function now only calls `Reporter$end_context()` if a context was previously started, resolving an error in `TeamcityReporter` and improving robustness.\n\n```APIDOC\ntest_file(path)\n  Behavior: Calls Reporter$end_context() conditionally, fixing TeamcityReporter error.\n```\n\n--------------------------------\n\n### Minor Fixes and Documentation for test_examples()\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_examples()` function now returns results invisibly, no longer assumes examples contain tests, and its documentation clarifies that it's not intended for routine use. These changes improve its behavior and usage guidance.\n\n```APIDOC\ntest_examples()\n  Returns: Invisibly.\n  Usage: Not intended for routine use.\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### R Package `ieegio` Example Execution Error\n\nSource: https://github.com/r-lib/testthat/blob/main/revdep/problems.md\n\nOutput from `R CMD check` showing an error during the execution of examples for the `ieegio` package. The error occurred in `ieegio-Ex.R` and is related to the inability to find the `conda` binary, suggesting issues with `Miniconda` or `Conda Environments` installation during the check.",
                "context": "pak::pak(\"r-lib/testthat\")"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### SummaryReporter Update\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\n`SummaryReporter` now records the file start time, not just the context start, which improves compatibility with modern testing styles that do not rely on `context()`.",
                "context": "  cannot install Conda Environments during R CMD check\nError: Unable to find conda binary. Is Anaconda installed?\nExecution halted"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Perform Basic Arithmetic in R Console\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-verify-output.txt\n\nDemonstrates a simple addition operation within an R console session, including the input prompt, the code, and the resulting output.",
                "context": "  Improvements:\n    - Records file start, not just context start.\n    - More compatible with modern style which does not use `context()`."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Informative Hints for Accepting testthat Snapshots\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/_snaps/snapshot.md\n\nProvides examples of the helpful hints generated by `testthat` for accepting or reviewing snapshot changes, including RStudio IDE links. These hints streamline the snapshot management workflow.",
                "context": "> # Output\n> 1 + 2\n[1] 3"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### testthat New Utility Functions\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThis section highlights new functions that simplify accessing package examples and improve mocking capabilities. `local_mock()` offers a more streamlined approach to mocking compared to `with_mock()`.",
                "context": "  cat(snapshot_accept_hint(\"_default\", \"bar.R\", reset_output = FALSE))\nOutput:\n  * Run  ]8;;ide:run:testthat::snapshot"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Generate a Sequence of Numbers in R\n\nSource: https://github.com/r-lib/testthat/blob/main/tests/testthat/test-parallel/snap/tests/testthat/_snaps/snap-1.md\n\nThis snippet demonstrates how to generate a sequence of integers from a starting number to an ending number in R using the colon operator. The output shows the resulting vector of numbers.",
                "context": "local_mock():\n  Description: Allows mocking a function without requiring additional indentation.\n  Comparison: Offers a more convenient alternative to `with_mock()`."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### test_file() Fix for Reporter Context Handling\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_file()` function now only calls `Reporter$end_context()` if a context was previously started, resolving an error in `TeamcityReporter` and improving robustness.",
                "context": "\n```R\n1:10"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Minor Fixes and Documentation for test_examples()\n\nSource: https://github.com/r-lib/testthat/blob/main/NEWS.md\n\nThe `test_examples()` function now returns results invisibly, no longer assumes examples contain tests, and its documentation clarifies that it's not intended for routine use. These changes improve its behavior and usage guidance.",
                "context": "```APIDOC\ntest_file(path)\n  Behavior: Calls Reporter$end_context() conditionally, fixing TeamcityReporter error."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/pytest-dev/pytest-mock",
        "packageName": "pytest-mock",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:37.816Z",
        "content": "### Console: Install pytest-mock using pip\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/index.rst\n\nProvides the command to install the pytest-mock package using pip, the standard package installer for Python. This command downloads and installs the latest version of the library.\n\n```console\n$ pip install pytest-mock\n```\n\n--------------------------------\n\n### Install pre-commit hooks\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/contributing.rst\n\nInstalls pre-commit, a framework for managing and maintaining multi-language pre-commit hooks. This ensures code quality and consistency before commits.\n\n```console\npre-commit install\n```\n\n--------------------------------\n\n### Install pytest-mock with dev extras\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/contributing.rst\n\nInstalls pytest-mock in editable mode with development dependencies. This command is essential for setting up your local environment for development and testing.\n\n```console\npip install --editable .[dev]\n```\n\n--------------------------------\n\n### pytest-mock mocker.patch Usage\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nShows how to use `mocker.patch` to mock imported functions or class methods by their string path. Includes examples for setting return values, side effects, and asserting calls.\n\n```python\nimport os\n\ndef test_patch_function(mocker):\n    # Mock os.remove function\n    mock_remove = mocker.patch('os.remove')\n\n    os.remove('/fake/path')\n\n    # Assert it was called with expected arguments\n    mock_remove.assert_called_once_with('/fake/path')\n    # File still exists - real function never executed\n\ndef test_patch_with_return_value(mocker):\n    # Mock with specific return value\n    mocker.patch('os.listdir', return_value=['file1.txt', 'file2.txt'])\n\n    result = os.listdir('/any/path')\n\n    assert result == ['file1.txt', 'file2.txt']\n\ndef test_patch_with_side_effect(mocker):\n    # Mock with exception\n    mocker.patch('os.remove', side_effect=PermissionError('Access denied'))\n\n    import pytest\n    with pytest.raises(PermissionError, match='Access denied'):\n        os.remove('/protected/file')\n\n```\n\n--------------------------------\n\n### pytest-mock mocker.patch.multiple Usage\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nIllustrates how to use `mocker.patch.multiple` to mock multiple attributes of an object simultaneously. Shows examples with default mocks and providing explicit implementations.\n\n```python\nimport os\n\ndef test_patch_multiple(mocker):\n    # Mock multiple functions at once\n    mocks = mocker.patch.multiple('os',\n                                   remove=mocker.DEFAULT,\n                                   listdir=mocker.DEFAULT,\n                                   makedirs=mocker.DEFAULT)\n\n    # Set specific behaviors\n    mocks['listdir'].return_value = ['file.txt']\n\n    os.listdir('/tmp')\n    os.remove('/file')\n    os.makedirs('/newdir')\n\n    # All mocks are accessible\n    mocks['listdir'].assert_called_once_with('/tmp')\n    mocks['remove'].assert_called_once_with('/file')\n    mocks['makedirs'].assert_called_once_with('/newdir')\n\ndef test_patch_multiple_with_values(mocker):\n    # Provide explicit mock implementations\n    mocker.patch.multiple('os',\n                          remove=lambda x: None,\n                          listdir=lambda x: ['data.txt'])\n\n    result = os.listdir('/any')\n    assert result == ['data.txt']\n\n```\n\n--------------------------------\n\n### Mocking with mocker.patch.object in Python (Discouraged)\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/usage.rst\n\nShows an example of using mocker.patch.object as a context manager within a pytest test. The documentation advises against this pattern, recommending direct use of the mocker fixture instead.\n\n```python\ndef test_context_manager(mocker):\n    a = A()\n    with mocker.patch.object(a, 'doIt', return_value=True, autospec=True):  # DO NOT DO THIS\n        assert a.doIt() == True\n```\n\n--------------------------------\n\n### Pytest-Mock: Using mocker.ANY and mocker.sentinel\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nIllustrates the use of mocker.ANY for flexible argument matching in mock assertions, allowing any value to satisfy a placeholder. It also shows how to use mocker.sentinel to create unique singleton objects for various purposes, such as default return values or distinguishing between different states.\n\n```python\ndef test_any_matcher(mocker):\n    mock_func = mocker.Mock()\n\n    mock_func(123, 'test', key={'nested': 'value'})\n\n    # ANY matches any value\n    mock_func.assert_called_with(mocker.ANY, 'test', key=mocker.ANY)\n    mock_func.assert_called_with(123, mocker.ANY, key={'nested': 'value'})\n\ndef test_sentinel_unique_objects(mocker):\n    # Sentinels are unique singleton objects\n    default_value = mocker.sentinel.DEFAULT\n    missing_value = mocker.sentinel.MISSING\n\n    mock_func = mocker.Mock(return_value=default_value)\n\n    result = mock_func()\n\n    assert result is default_value\n    assert result is not missing_value\n    assert result is mocker.sentinel.DEFAULT\n```\n\n--------------------------------\n\n### Create and Use mocker.Mock and mocker.MagicMock Objects\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nDemonstrates how to create basic mock objects and magic mocks using `mocker.Mock()` and `mocker.MagicMock()`. These allow configuring return values, side effects, and asserting calls on methods, including special methods for MagicMock. Mocks can also be created with a spec to enforce method signatures.\n\n```python\ndef test_mock_object(mocker):\n    # Basic Mock\n    mock_obj = mocker.Mock()\n    mock_obj.method.return_value = 42\n\n    result = mock_obj.method('arg')\n\n    assert result == 42\n    mock_obj.method.assert_called_once_with('arg')\n\ndef test_magic_mock_special_methods(mocker):\n    # MagicMock supports magic methods\n    mock_obj = mocker.MagicMock()\n    mock_obj.__len__.return_value = 5\n    mock_obj.__getitem__.return_value = 'item'\n\n    assert len(mock_obj) == 5\n    assert mock_obj[0] == 'item'\n    mock_obj.__getitem__.assert_called_with(0)\n\ndef test_mock_spec(mocker):\n    class RealService:\n        def process(self, data):\n            pass\n\n    # Mock with spec enforces method signatures\n    mock_service = mocker.Mock(spec=RealService)\n    mock_service.process.return_value = 'result'\n\n    result = mock_service.process('data')\n    assert result == 'result'\n\n    # This raises AttributeError - method doesn't exist on RealService\n    try:\n        mock_service.nonexistent_method()\n    except AttributeError:\n        pass\n\n```\n\n--------------------------------\n\n### Programmatic Configuration of pytest-mock\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nDemonstrates how to programmatically configure pytest-mock within a conftest.py file, such as adding custom markers.\n\n```python\n# conftest.py\ndef pytest_configure(config):\n    # Programmatic configuration\n    config.addinivalue_line(\n        \"markers\", \"mock: mark test to use mocking\"\n    )\n```\n\n--------------------------------\n\n### Using contextlib.ExitStack with mock.patch in Python\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/remarks.rst\n\nDemonstrates an alternative to nested 'with' statements using 'contextlib.ExitStack' to manage multiple 'mock.patch' context managers. This flattens the indentation, improving readability, but is noted as potentially more complex than using pytest-mock.\n\n```python\nimport contextlib\nimport mock\n\ndef test_unix_fs():\n    with contextlib.ExitStack() as stack:\n        stack.enter_context(mock.patch('os.remove'))\n        UnixFS.rm('file')\n        os.remove.assert_called_once_with('file')\n\n        stack.enter_context(mock.patch('os.listdir'))\n        assert UnixFS.ls('dir') == expected\n        # ...\n\n        stack.enter_context(mock.patch('shutil.copy'))\n        UnixFS.cp('src', 'dst')\n        # ...\n\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Console: Install pytest-mock using pip\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/index.rst\n\nProvides the command to install the pytest-mock package using pip, the standard package installer for Python. This command downloads and installs the latest version of the library.\n\n```console\n$ pip install pytest-mock\n```\n\n--------------------------------\n\n### Install pre-commit hooks\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/contributing.rst\n\nInstalls pre-commit, a framework for managing and maintaining multi-language pre-commit hooks. This ensures code quality and consistency before commits.\n\n```console\npre-commit install\n```\n\n--------------------------------\n\n### Install pytest-mock with dev extras\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/contributing.rst\n\nInstalls pytest-mock in editable mode with development dependencies. This command is essential for setting up your local environment for development and testing.\n\n```console\npip install --editable .[dev]\n```\n\n--------------------------------\n\n### pytest-mock mocker.patch Usage\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nShows how to use `mocker.patch` to mock imported functions or class methods by their string path. Includes examples for setting return values, side effects, and asserting calls.\n\n```python\nimport os\n\ndef test_patch_function(mocker):\n    # Mock os.remove function\n    mock_remove = mocker.patch('os.remove')\n\n    os.remove('/fake/path')\n\n    # Assert it was called with expected arguments\n    mock_remove.assert_called_once_with('/fake/path')\n    # File still exists - real function never executed\n\ndef test_patch_with_return_value(mocker):\n    # Mock with specific return value\n    mocker.patch('os.listdir', return_value=['file1.txt', 'file2.txt'])\n\n    result = os.listdir('/any/path')\n\n    assert result == ['file1.txt', 'file2.txt']\n\ndef test_patch_with_side_effect(mocker):\n    # Mock with exception\n    mocker.patch('os.remove', side_effect=PermissionError('Access denied'))\n\n    import pytest\n    with pytest.raises(PermissionError, match='Access denied'):\n        os.remove('/protected/file')\n\n```\n\n--------------------------------\n\n### pytest-mock mocker.patch.multiple Usage\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nIllustrates how to use `mocker.patch.multiple` to mock multiple attributes of an object simultaneously. Shows examples with default mocks and providing explicit implementations.\n\n```python\nimport os\n\ndef test_patch_multiple(mocker):\n    # Mock multiple functions at once\n    mocks = mocker.patch.multiple('os',\n                                   remove=mocker.DEFAULT,\n                                   listdir=mocker.DEFAULT,\n                                   makedirs=mocker.DEFAULT)\n\n    # Set specific behaviors\n    mocks['listdir'].return_value = ['file.txt']\n\n    os.listdir('/tmp')\n    os.remove('/file')\n    os.makedirs('/newdir')\n\n    # All mocks are accessible\n    mocks['listdir'].assert_called_once_with('/tmp')\n    mocks['remove'].assert_called_once_with('/file')\n    mocks['makedirs'].assert_called_once_with('/newdir')\n\ndef test_patch_multiple_with_values(mocker):\n    # Provide explicit mock implementations\n    mocker.patch.multiple('os',\n                          remove=lambda x: None,\n                          listdir=lambda x: ['data.txt'])\n\n    result = os.listdir('/any')\n    assert result == ['data.txt']\n\n```\n\n--------------------------------\n\n### Mocking with mocker.patch.object in Python (Discouraged)\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/usage.rst\n\nShows an example of using mocker.patch.object as a context manager within a pytest test. The documentation advises against this pattern, recommending direct use of the mocker fixture instead.\n\n```python\ndef test_context_manager(mocker):\n    a = A()\n    with mocker.patch.object(a, 'doIt', return_value=True, autospec=True):  # DO NOT DO THIS\n        assert a.doIt() == True\n```\n\n--------------------------------\n\n### Pytest-Mock: Using mocker.ANY and mocker.sentinel\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nIllustrates the use of mocker.ANY for flexible argument matching in mock assertions, allowing any value to satisfy a placeholder. It also shows how to use mocker.sentinel to create unique singleton objects for various purposes, such as default return values or distinguishing between different states.\n\n```python\ndef test_any_matcher(mocker):\n    mock_func = mocker.Mock()\n\n    mock_func(123, 'test', key={'nested': 'value'})\n\n    # ANY matches any value\n    mock_func.assert_called_with(mocker.ANY, 'test', key=mocker.ANY)\n    mock_func.assert_called_with(123, mocker.ANY, key={'nested': 'value'})\n\ndef test_sentinel_unique_objects(mocker):\n    # Sentinels are unique singleton objects\n    default_value = mocker.sentinel.DEFAULT\n    missing_value = mocker.sentinel.MISSING\n\n    mock_func = mocker.Mock(return_value=default_value)\n\n    result = mock_func()\n\n    assert result is default_value\n    assert result is not missing_value\n    assert result is mocker.sentinel.DEFAULT\n```\n\n--------------------------------\n\n### Create and Use mocker.Mock and mocker.MagicMock Objects\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nDemonstrates how to create basic mock objects and magic mocks using `mocker.Mock()` and `mocker.MagicMock()`. These allow configuring return values, side effects, and asserting calls on methods, including special methods for MagicMock. Mocks can also be created with a spec to enforce method signatures.\n\n```python\ndef test_mock_object(mocker):\n    # Basic Mock\n    mock_obj = mocker.Mock()\n    mock_obj.method.return_value = 42\n\n    result = mock_obj.method('arg')\n\n    assert result == 42\n    mock_obj.method.assert_called_once_with('arg')\n\ndef test_magic_mock_special_methods(mocker):\n    # MagicMock supports magic methods\n    mock_obj = mocker.MagicMock()\n    mock_obj.__len__.return_value = 5\n    mock_obj.__getitem__.return_value = 'item'\n\n    assert len(mock_obj) == 5\n    assert mock_obj[0] == 'item'\n    mock_obj.__getitem__.assert_called_with(0)\n\ndef test_mock_spec(mocker):\n    class RealService:\n        def process(self, data):\n            pass\n\n    # Mock with spec enforces method signatures\n    mock_service = mocker.Mock(spec=RealService)\n    mock_service.process.return_value = 'result'\n\n    result = mock_service.process('data')\n    assert result == 'result'\n\n    # This raises AttributeError - method doesn't exist on RealService\n    try:\n        mock_service.nonexistent_method()\n    except AttributeError:\n        pass\n\n```\n\n--------------------------------\n\n### Programmatic Configuration of pytest-mock\n\nSource: https://context7.com/pytest-dev/pytest-mock/llms.txt\n\nDemonstrates how to programmatically configure pytest-mock within a conftest.py file, such as adding custom markers.\n\n```python",
            "codeBlocks": [
              {
                "language": "console",
                "code": "$ pip install pytest-mock",
                "context": "n/docs/index.rst\n\nProvides the command to install the pytest-mock package using pip, the standard package installer for Python. This command downloads and installs the latest version of the library."
              },
              {
                "language": "console",
                "code": "pre-commit install",
                "context": "/pytest-mock/blob/main/docs/contributing.rst\n\nInstalls pre-commit, a framework for managing and maintaining multi-language pre-commit hooks. This ensures code quality and consistency before commits."
              },
              {
                "language": "console",
                "code": "pip install --editable .[dev]",
                "context": "ock/blob/main/docs/contributing.rst\n\nInstalls pytest-mock in editable mode with development dependencies. This command is essential for setting up your local environment for development and testing."
              },
              {
                "language": "python",
                "code": "import os\n\ndef test_patch_function(mocker):\n    # Mock os.remove function\n    mock_remove = mocker.patch('os.remove')\n\n    os.remove('/fake/path')\n\n    # Assert it was called with expected arguments\n    mock_remove.assert_called_once_with('/fake/path')\n    # File still exists - real function never executed\n\ndef test_patch_with_return_value(mocker):\n    # Mock with specific return value\n    mocker.patch('os.listdir', return_value=['file1.txt', 'file2.txt'])\n\n    result = os.listdir('/any/path')\n\n    assert result == ['file1.txt', 'file2.txt']\n\ndef test_patch_with_side_effect(mocker):\n    # Mock with exception\n    mocker.patch('os.remove', side_effect=PermissionError('Access denied'))\n\n    import pytest\n    with pytest.raises(PermissionError, match='Access denied'):\n        os.remove('/protected/file')",
                "context": "ytest-mock/llms.txt\n\nShows how to use `mocker.patch` to mock imported functions or class methods by their string path. Includes examples for setting return values, side effects, and asserting calls."
              },
              {
                "language": "python",
                "code": "import os\n\ndef test_patch_multiple(mocker):\n    # Mock multiple functions at once\n    mocks = mocker.patch.multiple('os',\n                                   remove=mocker.DEFAULT,\n                                   listdir=mocker.DEFAULT,\n                                   makedirs=mocker.DEFAULT)\n\n    # Set specific behaviors\n    mocks['listdir'].return_value = ['file.txt']\n\n    os.listdir('/tmp')\n    os.remove('/file')\n    os.makedirs('/newdir')\n\n    # All mocks are accessible\n    mocks['listdir'].assert_called_once_with('/tmp')\n    mocks['remove'].assert_called_once_with('/file')\n    mocks['makedirs'].assert_called_once_with('/newdir')\n\ndef test_patch_multiple_with_values(mocker):\n    # Provide explicit mock implementations\n    mocker.patch.multiple('os',\n                          remove=lambda x: None,\n                          listdir=lambda x: ['data.txt'])\n\n    result = os.listdir('/any')\n    assert result == ['data.txt']",
                "context": "ytest-mock/llms.txt\n\nIllustrates how to use `mocker.patch.multiple` to mock multiple attributes of an object simultaneously. Shows examples with default mocks and providing explicit implementations."
              },
              {
                "language": "python",
                "code": "def test_context_manager(mocker):\n    a = A()\n    with mocker.patch.object(a, 'doIt', return_value=True, autospec=True):  # DO NOT DO THIS\n        assert a.doIt() == True",
                "context": "e.rst\n\nShows an example of using mocker.patch.object as a context manager within a pytest test. The documentation advises against this pattern, recommending direct use of the mocker fixture instead."
              },
              {
                "language": "python",
                "code": "def test_any_matcher(mocker):\n    mock_func = mocker.Mock()\n\n    mock_func(123, 'test', key={'nested': 'value'})\n\n    # ANY matches any value\n    mock_func.assert_called_with(mocker.ANY, 'test', key=mocker.ANY)\n    mock_func.assert_called_with(123, mocker.ANY, key={'nested': 'value'})\n\ndef test_sentinel_unique_objects(mocker):\n    # Sentinels are unique singleton objects\n    default_value = mocker.sentinel.DEFAULT\n    missing_value = mocker.sentinel.MISSING\n\n    mock_func = mocker.Mock(return_value=default_value)\n\n    result = mock_func()\n\n    assert result is default_value\n    assert result is not missing_value\n    assert result is mocker.sentinel.DEFAULT",
                "context": "to satisfy a placeholder. It also shows how to use mocker.sentinel to create unique singleton objects for various purposes, such as default return values or distinguishing between different states."
              },
              {
                "language": "python",
                "code": "def test_mock_object(mocker):\n    # Basic Mock\n    mock_obj = mocker.Mock()\n    mock_obj.method.return_value = 42\n\n    result = mock_obj.method('arg')\n\n    assert result == 42\n    mock_obj.method.assert_called_once_with('arg')\n\ndef test_magic_mock_special_methods(mocker):\n    # MagicMock supports magic methods\n    mock_obj = mocker.MagicMock()\n    mock_obj.__len__.return_value = 5\n    mock_obj.__getitem__.return_value = 'item'\n\n    assert len(mock_obj) == 5\n    assert mock_obj[0] == 'item'\n    mock_obj.__getitem__.assert_called_with(0)\n\ndef test_mock_spec(mocker):\n    class RealService:\n        def process(self, data):\n            pass\n\n    # Mock with spec enforces method signatures\n    mock_service = mocker.Mock(spec=RealService)\n    mock_service.process.return_value = 'result'\n\n    result = mock_service.process('data')\n    assert result == 'result'\n\n    # This raises AttributeError - method doesn't exist on RealService\n    try:\n        mock_service.nonexistent_method()\n    except AttributeError:\n        pass",
                "context": "()`. These allow configuring return values, side effects, and asserting calls on methods, including special methods for MagicMock. Mocks can also be created with a spec to enforce method signatures."
              }
            ]
          },
          {
            "title": "conftest.py",
            "type": "other",
            "content": "def pytest_configure(config):\n    # Programmatic configuration\n    config.addinivalue_line(\n        \"markers\", \"mock: mark test to use mocking\"\n    )\n```\n\n--------------------------------\n\n### Using contextlib.ExitStack with mock.patch in Python\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/remarks.rst\n\nDemonstrates an alternative to nested 'with' statements using 'contextlib.ExitStack' to manage multiple 'mock.patch' context managers. This flattens the indentation, improving readability, but is noted as potentially more complex than using pytest-mock.\n\n```python\nimport contextlib\nimport mock\n\ndef test_unix_fs():\n    with contextlib.ExitStack() as stack:\n        stack.enter_context(mock.patch('os.remove'))\n        UnixFS.rm('file')\n        os.remove.assert_called_once_with('file')\n\n        stack.enter_context(mock.patch('os.listdir'))\n        assert UnixFS.ls('dir') == expected\n        # ...\n\n        stack.enter_context(mock.patch('shutil.copy'))\n        UnixFS.cp('src', 'dst')\n        # ...\n\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Using contextlib.ExitStack with mock.patch in Python\n\nSource: https://github.com/pytest-dev/pytest-mock/blob/main/docs/remarks.rst\n\nDemonstrates an alternative to nested 'with' statements using 'contextlib.ExitStack' to manage multiple 'mock.patch' context managers. This flattens the indentation, improving readability, but is noted as potentially more complex than using pytest-mock.",
                "context": "    config.addinivalue_line(\n        \"markers\", \"mock: mark test to use mocking\"\n    )"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/thomhurst/tunit",
        "packageName": "tunit",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:41.694Z",
        "content": "### TUnit Before Hooks Example\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/test-lifecycle/setup.md\n\nDemonstrates the usage of [Before(Class)] for asynchronous setup before the first test in a class and [Before(Test)] for setup before each individual test. It also shows a sample test method verifying the setup.\n\n```csharp\nusing TUnit.Core;\nusing System.Net;\n\nnamespace MyTestProject;\n\npublic class MyTestClass\n{\n    private int _value;\n    private static HttpResponseMessage? _pingResponse;\n\n    [Before(Class)]\n    public static async Task Ping()\n    {\n        _pingResponse = await new HttpClient().GetAsync(\"https://localhost/ping\");\n    }\n    \n    [Before(Test)]\n    public async Task Setup()\n    {\n        await Task.CompletedTask;\n        \n        _value = 99;\n    }\n\n    [Test]\n    public async Task MyTest()\n    {\n        await Assert.That(_value).IsEqualTo(99);\n        await Assert.That(_pingResponse?.StatusCode)\n            .IsNotNull()\n            .And.IsEqualTo(HttpStatusCode.OK);\n    }\n}\n```\n\n--------------------------------\n\n### Command Line: Adding TUnit to a Project\n\nSource: https://github.com/thomhurst/tunit/blob/main/README.md\n\nThese bash commands illustrate how to get started with TUnit. It shows how to install TUnit templates to create a new test project or add the TUnit package to an existing .NET project.\n\n```bash\n# Create a new test project\ndotnet new install TUnit.Templates && dotnet new TUnit -n \"MyTestProject\"\n\n# Or add to existing project\ndotnet add package TUnit --prerelease\n```\n\n--------------------------------\n\n### Basic Assertion Syntax - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nDemonstrates the fundamental pattern for writing assertions in TUnit. It involves starting with Assert.That(), chaining specific assertion methods, and importantly, awaiting the assertion as they are asynchronous.\n\n```csharp\nawait Assert.That(actualValue).IsEqualTo(expectedValue);\n```\n\n--------------------------------\n\n### Install TUnit Project Template\n\nSource: https://github.com/thomhurst/tunit/blob/main/README_Template.md\n\nInstalls the TUnit project templates globally and then creates a new test project using the TUnit template. This is the recommended way to start a new project with TUnit.\n\n```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\"\n```\n\n--------------------------------\n\n### .NET Project File Configuration for TUnit\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/getting-started/installation.md\n\nAn example .NET project (.csproj) file configured for TUnit. It includes standard properties and item group for the TUnit package reference.\n\n```xml\n<Project Sdk=\"Microsoft.NET.Sdk\">\n\n    <PropertyGroup>\n        <OutputType>Exe</OutputType>\n        <TargetFramework>net8.0</TargetFramework>\n        <ImplicitUsings>enable</ImplicitUsings>\n        <Nullable>enable</Nullable>\n    </PropertyGroup>\n\n    <ItemGroup>\n      <PackageReference Include=\"TUnit\" Version=\"*\" />\n    </ItemGroup>\n\n</Project>\n```\n\n--------------------------------\n\n### Equality and Comparison Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of common TUnit assertions for comparing values, including checking for equality, inequality, greater than, less than or equal to, and range checks.\n\n```csharp\nawait Assert.That(actual).IsEqualTo(expected);\nawait Assert.That(value).IsNotEqualTo(other);\nawait Assert.That(score).IsGreaterThan(70);\nawait Assert.That(age).IsLessThanOrEqualTo(100);\nawait Assert.That(temperature).IsBetween(20, 30);\n```\n\n--------------------------------\n\n### Advanced Collection Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of more advanced TUnit assertions for collections, including checking order, equivalence (same items in any order), and using predicates for element matching.\n\n```csharp\nvar numbers = new[] { 1, 2, 3, 4, 5 };\n\n// Count and emptiness\nawait Assert.That(numbers).HasCount(5);\nawait Assert.That(numbers).IsNotEmpty();\n\n// Membership\nawait Assert.That(numbers).Contains(3);\nawait Assert.That(numbers).DoesNotContain(10);\n\n// Predicates\nawait Assert.That(numbers).All(n => n > 0);\nawait Assert.That(numbers).Any(n => n == 3);\n\n// Ordering\nawait Assert.That(numbers).IsInOrder();\n\n// Equivalence (same items, any order)\nawait Assert.That(numbers).IsEquivalentTo(new[] { 5, 4, 3, 2, 1 });\n```\n\n--------------------------------\n\n### Install TUnit using Project Template or Package Manager\n\nSource: https://github.com/thomhurst/tunit/blob/main/README.md\n\nThis snippet shows how to install TUnit. The recommended method is using the project template, which streamlines project setup. Alternatively, you can add the TUnit package directly to an existing project using the .NET CLI.\n\n```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\"\n```\n\n```bash\ndotnet add package TUnit --prerelease\n```\n\n--------------------------------\n\n### TUnit Pre-Commit Checklist Example\n\nSource: https://github.com/thomhurst/tunit/blob/main/CLAUDE.md\n\nA comprehensive checklist to ensure all necessary steps are verified before committing code to the TUnit project. It covers testing, snapshot verification, dual-mode parity, performance, AOT compatibility, and code style.\n\n```text\n\n  All tests pass: dotnet test                          \n                                                         \n  If source generator changed:                         \n    Ran TUnit.Core.SourceGenerator.Tests               \n    Reviewed .received.txt files                       \n    Accepted snapshots (.verified.txt)                 \n    Committed .verified.txt files                      \n                                                         \n  If public API changed:                               \n    Ran TUnit.PublicAPI tests                          \n    Reviewed .received.txt files                       \n    Accepted snapshots (.verified.txt)                 \n    Committed .verified.txt files                      \n                                                         \n  If dual-mode feature:                                \n    Implemented in BOTH source-gen AND reflection      \n    Tested both modes explicitly                       \n    Verified identical behavior                        \n                                                         \n  If performance-critical:                             \n    Profiled before/after                              \n    No performance regression                          \n    Minimized allocations                              \n                                                         \n  If touching reflection:                              \n    Tested AOT: dotnet publish -p:PublishAot=true      \n    Added DynamicallyAccessedMembers annotations       \n                                                         \n  Code follows style guide                             \n  No breaking changes (or major version bump)          \n\n```\n\n--------------------------------\n\n### Implement Async Initialization for Tests (C#)\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/troubleshooting.md\n\nDemonstrates how to perform asynchronous initialization for tests by implementing the `IAsyncInitializer` interface. This ensures that asynchronous setup tasks, like database connections, are completed before tests begin execution.\n\n```csharp\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\nusing System.Threading.Tasks;\n\n// Assume DatabaseConnection is an async-capable class\npublic class DatabaseConnection\n{\n    public static async Task<DatabaseConnection> CreateAsync() \n    {\n        // Simulate async connection logic\n        await Task.Delay(100);\n        return new DatabaseConnection();\n    }\n    // ... other methods ...\n}\n\npublic class DatabaseTests : IAsyncInitializer\n{\n    private DatabaseConnection _connection;\n    \n    // Async initialization logic\n    public async Task InitializeAsync()\n    {\n        _connection = await DatabaseConnection.CreateAsync();\n        Console.WriteLine(\"Database connection established.\");\n    }\n    \n    [Test]\n    public async Task TestDatabaseReadOperation()\n    {\n        // '_connection' is guaranteed to be initialized and ready to use here\n        Console.WriteLine(\"Running database read test...\");\n        // Example: await _connection.ReadDataAsync();\n    }\n\n    // Other tests can also use the initialized _connection\n    [Test]\n    public void AnotherDatabaseTest() \n    {\n        Console.WriteLine(\"Running another database test...\");\n        // ... use _connection ...\n    }\n}\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### TUnit Before Hooks Example\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/test-lifecycle/setup.md\n\nDemonstrates the usage of [Before(Class)] for asynchronous setup before the first test in a class and [Before(Test)] for setup before each individual test. It also shows a sample test method verifying the setup.\n\n```csharp\nusing TUnit.Core;\nusing System.Net;\n\nnamespace MyTestProject;\n\npublic class MyTestClass\n{\n    private int _value;\n    private static HttpResponseMessage? _pingResponse;\n\n    [Before(Class)]\n    public static async Task Ping()\n    {\n        _pingResponse = await new HttpClient().GetAsync(\"https://localhost/ping\");\n    }\n    \n    [Before(Test)]\n    public async Task Setup()\n    {\n        await Task.CompletedTask;\n        \n        _value = 99;\n    }\n\n    [Test]\n    public async Task MyTest()\n    {\n        await Assert.That(_value).IsEqualTo(99);\n        await Assert.That(_pingResponse?.StatusCode)\n            .IsNotNull()\n            .And.IsEqualTo(HttpStatusCode.OK);\n    }\n}\n```\n\n--------------------------------\n\n### Command Line: Adding TUnit to a Project\n\nSource: https://github.com/thomhurst/tunit/blob/main/README.md\n\nThese bash commands illustrate how to get started with TUnit. It shows how to install TUnit templates to create a new test project or add the TUnit package to an existing .NET project.\n\n```bash",
            "codeBlocks": [
              {
                "language": "csharp",
                "code": "using TUnit.Core;\nusing System.Net;\n\nnamespace MyTestProject;\n\npublic class MyTestClass\n{\n    private int _value;\n    private static HttpResponseMessage? _pingResponse;\n\n    [Before(Class)]\n    public static async Task Ping()\n    {\n        _pingResponse = await new HttpClient().GetAsync(\"https://localhost/ping\");\n    }\n    \n    [Before(Test)]\n    public async Task Setup()\n    {\n        await Task.CompletedTask;\n        \n        _value = 99;\n    }\n\n    [Test]\n    public async Task MyTest()\n    {\n        await Assert.That(_value).IsEqualTo(99);\n        await Assert.That(_pingResponse?.StatusCode)\n            .IsNotNull()\n            .And.IsEqualTo(HttpStatusCode.OK);\n    }\n}",
                "context": "the usage of [Before(Class)] for asynchronous setup before the first test in a class and [Before(Test)] for setup before each individual test. It also shows a sample test method verifying the setup."
              }
            ]
          },
          {
            "title": "Create a new test project",
            "type": "other",
            "content": "dotnet new install TUnit.Templates && dotnet new TUnit -n \"MyTestProject\"",
            "codeBlocks": []
          },
          {
            "title": "Or add to existing project",
            "type": "other",
            "content": "dotnet add package TUnit --prerelease\n```\n\n--------------------------------\n\n### Basic Assertion Syntax - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nDemonstrates the fundamental pattern for writing assertions in TUnit. It involves starting with Assert.That(), chaining specific assertion methods, and importantly, awaiting the assertion as they are asynchronous.\n\n```csharp\nawait Assert.That(actualValue).IsEqualTo(expectedValue);\n```\n\n--------------------------------\n\n### Install TUnit Project Template\n\nSource: https://github.com/thomhurst/tunit/blob/main/README_Template.md\n\nInstalls the TUnit project templates globally and then creates a new test project using the TUnit template. This is the recommended way to start a new project with TUnit.\n\n```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\"\n```\n\n--------------------------------\n\n### .NET Project File Configuration for TUnit\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/getting-started/installation.md\n\nAn example .NET project (.csproj) file configured for TUnit. It includes standard properties and item group for the TUnit package reference.\n\n```xml\n<Project Sdk=\"Microsoft.NET.Sdk\">\n\n    <PropertyGroup>\n        <OutputType>Exe</OutputType>\n        <TargetFramework>net8.0</TargetFramework>\n        <ImplicitUsings>enable</ImplicitUsings>\n        <Nullable>enable</Nullable>\n    </PropertyGroup>\n\n    <ItemGroup>\n      <PackageReference Include=\"TUnit\" Version=\"*\" />\n    </ItemGroup>\n\n</Project>\n```\n\n--------------------------------\n\n### Equality and Comparison Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of common TUnit assertions for comparing values, including checking for equality, inequality, greater than, less than or equal to, and range checks.\n\n```csharp\nawait Assert.That(actual).IsEqualTo(expected);\nawait Assert.That(value).IsNotEqualTo(other);\nawait Assert.That(score).IsGreaterThan(70);\nawait Assert.That(age).IsLessThanOrEqualTo(100);\nawait Assert.That(temperature).IsBetween(20, 30);\n```\n\n--------------------------------\n\n### Advanced Collection Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of more advanced TUnit assertions for collections, including checking order, equivalence (same items in any order), and using predicates for element matching.\n\n```csharp\nvar numbers = new[] { 1, 2, 3, 4, 5 };\n\n// Count and emptiness\nawait Assert.That(numbers).HasCount(5);\nawait Assert.That(numbers).IsNotEmpty();\n\n// Membership\nawait Assert.That(numbers).Contains(3);\nawait Assert.That(numbers).DoesNotContain(10);\n\n// Predicates\nawait Assert.That(numbers).All(n => n > 0);\nawait Assert.That(numbers).Any(n => n == 3);\n\n// Ordering\nawait Assert.That(numbers).IsInOrder();\n\n// Equivalence (same items, any order)\nawait Assert.That(numbers).IsEquivalentTo(new[] { 5, 4, 3, 2, 1 });\n```\n\n--------------------------------\n\n### Install TUnit using Project Template or Package Manager\n\nSource: https://github.com/thomhurst/tunit/blob/main/README.md\n\nThis snippet shows how to install TUnit. The recommended method is using the project template, which streamlines project setup. Alternatively, you can add the TUnit package directly to an existing project using the .NET CLI.\n\n```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\"\n```\n\n```bash\ndotnet add package TUnit --prerelease\n```\n\n--------------------------------\n\n### TUnit Pre-Commit Checklist Example\n\nSource: https://github.com/thomhurst/tunit/blob/main/CLAUDE.md\n\nA comprehensive checklist to ensure all necessary steps are verified before committing code to the TUnit project. It covers testing, snapshot verification, dual-mode parity, performance, AOT compatibility, and code style.\n\n```text\n\n  All tests pass: dotnet test                          \n                                                         \n  If source generator changed:                         \n    Ran TUnit.Core.SourceGenerator.Tests               \n    Reviewed .received.txt files                       \n    Accepted snapshots (.verified.txt)                 \n    Committed .verified.txt files                      \n                                                         \n  If public API changed:                               \n    Ran TUnit.PublicAPI tests                          \n    Reviewed .received.txt files                       \n    Accepted snapshots (.verified.txt)                 \n    Committed .verified.txt files                      \n                                                         \n  If dual-mode feature:                                \n    Implemented in BOTH source-gen AND reflection      \n    Tested both modes explicitly                       \n    Verified identical behavior                        \n                                                         \n  If performance-critical:                             \n    Profiled before/after                              \n    No performance regression                          \n    Minimized allocations                              \n                                                         \n  If touching reflection:                              \n    Tested AOT: dotnet publish -p:PublishAot=true      \n    Added DynamicallyAccessedMembers annotations       \n                                                         \n  Code follows style guide                             \n  No breaking changes (or major version bump)          \n\n```\n\n--------------------------------\n\n### Implement Async Initialization for Tests (C#)\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/troubleshooting.md\n\nDemonstrates how to perform asynchronous initialization for tests by implementing the `IAsyncInitializer` interface. This ensures that asynchronous setup tasks, like database connections, are completed before tests begin execution.\n\n```csharp\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\nusing System.Threading.Tasks;\n\n// Assume DatabaseConnection is an async-capable class\npublic class DatabaseConnection\n{\n    public static async Task<DatabaseConnection> CreateAsync() \n    {\n        // Simulate async connection logic\n        await Task.Delay(100);\n        return new DatabaseConnection();\n    }\n    // ... other methods ...\n}\n\npublic class DatabaseTests : IAsyncInitializer\n{\n    private DatabaseConnection _connection;\n    \n    // Async initialization logic\n    public async Task InitializeAsync()\n    {\n        _connection = await DatabaseConnection.CreateAsync();\n        Console.WriteLine(\"Database connection established.\");\n    }\n    \n    [Test]\n    public async Task TestDatabaseReadOperation()\n    {\n        // '_connection' is guaranteed to be initialized and ready to use here\n        Console.WriteLine(\"Running database read test...\");\n        // Example: await _connection.ReadDataAsync();\n    }\n\n    // Other tests can also use the initialized _connection\n    [Test]\n    public void AnotherDatabaseTest() \n    {\n        Console.WriteLine(\"Running another database test...\");\n        // ... use _connection ...\n    }\n}\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Basic Assertion Syntax - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nDemonstrates the fundamental pattern for writing assertions in TUnit. It involves starting with Assert.That(), chaining specific assertion methods, and importantly, awaiting the assertion as they are asynchronous.",
                "context": "dotnet add package TUnit --prerelease"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install TUnit Project Template\n\nSource: https://github.com/thomhurst/tunit/blob/main/README_Template.md\n\nInstalls the TUnit project templates globally and then creates a new test project using the TUnit template. This is the recommended way to start a new project with TUnit.",
                "context": "\n```csharp\nawait Assert.That(actualValue).IsEqualTo(expectedValue);"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### .NET Project File Configuration for TUnit\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/getting-started/installation.md\n\nAn example .NET project (.csproj) file configured for TUnit. It includes standard properties and item group for the TUnit package reference.",
                "context": "```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Equality and Comparison Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of common TUnit assertions for comparing values, including checking for equality, inequality, greater than, less than or equal to, and range checks.",
                "context": "    </ItemGroup>\n\n</Project>"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Advanced Collection Assertions - C#\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/assertions/getting-started.md\n\nProvides examples of more advanced TUnit assertions for collections, including checking order, equivalence (same items in any order), and using predicates for element matching.",
                "context": "await Assert.That(score).IsGreaterThan(70);\nawait Assert.That(age).IsLessThanOrEqualTo(100);\nawait Assert.That(temperature).IsBetween(20, 30);"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install TUnit using Project Template or Package Manager\n\nSource: https://github.com/thomhurst/tunit/blob/main/README.md\n\nThis snippet shows how to install TUnit. The recommended method is using the project template, which streamlines project setup. Alternatively, you can add the TUnit package directly to an existing project using the .NET CLI.",
                "context": "\n// Equivalence (same items, any order)\nawait Assert.That(numbers).IsEquivalentTo(new[] { 5, 4, 3, 2, 1 });"
              },
              {
                "language": "text",
                "code": "",
                "context": "```bash\ndotnet new install TUnit.Templates\ndotnet new TUnit -n \"MyTestProject\""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### TUnit Pre-Commit Checklist Example\n\nSource: https://github.com/thomhurst/tunit/blob/main/CLAUDE.md\n\nA comprehensive checklist to ensure all necessary steps are verified before committing code to the TUnit project. It covers testing, snapshot verification, dual-mode parity, performance, AOT compatibility, and code style.",
                "context": "\n```bash\ndotnet add package TUnit --prerelease"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Implement Async Initialization for Tests (C#)\n\nSource: https://github.com/thomhurst/tunit/blob/main/docs/docs/troubleshooting.md\n\nDemonstrates how to perform asynchronous initialization for tests by implementing the `IAsyncInitializer` interface. This ensures that asynchronous setup tasks, like database connections, are completed before tests begin execution.",
                "context": "  Code follows style guide                             \n  No breaking changes (or major version bump)          \n"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/websites/testomat_io",
        "packageName": "testomat_io",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:45.393Z",
        "content": "### Setup Automated Test Run\n\nSource: https://docs.testomat.io/getting-started\n\nInstructions for setting up automated test runs in Testomat.io. This involves navigating to the 'Runs' page, clicking 'Extra', selecting 'Setup Automated', and choosing the appropriate framework. Users may need to add a Testomat.io plugin to their code and execute generated commands from their project folder.\n\n```Shell\ntestomatio run --project your_project_name\n```\n\n--------------------------------\n\n### Import Automated Tests from Source Code\n\nSource: https://docs.testomat.io/getting-started\n\nThis section details the process of importing automated tests into Testomat.io. It involves selecting project parameters like framework, programming language, and operating system, then executing a generated command in the terminal. Prerequisites include having specific Node.js or PHP versions and package managers installed.\n\n```Shell\nnpm install -g @testomatio/cli\ntestomatio import --project your_project_name\n```\n\n--------------------------------\n\n### Development Setup and Contribution\n\nSource: https://docs.testomat.io/test-reporting/python\n\nInstructions for setting up the development environment using Python 3.12, installing dependencies, and contributing code.\n\n```bash\npip install \".[dev]\"\n```\n\n```bash\npython ./smoke.py\n```\n\n```bash\ncz commit\n```\n\n```bash\ncz bump\n```\n\n```bash\ngit push remoteName branchName --tags\n```\n\n--------------------------------\n\n### Cypress: Setup Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nGuides on integrating the Testomat.io reporter with Cypress, providing instructions for both Cypress versions below and above 10.0.0. This setup allows for automatic artifact uploads.\n\n```javascript\n// For Cypress < 10.0.0 in cypress/plugins/index.js\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = (on, config) => {\n  // ... other plugins\n  on('task', {\n    // ... other tasks\n    testomat: testomat\n  });\n};\n\n// For Cypress >= 10.0.0 in cypress.config.js(ts)\nconst { defineConfig } = require('cypress');\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = defineConfig({\n  // ... other config\n  e2e: {\n    setupNodeEvents(on, config) {\n      // implement node event listeners here\n      on('task', {\n        testomat: testomat\n      });\n    },\n  },\n});\n```\n\n--------------------------------\n\n### Install Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/junit\n\nInstall the Testomat.io reporter package using npm or yarn to enable integration with your test runner.\n\n```bash\nnpm install @testomatio/reporter\n# or\nyarn add @testomatio/reporter\n```\n\n--------------------------------\n\n### Install Testomat.io Reporter Package\n\nSource: https://docs.testomat.io/reference/reporter\n\nInstructions on how to install the Testomat.io reporter package using common package managers. This is the initial step to enable test result reporting to app.testomat.io.\n\n```bash\nnpm install @testomatio/reporter\n# or\n# yarn add @testomatio/reporter\n# or\n# pnpm add @testomatio/reporter\n```\n\n--------------------------------\n\n### CodeceptJS Example Command\n\nSource: https://docs.testomat.io/integrations/continuous-integration/jenkins\n\nExample of how to pass configured parameters as environment variables to a test runner, specifically using CodeceptJS. This demonstrates how Jenkins parameters are utilized within the test execution environment.\n\n```bash\nnpx codeceptjs run --grep \"$grep\" --env $testomatio --test \"$run\"\n```\n\n--------------------------------\n\n### Vitest: Install Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstalls the Testomat.io reporter package for Vitest using npm or yarn. This is the first step to integrating Testomat.io with your Vitest test suite.\n\n```bash\nnpm install --save-dev @testomat/vitest\n# or\nyarn add --dev @testomat/vitest\n```\n\n--------------------------------\n\n### Install and Use Testomat.io Reporter for Nightwatch\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstructions for installing the Testomat.io reporter for Nightwatch and configuring your test run command. This allows Nightwatch tests to report results to Testomat.io.\n\n```bash\nnpm install @testomatio/reporter --save-dev\nTESTOMATIO={API_KEY} npx nightwatch --reporter @testomatio/reporter/nightwatch\n```\n\n--------------------------------\n\n### Start Test Run with Testomat.io CLI\n\nSource: https://docs.testomat.io/test-reporting/cli\n\nStarts a new test run and retrieves its unique ID. This command requires the Testomat.io API key to be configured via the TESTOMATIO environment variable. It supports loading environment variables from a specified file using the --env-file option.\n\n```bash\nnpx @testomatio/reporter start\n\n# Example with env-file:\nnpx @testomatio/reporter start --env-file .env\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Setup Automated Test Run\n\nSource: https://docs.testomat.io/getting-started\n\nInstructions for setting up automated test runs in Testomat.io. This involves navigating to the 'Runs' page, clicking 'Extra', selecting 'Setup Automated', and choosing the appropriate framework. Users may need to add a Testomat.io plugin to their code and execute generated commands from their project folder.\n\n```Shell\ntestomatio run --project your_project_name\n```\n\n--------------------------------\n\n### Import Automated Tests from Source Code\n\nSource: https://docs.testomat.io/getting-started\n\nThis section details the process of importing automated tests into Testomat.io. It involves selecting project parameters like framework, programming language, and operating system, then executing a generated command in the terminal. Prerequisites include having specific Node.js or PHP versions and package managers installed.\n\n```Shell\nnpm install -g @testomatio/cli\ntestomatio import --project your_project_name\n```\n\n--------------------------------\n\n### Development Setup and Contribution\n\nSource: https://docs.testomat.io/test-reporting/python\n\nInstructions for setting up the development environment using Python 3.12, installing dependencies, and contributing code.\n\n```bash\npip install \".[dev]\"\n```\n\n```bash\npython ./smoke.py\n```\n\n```bash\ncz commit\n```\n\n```bash\ncz bump\n```\n\n```bash\ngit push remoteName branchName --tags\n```\n\n--------------------------------\n\n### Cypress: Setup Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nGuides on integrating the Testomat.io reporter with Cypress, providing instructions for both Cypress versions below and above 10.0.0. This setup allows for automatic artifact uploads.\n\n```javascript\n// For Cypress < 10.0.0 in cypress/plugins/index.js\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = (on, config) => {\n  // ... other plugins\n  on('task', {\n    // ... other tasks\n    testomat: testomat\n  });\n};\n\n// For Cypress >= 10.0.0 in cypress.config.js(ts)\nconst { defineConfig } = require('cypress');\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = defineConfig({\n  // ... other config\n  e2e: {\n    setupNodeEvents(on, config) {\n      // implement node event listeners here\n      on('task', {\n        testomat: testomat\n      });\n    },\n  },\n});\n```\n\n--------------------------------\n\n### Install Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/junit\n\nInstall the Testomat.io reporter package using npm or yarn to enable integration with your test runner.\n\n```bash\nnpm install @testomatio/reporter",
            "codeBlocks": [
              {
                "language": "Shell",
                "code": "testomatio run --project your_project_name",
                "context": "cking 'Extra', selecting 'Setup Automated', and choosing the appropriate framework. Users may need to add a Testomat.io plugin to their code and execute generated commands from their project folder."
              },
              {
                "language": "Shell",
                "code": "npm install -g @testomatio/cli\ntestomatio import --project your_project_name",
                "context": "amework, programming language, and operating system, then executing a generated command in the terminal. Prerequisites include having specific Node.js or PHP versions and package managers installed."
              },
              {
                "language": "bash",
                "code": "pip install \".[dev]\"",
                "context": "Source: https://docs.testomat.io/test-reporting/python\n\nInstructions for setting up the development environment using Python 3.12, installing dependencies, and contributing code."
              },
              {
                "language": "bash",
                "code": "python ./smoke.py",
                "context": "```bash\npip install \".[dev]\"\n```"
              },
              {
                "language": "bash",
                "code": "cz commit",
                "context": "```bash\npython ./smoke.py\n```"
              },
              {
                "language": "bash",
                "code": "cz bump",
                "context": "```bash\ncz commit\n```"
              },
              {
                "language": "bash",
                "code": "git push remoteName branchName --tags",
                "context": "```bash\ncz bump\n```"
              },
              {
                "language": "javascript",
                "code": "// For Cypress < 10.0.0 in cypress/plugins/index.js\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = (on, config) => {\n  // ... other plugins\n  on('task', {\n    // ... other tasks\n    testomat: testomat\n  });\n};\n\n// For Cypress >= 10.0.0 in cypress.config.js(ts)\nconst { defineConfig } = require('cypress');\nconst testomat = require('@testomat/cypress');\n\nmodule.exports = defineConfig({\n  // ... other config\n  e2e: {\n    setupNodeEvents(on, config) {\n      // implement node event listeners here\n      on('task', {\n        testomat: testomat\n      });\n    },\n  },\n});",
                "context": "ng/frameworks\n\nGuides on integrating the Testomat.io reporter with Cypress, providing instructions for both Cypress versions below and above 10.0.0. This setup allows for automatic artifact uploads."
              }
            ]
          },
          {
            "title": "or",
            "type": "other",
            "content": "yarn add @testomatio/reporter\n```\n\n--------------------------------\n\n### Install Testomat.io Reporter Package\n\nSource: https://docs.testomat.io/reference/reporter\n\nInstructions on how to install the Testomat.io reporter package using common package managers. This is the initial step to enable test result reporting to app.testomat.io.\n\n```bash\nnpm install @testomatio/reporter",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install Testomat.io Reporter Package\n\nSource: https://docs.testomat.io/reference/reporter\n\nInstructions on how to install the Testomat.io reporter package using common package managers. This is the initial step to enable test result reporting to app.testomat.io.",
                "context": "yarn add @testomatio/reporter"
              }
            ]
          },
          {
            "title": "pnpm add @testomatio/reporter",
            "type": "other",
            "content": "```\n\n--------------------------------\n\n### CodeceptJS Example Command\n\nSource: https://docs.testomat.io/integrations/continuous-integration/jenkins\n\nExample of how to pass configured parameters as environment variables to a test runner, specifically using CodeceptJS. This demonstrates how Jenkins parameters are utilized within the test execution environment.\n\n```bash\nnpx codeceptjs run --grep \"$grep\" --env $testomatio --test \"$run\"\n```\n\n--------------------------------\n\n### Vitest: Install Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstalls the Testomat.io reporter package for Vitest using npm or yarn. This is the first step to integrating Testomat.io with your Vitest test suite.\n\n```bash\nnpm install --save-dev @testomat/vitest",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### CodeceptJS Example Command\n\nSource: https://docs.testomat.io/integrations/continuous-integration/jenkins\n\nExample of how to pass configured parameters as environment variables to a test runner, specifically using CodeceptJS. This demonstrates how Jenkins parameters are utilized within the test execution environment.",
                "context": ""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Vitest: Install Testomat.io Reporter\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstalls the Testomat.io reporter package for Vitest using npm or yarn. This is the first step to integrating Testomat.io with your Vitest test suite.",
                "context": "\n```bash\nnpx codeceptjs run --grep \"$grep\" --env $testomatio --test \"$run\""
              }
            ]
          },
          {
            "title": "or",
            "type": "other",
            "content": "yarn add --dev @testomat/vitest\n```\n\n--------------------------------\n\n### Install and Use Testomat.io Reporter for Nightwatch\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstructions for installing the Testomat.io reporter for Nightwatch and configuring your test run command. This allows Nightwatch tests to report results to Testomat.io.\n\n```bash\nnpm install @testomatio/reporter --save-dev\nTESTOMATIO={API_KEY} npx nightwatch --reporter @testomatio/reporter/nightwatch\n```\n\n--------------------------------\n\n### Start Test Run with Testomat.io CLI\n\nSource: https://docs.testomat.io/test-reporting/cli\n\nStarts a new test run and retrieves its unique ID. This command requires the Testomat.io API key to be configured via the TESTOMATIO environment variable. It supports loading environment variables from a specified file using the --env-file option.\n\n```bash\nnpx @testomatio/reporter start",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install and Use Testomat.io Reporter for Nightwatch\n\nSource: https://docs.testomat.io/test-reporting/frameworks\n\nInstructions for installing the Testomat.io reporter for Nightwatch and configuring your test run command. This allows Nightwatch tests to report results to Testomat.io.",
                "context": "yarn add --dev @testomat/vitest"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Start Test Run with Testomat.io CLI\n\nSource: https://docs.testomat.io/test-reporting/cli\n\nStarts a new test run and retrieves its unique ID. This command requires the Testomat.io API key to be configured via the TESTOMATIO environment variable. It supports loading environment variables from a specified file using the --env-file option.",
                "context": "```bash\nnpm install @testomatio/reporter --save-dev\nTESTOMATIO={API_KEY} npx nightwatch --reporter @testomatio/reporter/nightwatch"
              }
            ]
          },
          {
            "title": "Example with env-file:",
            "type": "example",
            "content": "npx @testomatio/reporter start --env-file .env\n```",
            "codeBlocks": []
          }
        ]
      },
      {
        "packageId": "/teamcapybara/capybara",
        "packageName": "capybara",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:49.010Z",
        "content": "### Capybara with Test::Unit Setup\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nProvides a base class `CapybaraTestCase` for integrating Capybara with Test::Unit. Includes `Capybara::DSL` and overrides `teardown` to reset sessions and use the default driver after each test.\n\n```ruby\nrequire 'capybara/dsl'\n\nclass CapybaraTestCase < Test::Unit::TestCase\n  include Capybara::DSL\n\n  def teardown\n    Capybara.reset_sessions!\n    Capybara.use_default_driver\n  end\nend\n```\n\n--------------------------------\n\n### Install Capybara Gem\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nThis code snippet shows how to add the Capybara gem to your project's Gemfile. Ensure you are using Ruby 3.0.0 or later. After adding this line, run 'bundle install' to install the gem.\n\n```ruby\ngem 'capybara'\n```\n\n--------------------------------\n\n### Cucumber BDD Feature Tests with Capybara\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis example shows how to integrate Capybara with Cucumber for Behavior-Driven Development (BDD). It includes setup in `env.rb`, step definitions for user actions in `user_steps.rb`, and a feature file (`user_authentication.feature`) defining a user authentication scenario.\n\n```ruby\n# features/support/env.rb\nrequire 'capybara/cucumber'\n\nCapybara.default_driver = :selenium_chrome\nCapybara.app = MyRackApp\n\n# features/step_definitions/user_steps.rb\nGiven('a user exists with email {string}') do |email|\n  User.create(email: email, password: 'password123')\nend\n\nWhen('I sign in as {string}') do |email|\n  visit '/login'\n  fill_in 'Email', with: email\n  fill_in 'Password', with: 'password123'\n  click_button 'Sign In'\nend\n\nThen('I should see the dashboard') do\n  expect(page).to have_content('Dashboard')\n  expect(page).to have_current_path('/dashboard')\nend\n\nWhen('I fill in the search form with {string}') do |query|\n  within('#search-form') do\n    fill_in 'q', with: query\n    click_button 'Search'\n  end\nend\n\nThen('I should see {int} search results') do |count|\n  expect(page).to have_selector('.search-result', count: count)\nend\n\n# features/user_authentication.feature\n@javascript\nFeature: User Authentication\n  As a user\n  I want to sign in to my account\n  So that I can access my dashboard\n\n  Scenario: Successful login\n    Given a user exists with email \"user@example.com\"\n    When I sign in as \"user@example.com\"\n    Then I should see the dashboard\n    And I should see \"Welcome back!\"\n```\n\n--------------------------------\n\n### Capybara with Cucumber Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nIntegrates Capybara with Cucumber for acceptance testing. Requires manual loading of `capybara/cucumber` if not using Rails. Allows using Capybara DSL in steps and tagging scenarios with `@javascript` to enable JavaScript drivers.\n\n```ruby\nrequire 'capybara/cucumber'\nCapybara.app = MyRackApp\n```\n\n```ruby\nWhen /I sign in/ do\n  within(\"#session\") do\n    fill_in 'Email', with: 'user@example.com'\n    fill_in 'Password', with: 'password'\n  end\n  click_button 'Sign in'\nend\n```\n\n```ruby\n@javascript\nScenario: do something Ajaxy\n  When I click the Ajax link\n  ...\n```\n\n--------------------------------\n\n### RSpec Integration with Capybara for Feature Testing\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet illustrates how to integrate Capybara with RSpec for writing feature tests. It includes setup in `spec_helper.rb` or `rails_helper.rb` and provides examples of feature specs for user authentication, login failures, password resets, and shopping cart interactions.\n\n```ruby\n# spec_helper.rb or rails_helper.rb\nrequire 'capybara/rspec'\n\nRSpec.configure do |config|\n  config.include Capybara::DSL\nend\n\n# Feature spec example\nRSpec.describe 'User authentication', type: :feature do\n  before do\n    User.create(email: 'user@example.com', password: 'password123')\n  end\n\n  scenario 'successful login' do\n    visit '/login'\n\n    within('#login-form') do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'password123'\n      click_button 'Sign In'\n    end\n\n    expect(page).to have_content('Welcome back!')\n    expect(page).to have_current_path('/dashboard')\n    expect(page).to have_link('Logout')\n  end\n\n  scenario 'failed login with invalid credentials', js: true do\n    visit '/login'\n\n    fill_in 'Email', with: 'wrong@example.com'\n    fill_in 'Password', with: 'wrongpassword'\n    click_button 'Sign In'\n\n    expect(page).to have_css('.alert-error', text: 'Invalid credentials')\n    expect(page).to have_current_path('/login')\n  end\n\n  scenario 'password reset flow', driver: :selenium_headless do\n    visit '/login'\n    click_link 'Forgot password?'\n\n    expect(page).to have_content('Reset your password')\n\n    fill_in 'Email', with: 'user@example.com'\n    click_button 'Send reset link'\n\n    expect(page).to have_content('Check your email')\n  end\nend\n\n# Using feature/scenario DSL\nfeature 'Shopping cart' do\n  background do\n    @product = Product.create(name: 'Ruby Book', price: 29.99)\n  end\n\n  scenario 'adding items to cart' do\n    visit \"/products/#{@product.id}\"\n    click_button 'Add to Cart'\n\n    expect(page).to have_content('Item added to cart')\n\n    visit '/cart'\n    expect(page).to have_content('Ruby Book')\n    expect(page).to have_content('$29.99')\n  end\nend\n```\n\n--------------------------------\n\n### Capybara with RSpec Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nEnables Capybara support in RSpec 3.5+ by requiring `capybara/rspec`. Supports Rails `spec/features` or `spec/system` directories and `type: :feature` or `type: :system` tags. Allows specifying JavaScript drivers with `js: true` or `:driver` option.\n\n```ruby\nrequire 'capybara/rspec'\n```\n\n```ruby\ndescribe \" the signin process\", type: :feature do\n  before :each do\n    User.create(email: 'user@example.com', password: 'password')\n  end\n\n  it \"signs me in\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'password'\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Success'\n  end\nend\n```\n\n```ruby\ndescribe 'some stuff which requires js', js: true do\n  it 'will use the default js driver'\n  it 'will switch to one specific driver', driver: :selenium\nend\n```\n\n```ruby\nfeature \"Signing in\" do\n  background do\n    User.create(email: 'user@example.com', password: 'caplin')\n  end\n\n  scenario \"Signing in with correct credentials\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'caplin'\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Success'\n  end\n\n  given(:other_user) { User.create(email: 'other@example.com', password: 'rous') }\n\n  scenario \"Signing in as another user\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: other_user.email\n      fill_in 'Password', with: other_user.password\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Invalid email or password'\n  end\nend\n```\n\n```ruby\nRSpec.describe \"todos/show.html.erb\", type: :view do\n  it \"displays the todo title\" do\n    assign :todo, Todo.new(title: \"Buy milk\")\n\n    render\n\n    expect(rendered).to have_css(\"header h1\", text: \"Buy milk\")\n  end\nend\n```\n\n--------------------------------\n\n### Capybara: Basic Post Creation Test (Ruby)\n\nSource: https://github.com/teamcapybara/capybara/wiki/Asynchronous-Javascript-drivers\n\nA basic Capybara test to create a new post. This example may fail due to asynchronous server processing.\n\n```ruby\nvisit \"/posts/new\"\nfill_in :title, :with => \"Hello\"\nclick_button \"Create Post\"\nPost.last.title.should == \"Hello\"\n```\n\n--------------------------------\n\n### Run Capybara Test Suite with Different Browsers\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nProvides bash commands to execute the Capybara test suite using different browser drivers. Requires appropriate WebDriver executables (geckodriver for Firefox, chromedriver for Chrome) to be installed and accessible in the system's PATH.\n\n```bash\nbundle install\nbundle exec rake  # run the test suite with Firefox - requires `geckodriver` to be installed\nbundle exec rake spec_chrome # run the test suite with Chrome - require `chromedriver` to be installed\n```\n\n--------------------------------\n\n### Switch Capybara Driver in Minitest Test\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nThis example demonstrates how to temporarily switch the Capybara driver for a specific test case within an ActionDispatch::IntegrationTest. It sets the `Capybara.current_driver` in the `setup` block and comments indicate the test will run with Selenium.\n\n```ruby\nclass BlogTest < ActionDispatch::IntegrationTest\n  setup do\n    Capybara.current_driver = Capybara.javascript_driver # :selenium by default\n  end\n\n  test 'shows blog posts' do\n    # ... this test is run with Selenium ...\n  end\nend\n```\n\n--------------------------------\n\n### Querying Page Content in Capybara\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet shows how to query the content and elements on a web page using Capybara. It includes methods to check for the presence or absence of text (`has_content?`, `has_no_content?`), CSS selectors (`has_css?`, `has_no_css?`), and XPath expressions (`has_xpath?`). The examples demonstrate basic usage and incorporating a `wait` parameter for asynchronous content.\n\n```ruby\n# Check for text content\npage.has_content?('Welcome back!')        # => true\npage.has_text?('Error', wait: 3)         # => false\npage.has_no_content?('Logged out')       # => true\n\n# Check for CSS selectors\npage.has_css?('div.alert-success')       # => true\npage.has_no_css?('.error-message')       # => true\n\n# Check for XPath\npage.has_xpath?('.//table//tr', count: 5) # => true\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Capybara with Test::Unit Setup\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nProvides a base class `CapybaraTestCase` for integrating Capybara with Test::Unit. Includes `Capybara::DSL` and overrides `teardown` to reset sessions and use the default driver after each test.\n\n```ruby\nrequire 'capybara/dsl'\n\nclass CapybaraTestCase < Test::Unit::TestCase\n  include Capybara::DSL\n\n  def teardown\n    Capybara.reset_sessions!\n    Capybara.use_default_driver\n  end\nend\n```\n\n--------------------------------\n\n### Install Capybara Gem\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nThis code snippet shows how to add the Capybara gem to your project's Gemfile. Ensure you are using Ruby 3.0.0 or later. After adding this line, run 'bundle install' to install the gem.\n\n```ruby\ngem 'capybara'\n```\n\n--------------------------------\n\n### Cucumber BDD Feature Tests with Capybara\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis example shows how to integrate Capybara with Cucumber for Behavior-Driven Development (BDD). It includes setup in `env.rb`, step definitions for user actions in `user_steps.rb`, and a feature file (`user_authentication.feature`) defining a user authentication scenario.\n\n```ruby",
            "codeBlocks": [
              {
                "language": "ruby",
                "code": "require 'capybara/dsl'\n\nclass CapybaraTestCase < Test::Unit::TestCase\n  include Capybara::DSL\n\n  def teardown\n    Capybara.reset_sessions!\n    Capybara.use_default_driver\n  end\nend",
                "context": "md\n\nProvides a base class `CapybaraTestCase` for integrating Capybara with Test::Unit. Includes `Capybara::DSL` and overrides `teardown` to reset sessions and use the default driver after each test."
              },
              {
                "language": "ruby",
                "code": "gem 'capybara'",
                "context": "r/README.md\n\nThis code snippet shows how to add the Capybara gem to your project's Gemfile. Ensure you are using Ruby 3.0.0 or later. After adding this line, run 'bundle install' to install the gem."
              }
            ]
          },
          {
            "title": "features/support/env.rb",
            "type": "other",
            "content": "require 'capybara/cucumber'\n\nCapybara.default_driver = :selenium_chrome\nCapybara.app = MyRackApp",
            "codeBlocks": []
          },
          {
            "title": "features/step_definitions/user_steps.rb",
            "type": "other",
            "content": "Given('a user exists with email {string}') do |email|\n  User.create(email: email, password: 'password123')\nend\n\nWhen('I sign in as {string}') do |email|\n  visit '/login'\n  fill_in 'Email', with: email\n  fill_in 'Password', with: 'password123'\n  click_button 'Sign In'\nend\n\nThen('I should see the dashboard') do\n  expect(page).to have_content('Dashboard')\n  expect(page).to have_current_path('/dashboard')\nend\n\nWhen('I fill in the search form with {string}') do |query|\n  within('#search-form') do\n    fill_in 'q', with: query\n    click_button 'Search'\n  end\nend\n\nThen('I should see {int} search results') do |count|\n  expect(page).to have_selector('.search-result', count: count)\nend",
            "codeBlocks": []
          },
          {
            "title": "features/user_authentication.feature",
            "type": "other",
            "content": "@javascript\nFeature: User Authentication\n  As a user\n  I want to sign in to my account\n  So that I can access my dashboard\n\n  Scenario: Successful login\n    Given a user exists with email \"user@example.com\"\n    When I sign in as \"user@example.com\"\n    Then I should see the dashboard\n    And I should see \"Welcome back!\"\n```\n\n--------------------------------\n\n### Capybara with Cucumber Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nIntegrates Capybara with Cucumber for acceptance testing. Requires manual loading of `capybara/cucumber` if not using Rails. Allows using Capybara DSL in steps and tagging scenarios with `@javascript` to enable JavaScript drivers.\n\n```ruby\nrequire 'capybara/cucumber'\nCapybara.app = MyRackApp\n```\n\n```ruby\nWhen /I sign in/ do\n  within(\"#session\") do\n    fill_in 'Email', with: 'user@example.com'\n    fill_in 'Password', with: 'password'\n  end\n  click_button 'Sign in'\nend\n```\n\n```ruby\n@javascript\nScenario: do something Ajaxy\n  When I click the Ajax link\n  ...\n```\n\n--------------------------------\n\n### RSpec Integration with Capybara for Feature Testing\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet illustrates how to integrate Capybara with RSpec for writing feature tests. It includes setup in `spec_helper.rb` or `rails_helper.rb` and provides examples of feature specs for user authentication, login failures, password resets, and shopping cart interactions.\n\n```ruby",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Capybara with Cucumber Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nIntegrates Capybara with Cucumber for acceptance testing. Requires manual loading of `capybara/cucumber` if not using Rails. Allows using Capybara DSL in steps and tagging scenarios with `@javascript` to enable JavaScript drivers.",
                "context": "    When I sign in as \"user@example.com\"\n    Then I should see the dashboard\n    And I should see \"Welcome back!\""
              },
              {
                "language": "text",
                "code": "",
                "context": "```ruby\nrequire 'capybara/cucumber'\nCapybara.app = MyRackApp"
              },
              {
                "language": "text",
                "code": "",
                "context": "  end\n  click_button 'Sign in'\nend"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### RSpec Integration with Capybara for Feature Testing\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet illustrates how to integrate Capybara with RSpec for writing feature tests. It includes setup in `spec_helper.rb` or `rails_helper.rb` and provides examples of feature specs for user authentication, login failures, password resets, and shopping cart interactions.",
                "context": "Scenario: do something Ajaxy\n  When I click the Ajax link\n  ..."
              }
            ]
          },
          {
            "title": "spec_helper.rb or rails_helper.rb",
            "type": "other",
            "content": "require 'capybara/rspec'\n\nRSpec.configure do |config|\n  config.include Capybara::DSL\nend",
            "codeBlocks": []
          },
          {
            "title": "Feature spec example",
            "type": "example",
            "content": "RSpec.describe 'User authentication', type: :feature do\n  before do\n    User.create(email: 'user@example.com', password: 'password123')\n  end\n\n  scenario 'successful login' do\n    visit '/login'\n\n    within('#login-form') do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'password123'\n      click_button 'Sign In'\n    end\n\n    expect(page).to have_content('Welcome back!')\n    expect(page).to have_current_path('/dashboard')\n    expect(page).to have_link('Logout')\n  end\n\n  scenario 'failed login with invalid credentials', js: true do\n    visit '/login'\n\n    fill_in 'Email', with: 'wrong@example.com'\n    fill_in 'Password', with: 'wrongpassword'\n    click_button 'Sign In'\n\n    expect(page).to have_css('.alert-error', text: 'Invalid credentials')\n    expect(page).to have_current_path('/login')\n  end\n\n  scenario 'password reset flow', driver: :selenium_headless do\n    visit '/login'\n    click_link 'Forgot password?'\n\n    expect(page).to have_content('Reset your password')\n\n    fill_in 'Email', with: 'user@example.com'\n    click_button 'Send reset link'\n\n    expect(page).to have_content('Check your email')\n  end\nend",
            "codeBlocks": []
          },
          {
            "title": "Using feature/scenario DSL",
            "type": "other",
            "content": "feature 'Shopping cart' do\n  background do\n    @product = Product.create(name: 'Ruby Book', price: 29.99)\n  end\n\n  scenario 'adding items to cart' do\n    visit \"/products/#{@product.id}\"\n    click_button 'Add to Cart'\n\n    expect(page).to have_content('Item added to cart')\n\n    visit '/cart'\n    expect(page).to have_content('Ruby Book')\n    expect(page).to have_content('$29.99')\n  end\nend\n```\n\n--------------------------------\n\n### Capybara with RSpec Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nEnables Capybara support in RSpec 3.5+ by requiring `capybara/rspec`. Supports Rails `spec/features` or `spec/system` directories and `type: :feature` or `type: :system` tags. Allows specifying JavaScript drivers with `js: true` or `:driver` option.\n\n```ruby\nrequire 'capybara/rspec'\n```\n\n```ruby\ndescribe \" the signin process\", type: :feature do\n  before :each do\n    User.create(email: 'user@example.com', password: 'password')\n  end\n\n  it \"signs me in\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'password'\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Success'\n  end\nend\n```\n\n```ruby\ndescribe 'some stuff which requires js', js: true do\n  it 'will use the default js driver'\n  it 'will switch to one specific driver', driver: :selenium\nend\n```\n\n```ruby\nfeature \"Signing in\" do\n  background do\n    User.create(email: 'user@example.com', password: 'caplin')\n  end\n\n  scenario \"Signing in with correct credentials\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: 'user@example.com'\n      fill_in 'Password', with: 'caplin'\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Success'\n  end\n\n  given(:other_user) { User.create(email: 'other@example.com', password: 'rous') }\n\n  scenario \"Signing in as another user\" do\n    visit '/sessions/new'\n    within(\"#session\") do\n      fill_in 'Email', with: other_user.email\n      fill_in 'Password', with: other_user.password\n    end\n    click_button 'Sign in'\n    expect(page).to have_content 'Invalid email or password'\n  end\nend\n```\n\n```ruby\nRSpec.describe \"todos/show.html.erb\", type: :view do\n  it \"displays the todo title\" do\n    assign :todo, Todo.new(title: \"Buy milk\")\n\n    render\n\n    expect(rendered).to have_css(\"header h1\", text: \"Buy milk\")\n  end\nend\n```\n\n--------------------------------\n\n### Capybara: Basic Post Creation Test (Ruby)\n\nSource: https://github.com/teamcapybara/capybara/wiki/Asynchronous-Javascript-drivers\n\nA basic Capybara test to create a new post. This example may fail due to asynchronous server processing.\n\n```ruby\nvisit \"/posts/new\"\nfill_in :title, :with => \"Hello\"\nclick_button \"Create Post\"\nPost.last.title.should == \"Hello\"\n```\n\n--------------------------------\n\n### Run Capybara Test Suite with Different Browsers\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nProvides bash commands to execute the Capybara test suite using different browser drivers. Requires appropriate WebDriver executables (geckodriver for Firefox, chromedriver for Chrome) to be installed and accessible in the system's PATH.\n\n```bash\nbundle install\nbundle exec rake  # run the test suite with Firefox - requires `geckodriver` to be installed\nbundle exec rake spec_chrome # run the test suite with Chrome - require `chromedriver` to be installed\n```\n\n--------------------------------\n\n### Switch Capybara Driver in Minitest Test\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nThis example demonstrates how to temporarily switch the Capybara driver for a specific test case within an ActionDispatch::IntegrationTest. It sets the `Capybara.current_driver` in the `setup` block and comments indicate the test will run with Selenium.\n\n```ruby\nclass BlogTest < ActionDispatch::IntegrationTest\n  setup do\n    Capybara.current_driver = Capybara.javascript_driver # :selenium by default\n  end\n\n  test 'shows blog posts' do\n    # ... this test is run with Selenium ...\n  end\nend\n```\n\n--------------------------------\n\n### Querying Page Content in Capybara\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet shows how to query the content and elements on a web page using Capybara. It includes methods to check for the presence or absence of text (`has_content?`, `has_no_content?`), CSS selectors (`has_css?`, `has_no_css?`), and XPath expressions (`has_xpath?`). The examples demonstrate basic usage and incorporating a `wait` parameter for asynchronous content.\n\n```ruby",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Capybara with RSpec Setup and Usage\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nEnables Capybara support in RSpec 3.5+ by requiring `capybara/rspec`. Supports Rails `spec/features` or `spec/system` directories and `type: :feature` or `type: :system` tags. Allows specifying JavaScript drivers with `js: true` or `:driver` option.",
                "context": "    expect(page).to have_content('$29.99')\n  end\nend"
              },
              {
                "language": "text",
                "code": "",
                "context": "\n```ruby\nrequire 'capybara/rspec'"
              },
              {
                "language": "text",
                "code": "",
                "context": "    expect(page).to have_content 'Success'\n  end\nend"
              },
              {
                "language": "text",
                "code": "",
                "context": "  it 'will use the default js driver'\n  it 'will switch to one specific driver', driver: :selenium\nend"
              },
              {
                "language": "text",
                "code": "",
                "context": "    expect(page).to have_content 'Invalid email or password'\n  end\nend"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Capybara: Basic Post Creation Test (Ruby)\n\nSource: https://github.com/teamcapybara/capybara/wiki/Asynchronous-Javascript-drivers\n\nA basic Capybara test to create a new post. This example may fail due to asynchronous server processing.",
                "context": "    expect(rendered).to have_css(\"header h1\", text: \"Buy milk\")\n  end\nend"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Run Capybara Test Suite with Different Browsers\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nProvides bash commands to execute the Capybara test suite using different browser drivers. Requires appropriate WebDriver executables (geckodriver for Firefox, chromedriver for Chrome) to be installed and accessible in the system's PATH.",
                "context": "fill_in :title, :with => \"Hello\"\nclick_button \"Create Post\"\nPost.last.title.should == \"Hello\""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Switch Capybara Driver in Minitest Test\n\nSource: https://github.com/teamcapybara/capybara/blob/master/README.md\n\nThis example demonstrates how to temporarily switch the Capybara driver for a specific test case within an ActionDispatch::IntegrationTest. It sets the `Capybara.current_driver` in the `setup` block and comments indicate the test will run with Selenium.",
                "context": "all\nbundle exec rake  # run the test suite with Firefox - requires `geckodriver` to be installed\nbundle exec rake spec_chrome # run the test suite with Chrome - require `chromedriver` to be installed"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Querying Page Content in Capybara\n\nSource: https://context7.com/teamcapybara/capybara/llms.txt\n\nThis snippet shows how to query the content and elements on a web page using Capybara. It includes methods to check for the presence or absence of text (`has_content?`, `has_no_content?`), CSS selectors (`has_css?`, `has_no_css?`), and XPath expressions (`has_xpath?`). The examples demonstrate basic usage and incorporating a `wait` parameter for asynchronous content.",
                "context": "    # ... this test is run with Selenium ...\n  end\nend"
              }
            ]
          },
          {
            "title": "Check for text content",
            "type": "other",
            "content": "page.has_content?('Welcome back!')        # => true\npage.has_text?('Error', wait: 3)         # => false\npage.has_no_content?('Logged out')       # => true",
            "codeBlocks": []
          },
          {
            "title": "Check for CSS selectors",
            "type": "other",
            "content": "page.has_css?('div.alert-success')       # => true\npage.has_no_css?('.error-message')       # => true",
            "codeBlocks": []
          },
          {
            "title": "Check for XPath",
            "type": "other",
            "content": "page.has_xpath?('.//table//tr', count: 5) # => true\n```",
            "codeBlocks": []
          }
        ]
      },
      {
        "packageId": "/x-datainitiative/tick",
        "packageName": "tick",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:52.644Z",
        "content": "### In-place Installation and PYTHONPATH Setup (Python)\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nBuilds C++ extensions in-place for development and configures the PYTHONPATH environment variable to allow using the package from the build directory.\n\n```shell\npython setup.py build_ext --inplace\nexport PYTHONPATH=$PYTHONPATH:$PWD\n```\n\n--------------------------------\n\n### Build and Install Tick from Source\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nBuilds and installs tick from its source code using the setup.py script. This compiles C++ extensions and installs them into the Python environment's site-packages.\n\n```shell\npython setup.py build install\n```\n\n--------------------------------\n\n### Build and Install SWIG from Source\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nClones, configures, builds, and installs SWIG from source. This is necessary for source installations of tick if precompiled binaries are not available or if a specific version is required.\n\n```shell\ngit clone https://github.com/swig/swig -b rel-4.0.0 swig && \\\n    cd swig && ./autogen.sh && ./configure --without-pcre && \\\n    make && make install\n```\n\n--------------------------------\n\n### Execute Windows Automated Installation Script\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nRuns a provided shell script designed to automate the setup process for tick on Windows. This script requires manual downloading of wheel files.\n\n```shell\n./tools/windows/install.sh\n```\n\n--------------------------------\n\n### Install cpplint and gtest for Tick Development\n\nSource: https://github.com/x-datainitiative/tick/blob/master/CONTRIBUTING.md\n\nThis snippet shows the commands to install cpplint via pip and gtest from its GitHub repository, including build and installation steps. These are dependencies for testing C++ code in the tick package.\n\n```shell\npip install cpplint\n```\n\n```shell\ngit clone https://github.com/google/googletest.git\n(cd googletest && mkdir -p build && cd build && cmake .. && make && make install)\n```\n\n--------------------------------\n\n### Clone Tick Repository and Initialize Submodules\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nClones the tick source code repository and initializes its submodules, which are necessary for building from source.\n\n```shell\ngit clone https://github.com/X-DataInitiative/tick.git\ngit submodule update --init\n```\n\n--------------------------------\n\n### Install tick using pip\n\nSource: https://github.com/x-datainitiative/tick/blob/master/README.md\n\nThis command installs the tick Python package using pip. Ensure you have Python 3.5 or newer and pip installed. The installation may take a few minutes as it builds C++ extensions.\n\n```shell\npip install tick\n```\n\n--------------------------------\n\n### Compiling Tick with Maiken Build Tool (Shell)\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nCompiles the tick project using the maiken build tool after it has been installed on the system. Requires Python 3 and pre-requisites like numpy and scipy.\n\n```shell\n./mkn.sh\n```\n\n--------------------------------\n\n### Install System Dependencies on Debian-based Systems\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nInstalls essential build tools and utilities on Debian-like systems required for compiling C++ extensions and SWIG.\n\n```shell\napt-get install -y autotools-dev automake gawk bison flex\n```\n\n--------------------------------\n\n### Verify tick installation\n\nSource: https://github.com/x-datainitiative/tick/blob/master/README.md\n\nThis command verifies the tick installation by importing the tick module in Python. If the import is successful without any errors, the installation is confirmed.\n\n```python\npython3 -c \"import tick;\"\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### In-place Installation and PYTHONPATH Setup (Python)\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nBuilds C++ extensions in-place for development and configures the PYTHONPATH environment variable to allow using the package from the build directory.\n\n```shell\npython setup.py build_ext --inplace\nexport PYTHONPATH=$PYTHONPATH:$PWD\n```\n\n--------------------------------\n\n### Build and Install Tick from Source\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nBuilds and installs tick from its source code using the setup.py script. This compiles C++ extensions and installs them into the Python environment's site-packages.\n\n```shell\npython setup.py build install\n```\n\n--------------------------------\n\n### Build and Install SWIG from Source\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nClones, configures, builds, and installs SWIG from source. This is necessary for source installations of tick if precompiled binaries are not available or if a specific version is required.\n\n```shell\ngit clone https://github.com/swig/swig -b rel-4.0.0 swig && \\\n    cd swig && ./autogen.sh && ./configure --without-pcre && \\\n    make && make install\n```\n\n--------------------------------\n\n### Execute Windows Automated Installation Script\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nRuns a provided shell script designed to automate the setup process for tick on Windows. This script requires manual downloading of wheel files.\n\n```shell\n./tools/windows/install.sh\n```\n\n--------------------------------\n\n### Install cpplint and gtest for Tick Development\n\nSource: https://github.com/x-datainitiative/tick/blob/master/CONTRIBUTING.md\n\nThis snippet shows the commands to install cpplint via pip and gtest from its GitHub repository, including build and installation steps. These are dependencies for testing C++ code in the tick package.\n\n```shell\npip install cpplint\n```\n\n```shell\ngit clone https://github.com/google/googletest.git\n(cd googletest && mkdir -p build && cd build && cmake .. && make && make install)\n```\n\n--------------------------------\n\n### Clone Tick Repository and Initialize Submodules\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nClones the tick source code repository and initializes its submodules, which are necessary for building from source.\n\n```shell\ngit clone https://github.com/X-DataInitiative/tick.git\ngit submodule update --init\n```\n\n--------------------------------\n\n### Install tick using pip\n\nSource: https://github.com/x-datainitiative/tick/blob/master/README.md\n\nThis command installs the tick Python package using pip. Ensure you have Python 3.5 or newer and pip installed. The installation may take a few minutes as it builds C++ extensions.\n\n```shell\npip install tick\n```\n\n--------------------------------\n\n### Compiling Tick with Maiken Build Tool (Shell)\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nCompiles the tick project using the maiken build tool after it has been installed on the system. Requires Python 3 and pre-requisites like numpy and scipy.\n\n```shell\n./mkn.sh\n```\n\n--------------------------------\n\n### Install System Dependencies on Debian-based Systems\n\nSource: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nInstalls essential build tools and utilities on Debian-like systems required for compiling C++ extensions and SWIG.\n\n```shell\napt-get install -y autotools-dev automake gawk bison flex\n```\n\n--------------------------------\n\n### Verify tick installation\n\nSource: https://github.com/x-datainitiative/tick/blob/master/README.md\n\nThis command verifies the tick installation by importing the tick module in Python. If the import is successful without any errors, the installation is confirmed.\n\n```python\npython3 -c \"import tick;\"\n```",
            "codeBlocks": [
              {
                "language": "shell",
                "code": "python setup.py build_ext --inplace\nexport PYTHONPATH=$PYTHONPATH:$PWD",
                "context": "m/x-datainitiative/tick/blob/master/INSTALL.md\n\nBuilds C++ extensions in-place for development and configures the PYTHONPATH environment variable to allow using the package from the build directory."
              },
              {
                "language": "shell",
                "code": "python setup.py build install",
                "context": "tive/tick/blob/master/INSTALL.md\n\nBuilds and installs tick from its source code using the setup.py script. This compiles C++ extensions and installs them into the Python environment's site-packages."
              },
              {
                "language": "shell",
                "code": "git clone https://github.com/swig/swig -b rel-4.0.0 swig && \\\n    cd swig && ./autogen.sh && ./configure --without-pcre && \\\n    make && make install",
                "context": "TALL.md\n\nClones, configures, builds, and installs SWIG from source. This is necessary for source installations of tick if precompiled binaries are not available or if a specific version is required."
              },
              {
                "language": "shell",
                "code": "./tools/windows/install.sh",
                "context": "hub.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nRuns a provided shell script designed to automate the setup process for tick on Windows. This script requires manual downloading of wheel files."
              },
              {
                "language": "shell",
                "code": "pip install cpplint",
                "context": "s snippet shows the commands to install cpplint via pip and gtest from its GitHub repository, including build and installation steps. These are dependencies for testing C++ code in the tick package."
              },
              {
                "language": "shell",
                "code": "git clone https://github.com/google/googletest.git\n(cd googletest && mkdir -p build && cd build && cmake .. && make && make install)",
                "context": "```shell\npip install cpplint\n```"
              },
              {
                "language": "shell",
                "code": "git clone https://github.com/X-DataInitiative/tick.git\ngit submodule update --init",
                "context": "Source: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nClones the tick source code repository and initializes its submodules, which are necessary for building from source."
              },
              {
                "language": "shell",
                "code": "pip install tick",
                "context": "master/README.md\n\nThis command installs the tick Python package using pip. Ensure you have Python 3.5 or newer and pip installed. The installation may take a few minutes as it builds C++ extensions."
              },
              {
                "language": "shell",
                "code": "./mkn.sh",
                "context": "atainitiative/tick/blob/master/INSTALL.md\n\nCompiles the tick project using the maiken build tool after it has been installed on the system. Requires Python 3 and pre-requisites like numpy and scipy."
              },
              {
                "language": "shell",
                "code": "apt-get install -y autotools-dev automake gawk bison flex",
                "context": "Source: https://github.com/x-datainitiative/tick/blob/master/INSTALL.md\n\nInstalls essential build tools and utilities on Debian-like systems required for compiling C++ extensions and SWIG."
              },
              {
                "language": "python",
                "code": "python3 -c \"import tick;\"",
                "context": "tiative/tick/blob/master/README.md\n\nThis command verifies the tick installation by importing the tick module in Python. If the import is successful without any errors, the installation is confirmed."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/morganstanley/testplan",
        "packageName": "testplan",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:56.123Z",
        "content": "### Running Testplan Examples on Windows\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands navigate to the examples directory, list available example categories, and then run a specific example demonstrating Testplan assertions. The last command creates a PDF report and opens it automatically.\n\n```text\n# See all the examples categories.\ncd examples\ndir\n\n# Run an example demonstrating testplan assertions.\ncd Assertions\\Basic\npython test_plan_basic.py\n```\n\n```text\n# Create a pdf report and open in automatically.\npython test_plan.py --pdf report.pdf -b\n```\n\n--------------------------------\n\n### Running Testplan Examples on Ubuntu/MacOS\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands navigate to the examples directory, list available example categories, and then run a specific example demonstrating Testplan assertions. The last command creates a PDF report and opens it automatically.\n\n```bash\n# See all the examples categories.\ncd examples\nls\n\n# Run an example demonstrating testplan assertions.\ncd Assertions/Basic\n./test_plan_basic.py\n```\n\n```bash\n# Create a pdf report and open in automatically.\n./test_plan.py --pdf report.pdf -b\n```\n\n--------------------------------\n\n### Setup Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file configures the setup for a Releaseherald plugin. It specifies the package metadata and dependencies required for the plugin to function correctly within the Releaseherald environment.\n\n```toml\n--8<-- \"example_plugin/setup.cfg\"\n```\n\n--------------------------------\n\n### Manually Starting App Driver Test Plan\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/download/App.rst\n\nThis example demonstrates how to manually start the App Driver in a Testplan test plan. It defines a test plan that starts the app driver manually.\n\n```Python\nimport os\n\nfrom testplan import Testplan\nfrom testplan.testing.multitest import MultiTest, testsuite, testcase\nfrom testplan.common.utils.path import module_resource\nfrom testplan.runners.app import App\n\nfixture_root = os.path.abspath(os.path.dirname(__file__))\n\n@testsuite\nclass SampleSuite:\n    @testcase\n    def test_case_one(self, env, result):\n        result.log('Testcase One')\n        result.equal(1, env.app.return_one())\n\n\ndef get_plan(name):\n    plan = Testplan(\n        name=name,\n        state_path='.'\n    )\n\n    app = App(\n        name='My App',\n        binary=module_resource('{{cookiecutter.app_name}}', 'app.py'),\n        autostart=False\n    )\n\n    mtest = MultiTest(\n        name='My Multitest',\n        suites=[SampleSuite()],\n        environment=[app]\n    )\n\n    plan.add(mtest)\n    return plan\n\n\nif __name__ == '__main__':\n    plan = get_plan('Autostart')\n    plan.run()\n```\n\n--------------------------------\n\n### Installing Testplan from GitHub Archive\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from a specific version archive hosted on GitHub. It downloads the .whl file and installs it using pip, allowing users to install a specific version of Testplan.\n\n```bash\npython3 -m pip install https://github.com/morganstanley/testplan/releases/download/24.9.2/testplan-24.9.2-py3-none-any.whl\n```\n\n--------------------------------\n\n### Working with Testplan Source Code\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands clone the Testplan repository, install Testplan in editable mode with development requirements, and build the interactive UI using doit.  Node.js is required for the interactive UI.\n\n```text\ngit clone https://github.com/morganstanley/testplan.git\ncd testplan\n\n# install testplan in editable mode & all dev requirements\npip install -e .\n\n# build the interactive UI (if you do not like it is opening a browserwindow remove the `-o`)\ndoit build_ui -o\n```\n\n--------------------------------\n\n### Installing Testplan from PyPI\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from the Python Package Index (PyPI) using pip. It ensures that the latest version of Testplan and its dependencies are installed in the current Python environment.\n\n```bash\npython3 -m pip install testplan\n```\n\n--------------------------------\n\n### Project Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file defines the project configuration for a Releaseherald plugin. It includes build system requirements and project metadata, ensuring the plugin can be built and installed correctly.\n\n```toml\n--8<-- \"example_plugin/pyproject.toml\"\n```\n\n--------------------------------\n\n### Dynamic Driver Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/drivers_basic.rst\n\nThis example demonstrates how to configure drivers dynamically using context values in a Testplan MultiTest environment. It shows how to define dependencies between drivers, ensuring that they start in the correct order.\n\n```python\n    # Example environment of three dynamically connecting drivers.\n    #  --------------         -----------------         ---------------\n    #  |            | ------> |               | ------> |             |\n    #  |   Client   |         |  Application  |         |   Service   |\n    #  |            | <------ |               | <------ |             |\n    #  --------------         -----------------         ---------------\n\n    ...  # Other arguments passed to MultiTest constructor\n    environment=[\n        Service(name='service'),\n        Application(name='app',\n                    host=context('service', '{{host}}'),\n                    port=context('service', '{{port}}')),\n        Client(name='client',\n               host=context('app', '{{host}}'),\n               port=context('app', '{{port}}'))\n    ],\n    ...  # Other arguments passed to MultiTest constructor\n```\n\n--------------------------------\n\n### Starting Test Environment Resources\n\nSource: https://github.com/morganstanley/testplan/blob/main/examples/Interactive/Environments/test_plan_notebook.ipynb\n\nStarts the test environment resources (server and client) using the interactive handler, specifying the test UID.\n\n```python\n# Start the test envirorment resources (Server & client).\nplan.i.start_test_resources(test_uid='Test1')\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Running Testplan Examples on Windows\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands navigate to the examples directory, list available example categories, and then run a specific example demonstrating Testplan assertions. The last command creates a PDF report and opens it automatically.\n\n```text",
            "codeBlocks": []
          },
          {
            "title": "See all the examples categories.",
            "type": "example",
            "content": "cd examples\ndir",
            "codeBlocks": []
          },
          {
            "title": "Run an example demonstrating testplan assertions.",
            "type": "example",
            "content": "cd Assertions\\Basic\npython test_plan_basic.py\n```\n\n```text",
            "codeBlocks": [
              {
                "language": "text",
                "code": "",
                "context": "cd Assertions\\Basic\npython test_plan_basic.py"
              }
            ]
          },
          {
            "title": "Create a pdf report and open in automatically.",
            "type": "other",
            "content": "python test_plan.py --pdf report.pdf -b\n```\n\n--------------------------------\n\n### Running Testplan Examples on Ubuntu/MacOS\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands navigate to the examples directory, list available example categories, and then run a specific example demonstrating Testplan assertions. The last command creates a PDF report and opens it automatically.\n\n```bash",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Running Testplan Examples on Ubuntu/MacOS\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands navigate to the examples directory, list available example categories, and then run a specific example demonstrating Testplan assertions. The last command creates a PDF report and opens it automatically.",
                "context": "python test_plan.py --pdf report.pdf -b"
              }
            ]
          },
          {
            "title": "See all the examples categories.",
            "type": "example",
            "content": "cd examples\nls",
            "codeBlocks": []
          },
          {
            "title": "Run an example demonstrating testplan assertions.",
            "type": "example",
            "content": "cd Assertions/Basic\n./test_plan_basic.py\n```\n\n```bash",
            "codeBlocks": [
              {
                "language": "text",
                "code": "",
                "context": "cd Assertions/Basic\n./test_plan_basic.py"
              }
            ]
          },
          {
            "title": "Create a pdf report and open in automatically.",
            "type": "other",
            "content": "./test_plan.py --pdf report.pdf -b\n```\n\n--------------------------------\n\n### Setup Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file configures the setup for a Releaseherald plugin. It specifies the package metadata and dependencies required for the plugin to function correctly within the Releaseherald environment.\n\n```toml\n--8<-- \"example_plugin/setup.cfg\"\n```\n\n--------------------------------\n\n### Manually Starting App Driver Test Plan\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/download/App.rst\n\nThis example demonstrates how to manually start the App Driver in a Testplan test plan. It defines a test plan that starts the app driver manually.\n\n```Python\nimport os\n\nfrom testplan import Testplan\nfrom testplan.testing.multitest import MultiTest, testsuite, testcase\nfrom testplan.common.utils.path import module_resource\nfrom testplan.runners.app import App\n\nfixture_root = os.path.abspath(os.path.dirname(__file__))\n\n@testsuite\nclass SampleSuite:\n    @testcase\n    def test_case_one(self, env, result):\n        result.log('Testcase One')\n        result.equal(1, env.app.return_one())\n\n\ndef get_plan(name):\n    plan = Testplan(\n        name=name,\n        state_path='.'\n    )\n\n    app = App(\n        name='My App',\n        binary=module_resource('{{cookiecutter.app_name}}', 'app.py'),\n        autostart=False\n    )\n\n    mtest = MultiTest(\n        name='My Multitest',\n        suites=[SampleSuite()],\n        environment=[app]\n    )\n\n    plan.add(mtest)\n    return plan\n\n\nif __name__ == '__main__':\n    plan = get_plan('Autostart')\n    plan.run()\n```\n\n--------------------------------\n\n### Installing Testplan from GitHub Archive\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from a specific version archive hosted on GitHub. It downloads the .whl file and installs it using pip, allowing users to install a specific version of Testplan.\n\n```bash\npython3 -m pip install https://github.com/morganstanley/testplan/releases/download/24.9.2/testplan-24.9.2-py3-none-any.whl\n```\n\n--------------------------------\n\n### Working with Testplan Source Code\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands clone the Testplan repository, install Testplan in editable mode with development requirements, and build the interactive UI using doit.  Node.js is required for the interactive UI.\n\n```text\ngit clone https://github.com/morganstanley/testplan.git\ncd testplan",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Setup Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file configures the setup for a Releaseherald plugin. It specifies the package metadata and dependencies required for the plugin to function correctly within the Releaseherald environment.",
                "context": "./test_plan.py --pdf report.pdf -b"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Manually Starting App Driver Test Plan\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/download/App.rst\n\nThis example demonstrates how to manually start the App Driver in a Testplan test plan. It defines a test plan that starts the app driver manually.",
                "context": "\n```toml\n--8<-- \"example_plugin/setup.cfg\""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Testplan from GitHub Archive\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from a specific version archive hosted on GitHub. It downloads the .whl file and installs it using pip, allowing users to install a specific version of Testplan.",
                "context": "if __name__ == '__main__':\n    plan = get_plan('Autostart')\n    plan.run()"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Working with Testplan Source Code\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThese commands clone the Testplan repository, install Testplan in editable mode with development requirements, and build the interactive UI using doit.  Node.js is required for the interactive UI.",
                "context": "\n```bash\npython3 -m pip install https://github.com/morganstanley/testplan/releases/download/24.9.2/testplan-24.9.2-py3-none-any.whl"
              }
            ]
          },
          {
            "title": "install testplan in editable mode & all dev requirements",
            "type": "other",
            "content": "pip install -e .",
            "codeBlocks": []
          },
          {
            "title": "build the interactive UI (if you do not like it is opening a browserwindow remove the `-o`)",
            "type": "other",
            "content": "doit build_ui -o\n```\n\n--------------------------------\n\n### Installing Testplan from PyPI\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from the Python Package Index (PyPI) using pip. It ensures that the latest version of Testplan and its dependencies are installed in the current Python environment.\n\n```bash\npython3 -m pip install testplan\n```\n\n--------------------------------\n\n### Project Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file defines the project configuration for a Releaseherald plugin. It includes build system requirements and project metadata, ensuring the plugin can be built and installed correctly.\n\n```toml\n--8<-- \"example_plugin/pyproject.toml\"\n```\n\n--------------------------------\n\n### Dynamic Driver Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/drivers_basic.rst\n\nThis example demonstrates how to configure drivers dynamically using context values in a Testplan MultiTest environment. It shows how to define dependencies between drivers, ensuring that they start in the correct order.\n\n```python\n    # Example environment of three dynamically connecting drivers.\n    #  --------------         -----------------         ---------------\n    #  |            | ------> |               | ------> |             |\n    #  |   Client   |         |  Application  |         |   Service   |\n    #  |            | <------ |               | <------ |             |\n    #  --------------         -----------------         ---------------\n\n    ...  # Other arguments passed to MultiTest constructor\n    environment=[\n        Service(name='service'),\n        Application(name='app',\n                    host=context('service', '{{host}}'),\n                    port=context('service', '{{port}}')),\n        Client(name='client',\n               host=context('app', '{{host}}'),\n               port=context('app', '{{port}}'))\n    ],\n    ...  # Other arguments passed to MultiTest constructor\n```\n\n--------------------------------\n\n### Starting Test Environment Resources\n\nSource: https://github.com/morganstanley/testplan/blob/main/examples/Interactive/Environments/test_plan_notebook.ipynb\n\nStarts the test environment resources (server and client) using the interactive handler, specifying the test UID.\n\n```python",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Testplan from PyPI\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/getting_started.rst\n\nThis command installs Testplan from the Python Package Index (PyPI) using pip. It ensures that the latest version of Testplan and its dependencies are installed in the current Python environment.",
                "context": "doit build_ui -o"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Project Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/releaseherald/docs/plugins/examples.md\n\nThis TOML file defines the project configuration for a Releaseherald plugin. It includes build system requirements and project metadata, ensuring the plugin can be built and installed correctly.",
                "context": "\n```bash\npython3 -m pip install testplan"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Dynamic Driver Configuration Example\n\nSource: https://github.com/morganstanley/testplan/blob/main/doc/en/drivers_basic.rst\n\nThis example demonstrates how to configure drivers dynamically using context values in a Testplan MultiTest environment. It shows how to define dependencies between drivers, ensuring that they start in the correct order.",
                "context": "\n```toml\n--8<-- \"example_plugin/pyproject.toml\""
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Starting Test Environment Resources\n\nSource: https://github.com/morganstanley/testplan/blob/main/examples/Interactive/Environments/test_plan_notebook.ipynb\n\nStarts the test environment resources (server and client) using the interactive handler, specifying the test UID.",
                "context": "               port=context('app', '{{port}}'))\n    ],\n    ...  # Other arguments passed to MultiTest constructor"
              }
            ]
          },
          {
            "title": "Start the test envirorment resources (Server & client).",
            "type": "other",
            "content": "plan.i.start_test_resources(test_uid='Test1')\n```",
            "codeBlocks": []
          }
        ]
      },
      {
        "packageId": "/websites/testbeats",
        "packageName": "testbeats",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:17:59.637Z",
        "content": "### Install and Verify TestBeats CLI\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nInstalls and verifies the TestBeats CLI tool. Requires Node.js for the recommended method or direct download for systems without Node.js. Outputs the installed version.\n\n```bash\nnpx testbeats --version\n```\n\n```bash\n# Download TestBeats executable\ncurl https://raw.githubusercontent.com/test-results-reporter/testbeats/main/scripts/download-latest.sh | bash\n\n# Verify installation (for Linux)\n./testbeats-linux --version\n\n# Verify installation (for macOS) \n./testbeats-macos --version\n\n# Verify installation (for Windows)\n./testbeats-win.exe --version\n```\n\n--------------------------------\n\n### Publish Test Results to TestBeats Portal\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nPublishes JUnit XML test results to the TestBeats Portal using the CLI. Requires an API key and the path to the results file. Outputs the publishing status.\n\n```bash\n# Using npx (if you have Node.js)\nnpx testbeats publish \\\n  --api-key <YOUR_API_KEY> \\\n  --junit results.xml\n\n# OR using direct executable\n./testbeats publish \\\n  --api-key <YOUR_API_KEY> \\\n  --junit results.xml\n```\n\n--------------------------------\n\n### Create Sample JUnit XML Test Results\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nCreates a sample JUnit XML file named 'results.xml'. This file simulates test results, including test suites, test cases, and failures, compatible with most testing frameworks.\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<testsuites name=\"\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"45.123\">\n  <testsuite name=\"Login Tests\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"30.456\">\n    <testcase name=\"User should be able to login with valid credentials\" classname=\"com.example.LoginTest\" time=\"15.123\"/>\n    <testcase name=\"User should not be able to login with invalid credentials\" classname=\"com.example.LoginTest\" time=\"15.333\">\n      <failure message=\"Expected login to fail but it succeeded\">\n        AssertionError: Login should have failed with invalid credentials\n        at LoginTest.testInvalidLogin(LoginTest.java:25)\n      </failure>\n    </testcase>\n  </testsuite>\n</testsuites>\n```\n\n--------------------------------\n\n### Example Configuration: Using launch_id (JSON)\n\nSource: https://docs.testbeats.com/references/extensions/report-portal-analysis\n\nAn example configuration demonstrating how to use the ReportPortal Analysis extension with a specific launch ID. This setup targets Microsoft Teams and includes the necessary ReportPortal credentials and the launch ID for analysis. It also specifies the TestNG results file for processing.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"teams\",\n      \"inputs\": {\n        \"url\": \"<teams-incoming-webhook-url>\"\n      },\n      \"extensions\": [\n        {\n          \"name\": \"report-portal-analysis\",\n          \"inputs\": {\n            \"url\": \"<report-portal-base-url>\",\n            \"api_key\": \"<api-key>\",\n            \"project\": \"<project-id>\",\n            \"launch_id\": \"<launch-id>\"\n          }\n        }\n      ]\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\"path/to/testng-results.xml\"]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Install TestBeats via NPX\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls and runs the latest version of TestBeats using NPX. This is the recommended installation method for quick access to the CLI.\n\n```bash\nnpx testbeats@latest [command]\n```\n\n--------------------------------\n\n### TestBeats Configuration File Example\n\nSource: https://docs.testbeats.com/references/configuration\n\nExample of the main TestBeats configuration file (.testbeats.json). It specifies API key, project details, results sources, communication targets, and extensions.\n\n```json\n{\n  \"api_key\": \"<your-api-key>\",\n  \"project\": \"my-awesome-project\",\n  \"run\": \"Build 123 - main\",\n  \"results\": [\n    {\n      \"type\": \"junit\",\n      \"files\": [\"**/test-results.xml\"]\n    }\n  ],\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"condition\": \"fail\",\n      \"inputs\": {\n        \"url\": \"<slack-webhook-url>\",\n        \"publish\": \"failure-details\"\n      }\n    }\n  ],\n  \"extensions\": [\n    {\n      \"name\": \"mentions\",\n      \"inputs\": {\n        \"users\": [\"john.doe@company.com\"]\n      }\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Install TestBeats via NPM Package\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls TestBeats globally using NPM, making the 'testbeats' command available system-wide. This allows for direct execution of TestBeats commands after installation.\n\n```bash\nnpm install testbeats -g\ntestbeats [command]\n```\n\n--------------------------------\n\n### Configuration Example: Single Team Channel\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nIllustrates a configuration file setup for sending failure details to a specific team channel in Slack. It uses a 'fail' condition and customizes the title and failure-only reporting.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"condition\": \"fail\",\n      \"inputs\": {\n        \"url\": \"<dev-channel-webhook>\",\n        \"publish\": \"failure-details\",\n        \"title\": \" Test Failures\",\n        \"only_failures\": true\n      }\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\n        \"path/to/testng-results.xml\"\n      ]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Configuration Example: Multiple Environments\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nDemonstrates a configuration for sending different types of test summaries to multiple Slack channels for staging and production environments. This example shows how to target different webhooks with distinct suffixes and report formats.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"inputs\": {\n        \"url\": \"<staging-webhook>\",\n        \"title_suffix\": \" - Staging\"\n      }\n    },\n    {\n      \"name\": \"slack\",\n      \"inputs\": {\n        \"url\": \"<production-webhook>\",\n        \"title_suffix\": \" - Production\",\n        \"publish\": \"test-summary-slim\"\n      }\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\n        \"path/to/testng-results.xml\"\n      ]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Create Local Development .env File\n\nSource: https://docs.testbeats.com/references/configuration/env-vars\n\nExample .env file for local development setup. Includes API keys, project configuration, webhook URLs, and CI detection overrides for testing without CI/CD platform.\n\n```bash\n# .env (local development)\nTEST_BEATS_API_KEY=tb_dev_key_123\nTEST_BEATS_PROJECT=my-project-local\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/.../dev-channel\n\n# Override CI detection for local testing\nTEST_BEATS_CI_NAME=Local Development\nTEST_BEATS_CI_USER=developer\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install and Verify TestBeats CLI\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nInstalls and verifies the TestBeats CLI tool. Requires Node.js for the recommended method or direct download for systems without Node.js. Outputs the installed version.\n\n```bash\nnpx testbeats --version\n```\n\n```bash",
            "codeBlocks": [
              {
                "language": "bash",
                "code": "npx testbeats --version",
                "context": "s.com/guides/getting-started\n\nInstalls and verifies the TestBeats CLI tool. Requires Node.js for the recommended method or direct download for systems without Node.js. Outputs the installed version."
              }
            ]
          },
          {
            "title": "Download TestBeats executable",
            "type": "other",
            "content": "curl https://raw.githubusercontent.com/test-results-reporter/testbeats/main/scripts/download-latest.sh | bash",
            "codeBlocks": []
          },
          {
            "title": "Verify installation (for Linux)",
            "type": "other",
            "content": "./testbeats-linux --version",
            "codeBlocks": []
          },
          {
            "title": "Verify installation (for macOS) ",
            "type": "other",
            "content": "./testbeats-macos --version",
            "codeBlocks": []
          },
          {
            "title": "Verify installation (for Windows)",
            "type": "other",
            "content": "./testbeats-win.exe --version\n```\n\n--------------------------------\n\n### Publish Test Results to TestBeats Portal\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nPublishes JUnit XML test results to the TestBeats Portal using the CLI. Requires an API key and the path to the results file. Outputs the publishing status.\n\n```bash",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Publish Test Results to TestBeats Portal\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nPublishes JUnit XML test results to the TestBeats Portal using the CLI. Requires an API key and the path to the results file. Outputs the publishing status.",
                "context": "./testbeats-win.exe --version"
              }
            ]
          },
          {
            "title": "Using npx (if you have Node.js)",
            "type": "other",
            "content": "npx testbeats publish \\\n  --api-key <YOUR_API_KEY> \\\n  --junit results.xml",
            "codeBlocks": []
          },
          {
            "title": "OR using direct executable",
            "type": "other",
            "content": "./testbeats publish \\\n  --api-key <YOUR_API_KEY> \\\n  --junit results.xml\n```\n\n--------------------------------\n\n### Create Sample JUnit XML Test Results\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nCreates a sample JUnit XML file named 'results.xml'. This file simulates test results, including test suites, test cases, and failures, compatible with most testing frameworks.\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<testsuites name=\"\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"45.123\">\n  <testsuite name=\"Login Tests\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"30.456\">\n    <testcase name=\"User should be able to login with valid credentials\" classname=\"com.example.LoginTest\" time=\"15.123\"/>\n    <testcase name=\"User should not be able to login with invalid credentials\" classname=\"com.example.LoginTest\" time=\"15.333\">\n      <failure message=\"Expected login to fail but it succeeded\">\n        AssertionError: Login should have failed with invalid credentials\n        at LoginTest.testInvalidLogin(LoginTest.java:25)\n      </failure>\n    </testcase>\n  </testsuite>\n</testsuites>\n```\n\n--------------------------------\n\n### Example Configuration: Using launch_id (JSON)\n\nSource: https://docs.testbeats.com/references/extensions/report-portal-analysis\n\nAn example configuration demonstrating how to use the ReportPortal Analysis extension with a specific launch ID. This setup targets Microsoft Teams and includes the necessary ReportPortal credentials and the launch ID for analysis. It also specifies the TestNG results file for processing.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"teams\",\n      \"inputs\": {\n        \"url\": \"<teams-incoming-webhook-url>\"\n      },\n      \"extensions\": [\n        {\n          \"name\": \"report-portal-analysis\",\n          \"inputs\": {\n            \"url\": \"<report-portal-base-url>\",\n            \"api_key\": \"<api-key>\",\n            \"project\": \"<project-id>\",\n            \"launch_id\": \"<launch-id>\"\n          }\n        }\n      ]\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\"path/to/testng-results.xml\"]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Install TestBeats via NPX\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls and runs the latest version of TestBeats using NPX. This is the recommended installation method for quick access to the CLI.\n\n```bash\nnpx testbeats@latest [command]\n```\n\n--------------------------------\n\n### TestBeats Configuration File Example\n\nSource: https://docs.testbeats.com/references/configuration\n\nExample of the main TestBeats configuration file (.testbeats.json). It specifies API key, project details, results sources, communication targets, and extensions.\n\n```json\n{\n  \"api_key\": \"<your-api-key>\",\n  \"project\": \"my-awesome-project\",\n  \"run\": \"Build 123 - main\",\n  \"results\": [\n    {\n      \"type\": \"junit\",\n      \"files\": [\"**/test-results.xml\"]\n    }\n  ],\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"condition\": \"fail\",\n      \"inputs\": {\n        \"url\": \"<slack-webhook-url>\",\n        \"publish\": \"failure-details\"\n      }\n    }\n  ],\n  \"extensions\": [\n    {\n      \"name\": \"mentions\",\n      \"inputs\": {\n        \"users\": [\"john.doe@company.com\"]\n      }\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Install TestBeats via NPM Package\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls TestBeats globally using NPM, making the 'testbeats' command available system-wide. This allows for direct execution of TestBeats commands after installation.\n\n```bash\nnpm install testbeats -g\ntestbeats [command]\n```\n\n--------------------------------\n\n### Configuration Example: Single Team Channel\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nIllustrates a configuration file setup for sending failure details to a specific team channel in Slack. It uses a 'fail' condition and customizes the title and failure-only reporting.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"condition\": \"fail\",\n      \"inputs\": {\n        \"url\": \"<dev-channel-webhook>\",\n        \"publish\": \"failure-details\",\n        \"title\": \" Test Failures\",\n        \"only_failures\": true\n      }\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\n        \"path/to/testng-results.xml\"\n      ]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Configuration Example: Multiple Environments\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nDemonstrates a configuration for sending different types of test summaries to multiple Slack channels for staging and production environments. This example shows how to target different webhooks with distinct suffixes and report formats.\n\n```json\n{\n  \"targets\": [\n    {\n      \"name\": \"slack\",\n      \"inputs\": {\n        \"url\": \"<staging-webhook>\",\n        \"title_suffix\": \" - Staging\"\n      }\n    },\n    {\n      \"name\": \"slack\",\n      \"inputs\": {\n        \"url\": \"<production-webhook>\",\n        \"title_suffix\": \" - Production\",\n        \"publish\": \"test-summary-slim\"\n      }\n    }\n  ],\n  \"results\": [\n    {\n      \"type\": \"testng\",\n      \"files\": [\n        \"path/to/testng-results.xml\"\n      ]\n    }\n  ]\n}\n```\n\n--------------------------------\n\n### Create Local Development .env File\n\nSource: https://docs.testbeats.com/references/configuration/env-vars\n\nExample .env file for local development setup. Includes API keys, project configuration, webhook URLs, and CI detection overrides for testing without CI/CD platform.\n\n```bash",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Create Sample JUnit XML Test Results\n\nSource: https://docs.testbeats.com/guides/getting-started\n\nCreates a sample JUnit XML file named 'results.xml'. This file simulates test results, including test suites, test cases, and failures, compatible with most testing frameworks.",
                "context": "./testbeats publish \\\n  --api-key <YOUR_API_KEY> \\\n  --junit results.xml"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Example Configuration: Using launch_id (JSON)\n\nSource: https://docs.testbeats.com/references/extensions/report-portal-analysis\n\nAn example configuration demonstrating how to use the ReportPortal Analysis extension with a specific launch ID. This setup targets Microsoft Teams and includes the necessary ReportPortal credentials and the launch ID for analysis. It also specifies the TestNG results file for processing.",
                "context": "    </testcase>\n  </testsuite>\n</testsuites>"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install TestBeats via NPX\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls and runs the latest version of TestBeats using NPX. This is the recommended installation method for quick access to the CLI.",
                "context": "    }\n  ]\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### TestBeats Configuration File Example\n\nSource: https://docs.testbeats.com/references/configuration\n\nExample of the main TestBeats configuration file (.testbeats.json). It specifies API key, project details, results sources, communication targets, and extensions.",
                "context": "\n```bash\nnpx testbeats@latest [command]"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install TestBeats via NPM Package\n\nSource: https://docs.testbeats.com/references/cli\n\nInstalls TestBeats globally using NPM, making the 'testbeats' command available system-wide. This allows for direct execution of TestBeats commands after installation.",
                "context": "    }\n  ]\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Configuration Example: Single Team Channel\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nIllustrates a configuration file setup for sending failure details to a specific team channel in Slack. It uses a 'fail' condition and customizes the title and failure-only reporting.",
                "context": "```bash\nnpm install testbeats -g\ntestbeats [command]"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Configuration Example: Multiple Environments\n\nSource: https://docs.testbeats.com/references/targets/slack\n\nDemonstrates a configuration for sending different types of test summaries to multiple Slack channels for staging and production environments. This example shows how to target different webhooks with distinct suffixes and report formats.",
                "context": "    }\n  ]\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Create Local Development .env File\n\nSource: https://docs.testbeats.com/references/configuration/env-vars\n\nExample .env file for local development setup. Includes API keys, project configuration, webhook URLs, and CI detection overrides for testing without CI/CD platform.",
                "context": "    }\n  ]\n}"
              }
            ]
          },
          {
            "title": ".env (local development)",
            "type": "other",
            "content": "TEST_BEATS_API_KEY=tb_dev_key_123\nTEST_BEATS_PROJECT=my-project-local\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/.../dev-channel",
            "codeBlocks": []
          },
          {
            "title": "Override CI detection for local testing",
            "type": "other",
            "content": "TEST_BEATS_CI_NAME=Local Development\nTEST_BEATS_CI_USER=developer\n```",
            "codeBlocks": []
          }
        ]
      },
      {
        "packageId": "/airtestproject/poco",
        "packageName": "poco",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:03.149Z",
        "content": "### Initializing Test Setup with Hunter and RPC in Python\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/netease-internal-use-template.rst\n\nThe `setUp` method is executed before each test method. This snippet demonstrates how to use `self.hunter.script` to execute remote commands and `self.hunter.rpc.remote` to obtain an RPC object for interacting with remote services, facilitating test environment setup or data retrieval.\n\n```python\ndef setUp(self):\n    # hunter\n    self.hunter.script('print 23333', lang='python')\n\n    # hunter rpc\n    remote_obj = self.hunter.rpc.remote('safaia-rpc-test')  # see http://hunter.nie.netease.com/mywork/instruction?insids=3086\n    print(remote_obj.get_value())\n```\n\n--------------------------------\n\n### Getting Poco SDK - Cloning Repository\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/implementation_guide.rst\n\nFirst clone the poco-sdk repository. Each language's SDK is located separately within 'poco-sdk/sdk/*'. You can copy the source code of the corresponding language to your project script folder.\n\n```bash\ngit clone https://github.com/AirtestProject/Poco-SDK.git\n```\n\n--------------------------------\n\n### Setting up a SimpleRPC Server (Python)\n\nSource: https://github.com/airtestproject/poco/blob/master/poco/utils/simplerpc/README.md\n\nThis snippet illustrates the basic setup for a SimpleRPC server. It shows how to instantiate an RpcServer, providing it with a chosen transport layer like TCP or SSZMQ, and then start the server to listen for incoming RPC requests.\n\n```Python\n### server\nfrom simplerpc.rpcserver import RpcServer\nfrom simplerpc.transport.tcp import TcpServer\nfrom simplerpc.transport.sszmq import SSZmqServer\ns = RpcServer(TcpServer())\n# s = RpcServer(SSZmqServer())\ns.run()\n```\n\n--------------------------------\n\n### Installing PocoUnit Framework with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/play_with_unittest_framework.rst\n\nThis snippet provides the command-line instruction to install the `pocounit` testing framework using pip, the Python package installer. It's a prerequisite for using `pocounit` in Python projects.\n\n```bash\npip install pocounit\n```\n\n--------------------------------\n\n### Installing Poco UI for Android\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/android-native-app.rst\n\nThis command installs the Poco UI library, which is essential for interacting with Android native applications. It's the first step to set up the testing environment.\n\n```bash\npip install pocoui\n```\n\n--------------------------------\n\n### Performing a Click Operation - Poco (Python)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/handling_exceptions.rst\n\nThis snippet executes a click action on the `start` UI element. This is a basic interaction to trigger an event associated with the element.\n\n```Python\nstart.click()\n```\n\n--------------------------------\n\n### Installing Poco Python Library (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/README.rst\n\nThis Bash command installs the `pocoui` Python library using `pip`. This library is the host-side component required to interact with the Poco SDK integrated into the target application, enabling UI automation.\n\n```Bash\npip install pocoui\n```\n\n--------------------------------\n\n### Installing Poco Python Library with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/DocBuilder/index.rst\n\nThis snippet demonstrates how to install the Poco Python library using the pip package manager. This is the first step to set up Poco on your host machine, enabling you to write UI automation scripts.\n\n```bash\npip install pocoui\n```\n\n--------------------------------\n\n### Defining a TestFlow Poco Test Case in Python\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/netease-internal-use-template.rst\n\nThis snippet defines the `MyTestCase` class, inheriting from `CommonCase`, which is the standard structure for a `testflow` test. It encapsulates the entire test lifecycle, including setup, test execution, and teardown phases, providing a complete example of a UI automation test.\n\n```python\nfrom testflow.lib.case.netease_case import CommonCase\n\n# CommonCase\n# Case\nclass MyTestCase(CommonCase):\n    def setUp(self):\n        # hunter\n        self.hunter.script('print 23333', lang='python')\n\n        # hunter rpc\n        remote_obj = self.hunter.rpc.remote('safaia-rpc-test')  # see http://hunter.nie.netease.com/mywork/instruction?insids=3086\n        print(remote_obj.get_value())\n\n    def runTest(self):\n        # self\n        self.poco(text='').click()\n\n        # python unittest\n        self.assertTrue(self.poco(text='').wait(3).exists(), \"\")\n\n        self.poco('btn_close').click()\n        self.poco('movetouch_panel').offspring('point_img').swipe('up')\n\n        self.assertTrue(False, '')\n\n    def tearDown(self):\n        # \n        # \n        a = 1 / 0\n```\n\n--------------------------------\n\n### Installing Dependencies for OSX Poco SDK (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/osx-app.rst\n\nThis snippet shows how to install the necessary Python libraries, `pyatomac` and `pyautogui`, required to run the OSX Poco SDK. Xcode must be installed prior to these Python dependencies.\n\n```bash\npip install pyatomac pyautogui\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Initializing Test Setup with Hunter and RPC in Python\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/netease-internal-use-template.rst\n\nThe `setUp` method is executed before each test method. This snippet demonstrates how to use `self.hunter.script` to execute remote commands and `self.hunter.rpc.remote` to obtain an RPC object for interacting with remote services, facilitating test environment setup or data retrieval.\n\n```python\ndef setUp(self):\n    # hunter\n    self.hunter.script('print 23333', lang='python')\n\n    # hunter rpc\n    remote_obj = self.hunter.rpc.remote('safaia-rpc-test')  # see http://hunter.nie.netease.com/mywork/instruction?insids=3086\n    print(remote_obj.get_value())\n```\n\n--------------------------------\n\n### Getting Poco SDK - Cloning Repository\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/implementation_guide.rst\n\nFirst clone the poco-sdk repository. Each language's SDK is located separately within 'poco-sdk/sdk/*'. You can copy the source code of the corresponding language to your project script folder.\n\n```bash\ngit clone https://github.com/AirtestProject/Poco-SDK.git\n```\n\n--------------------------------\n\n### Setting up a SimpleRPC Server (Python)\n\nSource: https://github.com/airtestproject/poco/blob/master/poco/utils/simplerpc/README.md\n\nThis snippet illustrates the basic setup for a SimpleRPC server. It shows how to instantiate an RpcServer, providing it with a chosen transport layer like TCP or SSZMQ, and then start the server to listen for incoming RPC requests.\n\n```Python\n### server\nfrom simplerpc.rpcserver import RpcServer\nfrom simplerpc.transport.tcp import TcpServer\nfrom simplerpc.transport.sszmq import SSZmqServer\ns = RpcServer(TcpServer())",
            "codeBlocks": [
              {
                "language": "python",
                "code": "def setUp(self):\n    # hunter\n    self.hunter.script('print 23333', lang='python')\n\n    # hunter rpc\n    remote_obj = self.hunter.rpc.remote('safaia-rpc-test')  # see http://hunter.nie.netease.com/mywork/instruction?insids=3086\n    print(remote_obj.get_value())",
                "context": "use `self.hunter.script` to execute remote commands and `self.hunter.rpc.remote` to obtain an RPC object for interacting with remote services, facilitating test environment setup or data retrieval."
              },
              {
                "language": "bash",
                "code": "git clone https://github.com/AirtestProject/Poco-SDK.git",
                "context": "rst\n\nFirst clone the poco-sdk repository. Each language's SDK is located separately within 'poco-sdk/sdk/*'. You can copy the source code of the corresponding language to your project script folder."
              }
            ]
          },
          {
            "title": "s = RpcServer(SSZmqServer())",
            "type": "other",
            "content": "s.run()\n```\n\n--------------------------------\n\n### Installing PocoUnit Framework with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/play_with_unittest_framework.rst\n\nThis snippet provides the command-line instruction to install the `pocounit` testing framework using pip, the Python package installer. It's a prerequisite for using `pocounit` in Python projects.\n\n```bash\npip install pocounit\n```\n\n--------------------------------\n\n### Installing Poco UI for Android\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/android-native-app.rst\n\nThis command installs the Poco UI library, which is essential for interacting with Android native applications. It's the first step to set up the testing environment.\n\n```bash\npip install pocoui\n```\n\n--------------------------------\n\n### Performing a Click Operation - Poco (Python)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/handling_exceptions.rst\n\nThis snippet executes a click action on the `start` UI element. This is a basic interaction to trigger an event associated with the element.\n\n```Python\nstart.click()\n```\n\n--------------------------------\n\n### Installing Poco Python Library (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/README.rst\n\nThis Bash command installs the `pocoui` Python library using `pip`. This library is the host-side component required to interact with the Poco SDK integrated into the target application, enabling UI automation.\n\n```Bash\npip install pocoui\n```\n\n--------------------------------\n\n### Installing Poco Python Library with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/DocBuilder/index.rst\n\nThis snippet demonstrates how to install the Poco Python library using the pip package manager. This is the first step to set up Poco on your host machine, enabling you to write UI automation scripts.\n\n```bash\npip install pocoui\n```\n\n--------------------------------\n\n### Defining a TestFlow Poco Test Case in Python\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/netease-internal-use-template.rst\n\nThis snippet defines the `MyTestCase` class, inheriting from `CommonCase`, which is the standard structure for a `testflow` test. It encapsulates the entire test lifecycle, including setup, test execution, and teardown phases, providing a complete example of a UI automation test.\n\n```python\nfrom testflow.lib.case.netease_case import CommonCase",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing PocoUnit Framework with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/play_with_unittest_framework.rst\n\nThis snippet provides the command-line instruction to install the `pocounit` testing framework using pip, the Python package installer. It's a prerequisite for using `pocounit` in Python projects.",
                "context": "s.run()"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Poco UI for Android\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/android-native-app.rst\n\nThis command installs the Poco UI library, which is essential for interacting with Android native applications. It's the first step to set up the testing environment.",
                "context": "\n```bash\npip install pocounit"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Performing a Click Operation - Poco (Python)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/poco-example/handling_exceptions.rst\n\nThis snippet executes a click action on the `start` UI element. This is a basic interaction to trigger an event associated with the element.",
                "context": "\n```bash\npip install pocoui"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Poco Python Library (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/README.rst\n\nThis Bash command installs the `pocoui` Python library using `pip`. This library is the host-side component required to interact with the Poco SDK integrated into the target application, enabling UI automation.",
                "context": "\n```Python\nstart.click()"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Poco Python Library with pip (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/DocBuilder/index.rst\n\nThis snippet demonstrates how to install the Poco Python library using the pip package manager. This is the first step to set up Poco on your host machine, enabling you to write UI automation scripts.",
                "context": "\n```Bash\npip install pocoui"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Defining a TestFlow Poco Test Case in Python\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/netease-internal-use-template.rst\n\nThis snippet defines the `MyTestCase` class, inheriting from `CommonCase`, which is the standard structure for a `testflow` test. It encapsulates the entire test lifecycle, including setup, test execution, and teardown phases, providing a complete example of a UI automation test.",
                "context": "\n```bash\npip install pocoui"
              }
            ]
          },
          {
            "title": "Case",
            "type": "other",
            "content": "class MyTestCase(CommonCase):\n    def setUp(self):\n        # hunter\n        self.hunter.script('print 23333', lang='python')\n\n        # hunter rpc\n        remote_obj = self.hunter.rpc.remote('safaia-rpc-test')  # see http://hunter.nie.netease.com/mywork/instruction?insids=3086\n        print(remote_obj.get_value())\n\n    def runTest(self):\n        # self\n        self.poco(text='').click()\n\n        # python unittest\n        self.assertTrue(self.poco(text='').wait(3).exists(), \"\")\n\n        self.poco('btn_close').click()\n        self.poco('movetouch_panel').offspring('point_img').swipe('up')\n\n        self.assertTrue(False, '')\n\n    def tearDown(self):\n        # \n        # \n        a = 1 / 0\n```\n\n--------------------------------\n\n### Installing Dependencies for OSX Poco SDK (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/osx-app.rst\n\nThis snippet shows how to install the necessary Python libraries, `pyatomac` and `pyautogui`, required to run the OSX Poco SDK. Xcode must be installed prior to these Python dependencies.\n\n```bash\npip install pyatomac pyautogui\n```",
            "codeBlocks": [
              {
                "language": "text",
                "code": "--------------------------------\n\n### Installing Dependencies for OSX Poco SDK (Bash)\n\nSource: https://github.com/airtestproject/poco/blob/master/doc/drivers/osx-app.rst\n\nThis snippet shows how to install the necessary Python libraries, `pyatomac` and `pyautogui`, required to run the OSX Poco SDK. Xcode must be installed prior to these Python dependencies.",
                "context": "        # \n        # \n        a = 1 / 0"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/websites/brikev_github_io_twd",
        "packageName": "brikev_github_io_twd",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:06.600Z",
        "content": "### Create TWD test file with component testing\n\nSource: https://brikev.github.io/twd/getting-started\n\nExample test file demonstrating TWD testing patterns including page navigation, element selection, user interaction simulation, and assertion methods. Shows both visibility testing and user event handling for interactive components.\n\n```ts\n// src/App.twd.test.ts\nimport { twd, userEvent } from \"twd-js\";\nimport { describe, it } from \"twd-js/runner\";\n\ndescribe(\"App Component\", () => {\n  it(\"should render the main heading\", async () => {\n    await twd.visit(\"/\");\n    \n    const heading = await twd.get(\"h1\");\n    heading.should(\"be.visible\");\n  });\n\n  it(\"should handle button clicks\", async () => {\n    await twd.visit(\"/\");\n    \n    const user = userEvent.setup();\n    const button = await twd.get(\"button\");\n    \n    await user.click(button.el);\n    \n    // Add your assertions here\n    const result = await twd.get(\"#result\");\n    result.should(\"have.text\", \"Button clicked!\");\n  });\n});\n```\n\n--------------------------------\n\n### Clone and Setup Project in Bash\n\nSource: https://brikev.github.io/twd/tutorial/installation\n\nThis snippet clones the tutorial repository, navigates to the project directory, checks out the setup branch, and installs dependencies using npm. It requires Git and Node.js to be installed. The input is none, output is a set up project ready for development. Limitations include requiring internet access for cloning and compatible Git setup.\n\n```bash\ngit clone git@github.com:BRIKEV/twd-docs-tutorial.git\ncd twd-docs-tutorial\ngit checkout 01-setup\nnpm i\n```\n\n--------------------------------\n\n### Install TWD package manager dependencies\n\nSource: https://brikev.github.io/twd/getting-started\n\nInstalls the TWD testing framework package using popular JavaScript package managers. Choose npm, yarn, or pnpm based on your project's package management preference. This is the first step to integrate TWD into your React application.\n\n```bash\nnpm install twd-js\n```\n\n```bash\nyarn add twd-js\n```\n\n```bash\npnpm add twd-js\n```\n\n--------------------------------\n\n### Start development server for TWD testing\n\nSource: https://brikev.github.io/twd/getting-started\n\nLaunches the development server to start the application with TWD functionality enabled. This standard development command triggers the Vite dev server which activates the TWD sidebar and test loading in development mode.\n\n```bash\nnpm run dev\n```\n\n--------------------------------\n\n### Configure main entry file for TWD initialization\n\nSource: https://brikev.github.io/twd/getting-started\n\nConfigures the main application entry point to load TWD sidebar and tests automatically in development mode. Uses Vite's glob import to discover test files, initializes the test framework with sidebar options, and sets up optional request mocking for API testing.\n\n```tsx\n// src/main.tsx\nimport { StrictMode } from 'react';\nimport { createRoot } from 'react-dom/client';\nimport './index.css';\nimport App from './App';\n\n// Only load the test sidebar and tests in development mode\nif (import.meta.env.DEV) {\n  // Use Vite's glob import to find all test files\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  // You need to pass the test modules, the sidebar component, and createRoot function\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  // Optionally initialize request mocking\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}\n\ncreateRoot(document.getElementById('root')!).render(\n  <StrictMode>\n    <App />\n  </StrictMode>,\n);\n```\n\n--------------------------------\n\n### Git commands for repo setup\n\nSource: https://brikev.github.io/twd/tutorial/first-test\n\nCommands to reset, clean, and checkout a specific branch in a Git repository. Useful for ensuring a clean state before starting the tutorial.\n\n```bash\ngit clone git@github.com:BRIKEV/twd-docs-tutorial.git\ngit reset --hard\ngit clean -d -f\ngit checkout 02-assertions\nnpm run serve:dev\n```\n\n--------------------------------\n\n### Initialize mock service worker for API mocking\n\nSource: https://brikev.github.io/twd/getting-started\n\nSets up the mock service worker file for API mocking functionality in TWD. This optional step copies the required mock-sw.js file to the public directory, enabling HTTP request interception and response mocking during testing.\n\n```bash\nnpx twd-js init public\n```\n\n--------------------------------\n\n### Vite + React: Initialize TWD Tests and Mocking\n\nSource: https://brikev.github.io/twd/frameworks\n\nStandard setup for Vite-based React applications to initialize TWD tests and request mocking in development. It uses `import.meta.glob()` to load test files and `twd-js` for initialization.\n\n```typescript\nif (import.meta.env.DEV) {\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}\n```\n\n--------------------------------\n\n### beforeEach(fn) - Setup Before Each Test\n\nSource: https://brikev.github.io/twd/api/test-functions\n\nRuns a setup function before each test in the current `describe` block. The 'fn' parameter is the setup function, which can be synchronous or asynchronous.\n\n```APIDOC\n## beforeEach(fn)\n\n### Description\nRuns a setup function before each test in the current `describe` block.\n\n### Method\n`beforeEach`\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\nNone\n\n### Parameters\n- **fn** (`function`) - Setup function to run before each test (can be async)\n\n### Request Example\n```ts\ndescribe(\"User Dashboard\", () => {\n  beforeEach(() => {\n    // Reset state before each test\n    localStorage.clear();\n    twd.clearRequestMockRules();\n  });\n\n  it(\"should show user profile\", async () => {\n    // Clean state guaranteed\n  });\n\n  it(\"should display recent orders\", async () => {\n    // Clean state guaranteed\n  });\n});\n```\n\n### Response\n#### Success Response (200)\nNone (This function defines test structure, does not return data)\n\n#### Response Example\nNone\n```\n\n--------------------------------\n\n### Install TWD Package in Bash\n\nSource: https://brikev.github.io/twd/tutorial/installation\n\nThis snippet installs the twd-js package as a dev dependency using npm. It requires Node.js and npm/yarn/pnpm. Input is the project directory, output is the package installed. Limitations include compatibility with the project's package.json and dev environment.\n\n```bash\nnpm i --save-dev twd-js\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Create TWD test file with component testing\n\nSource: https://brikev.github.io/twd/getting-started\n\nExample test file demonstrating TWD testing patterns including page navigation, element selection, user interaction simulation, and assertion methods. Shows both visibility testing and user event handling for interactive components.\n\n```ts\n// src/App.twd.test.ts\nimport { twd, userEvent } from \"twd-js\";\nimport { describe, it } from \"twd-js/runner\";\n\ndescribe(\"App Component\", () => {\n  it(\"should render the main heading\", async () => {\n    await twd.visit(\"/\");\n    \n    const heading = await twd.get(\"h1\");\n    heading.should(\"be.visible\");\n  });\n\n  it(\"should handle button clicks\", async () => {\n    await twd.visit(\"/\");\n    \n    const user = userEvent.setup();\n    const button = await twd.get(\"button\");\n    \n    await user.click(button.el);\n    \n    // Add your assertions here\n    const result = await twd.get(\"#result\");\n    result.should(\"have.text\", \"Button clicked!\");\n  });\n});\n```\n\n--------------------------------\n\n### Clone and Setup Project in Bash\n\nSource: https://brikev.github.io/twd/tutorial/installation\n\nThis snippet clones the tutorial repository, navigates to the project directory, checks out the setup branch, and installs dependencies using npm. It requires Git and Node.js to be installed. The input is none, output is a set up project ready for development. Limitations include requiring internet access for cloning and compatible Git setup.\n\n```bash\ngit clone git@github.com:BRIKEV/twd-docs-tutorial.git\ncd twd-docs-tutorial\ngit checkout 01-setup\nnpm i\n```\n\n--------------------------------\n\n### Install TWD package manager dependencies\n\nSource: https://brikev.github.io/twd/getting-started\n\nInstalls the TWD testing framework package using popular JavaScript package managers. Choose npm, yarn, or pnpm based on your project's package management preference. This is the first step to integrate TWD into your React application.\n\n```bash\nnpm install twd-js\n```\n\n```bash\nyarn add twd-js\n```\n\n```bash\npnpm add twd-js\n```\n\n--------------------------------\n\n### Start development server for TWD testing\n\nSource: https://brikev.github.io/twd/getting-started\n\nLaunches the development server to start the application with TWD functionality enabled. This standard development command triggers the Vite dev server which activates the TWD sidebar and test loading in development mode.\n\n```bash\nnpm run dev\n```\n\n--------------------------------\n\n### Configure main entry file for TWD initialization\n\nSource: https://brikev.github.io/twd/getting-started\n\nConfigures the main application entry point to load TWD sidebar and tests automatically in development mode. Uses Vite's glob import to discover test files, initializes the test framework with sidebar options, and sets up optional request mocking for API testing.\n\n```tsx\n// src/main.tsx\nimport { StrictMode } from 'react';\nimport { createRoot } from 'react-dom/client';\nimport './index.css';\nimport App from './App';\n\n// Only load the test sidebar and tests in development mode\nif (import.meta.env.DEV) {\n  // Use Vite's glob import to find all test files\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  // You need to pass the test modules, the sidebar component, and createRoot function\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  // Optionally initialize request mocking\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}\n\ncreateRoot(document.getElementById('root')!).render(\n  <StrictMode>\n    <App />\n  </StrictMode>,\n);\n```\n\n--------------------------------\n\n### Git commands for repo setup\n\nSource: https://brikev.github.io/twd/tutorial/first-test\n\nCommands to reset, clean, and checkout a specific branch in a Git repository. Useful for ensuring a clean state before starting the tutorial.\n\n```bash\ngit clone git@github.com:BRIKEV/twd-docs-tutorial.git\ngit reset --hard\ngit clean -d -f\ngit checkout 02-assertions\nnpm run serve:dev\n```\n\n--------------------------------\n\n### Initialize mock service worker for API mocking\n\nSource: https://brikev.github.io/twd/getting-started\n\nSets up the mock service worker file for API mocking functionality in TWD. This optional step copies the required mock-sw.js file to the public directory, enabling HTTP request interception and response mocking during testing.\n\n```bash\nnpx twd-js init public\n```\n\n--------------------------------\n\n### Vite + React: Initialize TWD Tests and Mocking\n\nSource: https://brikev.github.io/twd/frameworks\n\nStandard setup for Vite-based React applications to initialize TWD tests and request mocking in development. It uses `import.meta.glob()` to load test files and `twd-js` for initialization.\n\n```typescript\nif (import.meta.env.DEV) {\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}\n```\n\n--------------------------------\n\n### beforeEach(fn) - Setup Before Each Test\n\nSource: https://brikev.github.io/twd/api/test-functions\n\nRuns a setup function before each test in the current `describe` block. The 'fn' parameter is the setup function, which can be synchronous or asynchronous.\n\n```APIDOC",
            "codeBlocks": [
              {
                "language": "ts",
                "code": "// src/App.twd.test.ts\nimport { twd, userEvent } from \"twd-js\";\nimport { describe, it } from \"twd-js/runner\";\n\ndescribe(\"App Component\", () => {\n  it(\"should render the main heading\", async () => {\n    await twd.visit(\"/\");\n    \n    const heading = await twd.get(\"h1\");\n    heading.should(\"be.visible\");\n  });\n\n  it(\"should handle button clicks\", async () => {\n    await twd.visit(\"/\");\n    \n    const user = userEvent.setup();\n    const button = await twd.get(\"button\");\n    \n    await user.click(button.el);\n    \n    // Add your assertions here\n    const result = await twd.get(\"#result\");\n    result.should(\"have.text\", \"Button clicked!\");\n  });\n});",
                "context": "D testing patterns including page navigation, element selection, user interaction simulation, and assertion methods. Shows both visibility testing and user event handling for interactive components."
              },
              {
                "language": "bash",
                "code": "git clone git@github.com:BRIKEV/twd-docs-tutorial.git\ncd twd-docs-tutorial\ngit checkout 01-setup\nnpm i",
                "context": "It requires Git and Node.js to be installed. The input is none, output is a set up project ready for development. Limitations include requiring internet access for cloning and compatible Git setup."
              },
              {
                "language": "bash",
                "code": "npm install twd-js",
                "context": "ckage using popular JavaScript package managers. Choose npm, yarn, or pnpm based on your project's package management preference. This is the first step to integrate TWD into your React application."
              },
              {
                "language": "bash",
                "code": "yarn add twd-js",
                "context": "```bash\nnpm install twd-js\n```"
              },
              {
                "language": "bash",
                "code": "pnpm add twd-js",
                "context": "```bash\nyarn add twd-js\n```"
              },
              {
                "language": "bash",
                "code": "npm run dev",
                "context": "t server to start the application with TWD functionality enabled. This standard development command triggers the Vite dev server which activates the TWD sidebar and test loading in development mode."
              },
              {
                "language": "tsx",
                "code": "// src/main.tsx\nimport { StrictMode } from 'react';\nimport { createRoot } from 'react-dom/client';\nimport './index.css';\nimport App from './App';\n\n// Only load the test sidebar and tests in development mode\nif (import.meta.env.DEV) {\n  // Use Vite's glob import to find all test files\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  // You need to pass the test modules, the sidebar component, and createRoot function\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  // Optionally initialize request mocking\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}\n\ncreateRoot(document.getElementById('root')!).render(\n  <StrictMode>\n    <App />\n  </StrictMode>,\n);",
                "context": "nd tests automatically in development mode. Uses Vite's glob import to discover test files, initializes the test framework with sidebar options, and sets up optional request mocking for API testing."
              },
              {
                "language": "bash",
                "code": "git clone git@github.com:BRIKEV/twd-docs-tutorial.git\ngit reset --hard\ngit clean -d -f\ngit checkout 02-assertions\nnpm run serve:dev",
                "context": "ource: https://brikev.github.io/twd/tutorial/first-test\n\nCommands to reset, clean, and checkout a specific branch in a Git repository. Useful for ensuring a clean state before starting the tutorial."
              },
              {
                "language": "bash",
                "code": "npx twd-js init public",
                "context": "ker file for API mocking functionality in TWD. This optional step copies the required mock-sw.js file to the public directory, enabling HTTP request interception and response mocking during testing."
              },
              {
                "language": "typescript",
                "code": "if (import.meta.env.DEV) {\n  const testModules = import.meta.glob(\"./**/*.twd.test.ts\");\n  const { initTests, twd, TWDSidebar } = await import('twd-js');\n  initTests(testModules, <TWDSidebar open={true} position=\"left\" />, createRoot);\n  twd.initRequestMocking()\n    .then(() => {\n      console.log(\"Request mocking initialized\");\n    })\n    .catch((err) => {\n      console.error(\"Error initializing request mocking:\", err);\n    });\n}",
                "context": "meworks\n\nStandard setup for Vite-based React applications to initialize TWD tests and request mocking in development. It uses `import.meta.glob()` to load test files and `twd-js` for initialization."
              }
            ]
          },
          {
            "title": "beforeEach(fn)",
            "type": "other",
            "content": "### Description\nRuns a setup function before each test in the current `describe` block.\n\n### Method\n`beforeEach`\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\nNone\n\n### Parameters\n- **fn** (`function`) - Setup function to run before each test (can be async)\n\n### Request Example\n```ts\ndescribe(\"User Dashboard\", () => {\n  beforeEach(() => {\n    // Reset state before each test\n    localStorage.clear();\n    twd.clearRequestMockRules();\n  });\n\n  it(\"should show user profile\", async () => {\n    // Clean state guaranteed\n  });\n\n  it(\"should display recent orders\", async () => {\n    // Clean state guaranteed\n  });\n});\n```\n\n### Response\n#### Success Response (200)\nNone (This function defines test structure, does not return data)\n\n#### Response Example\nNone\n```\n\n--------------------------------\n\n### Install TWD Package in Bash\n\nSource: https://brikev.github.io/twd/tutorial/installation\n\nThis snippet installs the twd-js package as a dev dependency using npm. It requires Node.js and npm/yarn/pnpm. Input is the project directory, output is the package installed. Limitations include compatibility with the project's package.json and dev environment.\n\n```bash\nnpm i --save-dev twd-js\n```",
            "codeBlocks": [
              {
                "language": "ts",
                "code": "describe(\"User Dashboard\", () => {\n  beforeEach(() => {\n    // Reset state before each test\n    localStorage.clear();\n    twd.clearRequestMockRules();\n  });\n\n  it(\"should show user profile\", async () => {\n    // Clean state guaranteed\n  });\n\n  it(\"should display recent orders\", async () => {\n    // Clean state guaranteed\n  });\n});",
                "context": "- **fn** (`function`) - Setup function to run before each test (can be async)\n\n### Request Example"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Install TWD Package in Bash\n\nSource: https://brikev.github.io/twd/tutorial/installation\n\nThis snippet installs the twd-js package as a dev dependency using npm. It requires Node.js and npm/yarn/pnpm. Input is the project directory, output is the package installed. Limitations include compatibility with the project's package.json and dev environment.",
                "context": "\n#### Response Example\nNone"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/google/googletest",
        "packageName": "googletest",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:10.021Z",
        "content": "### Project Setup and Directory Creation Command\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-cmake.md\n\nA shell command to create a new project directory and navigate into it. This is the initial step for setting up a new project that will use CMake and GoogleTest.\n\n```bash\n$ mkdir my_project && cd my_project\n```\n\n--------------------------------\n\n### GoogleTest Example Test Case (hello_test.cc)\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-bazel.md\n\nThis C++ code demonstrates a basic GoogleTest case with simple assertions. It includes the GoogleTest header and defines a test suite 'HelloTest' with a test 'BasicAssertions' that checks string inequality and integer equality.\n\n```cpp\n#include <gtest/gtest.h>\n\n// Demonstrate some basic assertions.\nTEST(HelloTest, BasicAssertions) {\n  // Expect two strings not to be equal.\n  EXPECT_STRNE(\"hello\", \"world\");\n  // Expect equality.\n  EXPECT_EQ(7 * 6, 42);\n}\n```\n\n--------------------------------\n\n### C++ GoogleTest Per-Test-Suite Setup and Teardown Example\n\nSource: https://github.com/google/googletest/blob/main/docs/advanced.md\n\nThis C++ code demonstrates how to share resources between tests in the same test suite using GoogleTest's `SetUpTestSuite()` and `TearDownTestSuite()` methods. It defines static members for shared resources and outlines the lifecycle for per-test-suite setup and teardown, including important considerations for resource management and potential issues with derived classes.\n\n```c++\nclass FooTest : public testing::Test {\n protected:\n  // Per-test-suite set-up.\n  // Called before the first test in this test suite.\n  // Can be omitted if not needed.\n  static void SetUpTestSuite() {\n    shared_resource_ = new ...;\n\n    // If `shared_resource_` is **not deleted** in `TearDownTestSuite()`,\n    // reallocation should be prevented because `SetUpTestSuite()` may be called\n    // in subclasses of FooTest and lead to memory leak.\n    //\n    // if (shared_resource_ == nullptr) {\n    //   shared_resource_ = new ...;\n    // }\n  }\n\n  // Per-test-suite tear-down.\n  // Called after the last test in this test suite.\n  // Can be omitted if not needed.\n  static void TearDownTestSuite() {\n    delete shared_resource_;\n    shared_resource_ = nullptr;\n  }\n\n  // You can define per-test set-up logic as usual.\n  void SetUp() override { ... }\n\n  // You can define per-test tear-down logic as usual.\n  void TearDown() override { ... }\n\n  // Some expensive resource shared by all tests.\n  static T* shared_resource_;\n};\n\nT* FooTest::shared_resource_ = nullptr;\n\nTEST_F(FooTest, Test1) {\n  ... you can refer to shared_resource_ here ...\n}\n\nTEST_F(FooTest, Test2) {\n  ... you can refer to shared_resource_ here ...\n}\n```\n\n--------------------------------\n\n### Build and Install GoogleTest on *nix\n\nSource: https://github.com/google/googletest/blob/main/googletest/README.md\n\nCommands to build the cloned GoogleTest project using 'make' and install it system-wide.\n\n```bash\nmake\nsudo make install    # Install in /usr/local/ by default\n```\n\n--------------------------------\n\n### GoogleTest Test Suite Setup Failure Example\n\nSource: https://github.com/google/googletest/blob/main/googletest/test/googletest-output-test-golden-lin.txt\n\nIllustrates a GoogleTest failure during the setup phase of a test suite. The log shows a failure in 'TestSuiteThatFailsToSetUp' where an expected condition (true) was not met, resulting in the test not running.\n\n```cpp\nclass TestSuiteThatFailsToSetUp : public ::testing::Test {\nprotected:\n  static void SetUpTestSuite() {\n    ASSERT_TRUE(false); // This will cause the setup to fail\n  }\n};\n\nTEST_F(TestSuiteThatFailsToSetUp, ShouldNotRun) {\n  // This test will not run because SetUpTestSuite fails\n}\n```\n\n--------------------------------\n\n### CMake: Install Project\n\nSource: https://github.com/google/googletest/blob/main/googlemock/CMakeLists.txt\n\nInstalls the Google Mock and gmock_main targets. This command is typically defined in a separate include file (install_project.cmake) and handles the packaging and deployment of the built libraries.\n\n```cmake\ninstall_project(gmock gmock_main)\n```\n\n--------------------------------\n\n### ACTION_TEMPLATE Syntax and Example in C++\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nDemonstrates the syntax for defining action templates with explicit template parameters and value parameters using ACTION_TEMPLATE. Includes an example of DuplicateArg action for copying arguments.\n\n```cpp\n#define ACTION_TEMPLATE(ActionName, HAS_m_TEMPLATE_PARAMS, AND_n_VALUE_PARAMS) ... {\n  statements;\n}\n```\n\n```cpp\n// DuplicateArg<k, T>(output) converts the k-th argument of the mock\n// function to type T and copies it to *output.\nACTION_TEMPLATE(DuplicateArg,\n                // Note the comma between int and k:\n                HAS_2_TEMPLATE_PARAMS(int, k, typename, T),\n                AND_1_VALUE_PARAMS(output)) {\n  *output = T(std::get<k>(args));\n}\n```\n\n--------------------------------\n\n### gMock Action State Sharing Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nIllustrates the difference in behavior when using a shared stateful action versus separate actions from the same factory. The first example shows independent counters, while the second demonstrates a shared counter.\n\n```cpp\nEXPECT_CALL(foo, DoThis())\n    .WillRepeatedly(IncrementCounter(0));\nEXPECT_CALL(foo, DoThat())\n    .WillRepeatedly(IncrementCounter(0));\nfoo.DoThis();  // Returns 1.\nfoo.DoThis();  // Returns 2.\nfoo.DoThat();  // Returns 1 - DoThat() uses a different\n                 // counter than DoThis()'s.\n```\n\n```cpp\nusing ::testing::Action;\n...\n  Action<int()> increment = IncrementCounter(0);\n  EXPECT_CALL(foo, DoThis())\n      .WillRepeatedly(increment);\n  EXPECT_CALL(foo, DoThat())\n      .WillRepeatedly(increment);\n  foo.DoThis();  // Returns 1.\n  foo.DoThis();  // Returns 2.\n  foo.DoThat();  // Returns 3 - the counter is shared.\n```\n\n--------------------------------\n\n### Shell Output Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nProvides an example of the failure output from a Google Test assertion using a custom matcher, showing the expected versus actual values.\n\n```shell\n  Value of: some_expression\n  Expected: is divisible by 7\n    Actual: 27\n  ...\n  Value of: some_other_expression\n  Expected: not (is divisible by 7)\n    Actual: 21\n```\n\n```shell\n  Expected equality of these values:\n    remainder\n      Which is: 6\n    0\n```\n\n--------------------------------\n\n### Create Bazel Workspace Directory\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-bazel.md\n\nThis command creates a new directory for your Bazel workspace and navigates into it. This is the first step in setting up a new project with Bazel.\n\n```bash\nmkdir my_workspace && cd my_workspace\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Project Setup and Directory Creation Command\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-cmake.md\n\nA shell command to create a new project directory and navigate into it. This is the initial step for setting up a new project that will use CMake and GoogleTest.\n\n```bash\n$ mkdir my_project && cd my_project\n```\n\n--------------------------------\n\n### GoogleTest Example Test Case (hello_test.cc)\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-bazel.md\n\nThis C++ code demonstrates a basic GoogleTest case with simple assertions. It includes the GoogleTest header and defines a test suite 'HelloTest' with a test 'BasicAssertions' that checks string inequality and integer equality.\n\n```cpp\n#include <gtest/gtest.h>\n\n// Demonstrate some basic assertions.\nTEST(HelloTest, BasicAssertions) {\n  // Expect two strings not to be equal.\n  EXPECT_STRNE(\"hello\", \"world\");\n  // Expect equality.\n  EXPECT_EQ(7 * 6, 42);\n}\n```\n\n--------------------------------\n\n### C++ GoogleTest Per-Test-Suite Setup and Teardown Example\n\nSource: https://github.com/google/googletest/blob/main/docs/advanced.md\n\nThis C++ code demonstrates how to share resources between tests in the same test suite using GoogleTest's `SetUpTestSuite()` and `TearDownTestSuite()` methods. It defines static members for shared resources and outlines the lifecycle for per-test-suite setup and teardown, including important considerations for resource management and potential issues with derived classes.\n\n```c++\nclass FooTest : public testing::Test {\n protected:\n  // Per-test-suite set-up.\n  // Called before the first test in this test suite.\n  // Can be omitted if not needed.\n  static void SetUpTestSuite() {\n    shared_resource_ = new ...;\n\n    // If `shared_resource_` is **not deleted** in `TearDownTestSuite()`,\n    // reallocation should be prevented because `SetUpTestSuite()` may be called\n    // in subclasses of FooTest and lead to memory leak.\n    //\n    // if (shared_resource_ == nullptr) {\n    //   shared_resource_ = new ...;\n    // }\n  }\n\n  // Per-test-suite tear-down.\n  // Called after the last test in this test suite.\n  // Can be omitted if not needed.\n  static void TearDownTestSuite() {\n    delete shared_resource_;\n    shared_resource_ = nullptr;\n  }\n\n  // You can define per-test set-up logic as usual.\n  void SetUp() override { ... }\n\n  // You can define per-test tear-down logic as usual.\n  void TearDown() override { ... }\n\n  // Some expensive resource shared by all tests.\n  static T* shared_resource_;\n};\n\nT* FooTest::shared_resource_ = nullptr;\n\nTEST_F(FooTest, Test1) {\n  ... you can refer to shared_resource_ here ...\n}\n\nTEST_F(FooTest, Test2) {\n  ... you can refer to shared_resource_ here ...\n}\n```\n\n--------------------------------\n\n### Build and Install GoogleTest on *nix\n\nSource: https://github.com/google/googletest/blob/main/googletest/README.md\n\nCommands to build the cloned GoogleTest project using 'make' and install it system-wide.\n\n```bash\nmake\nsudo make install    # Install in /usr/local/ by default\n```\n\n--------------------------------\n\n### GoogleTest Test Suite Setup Failure Example\n\nSource: https://github.com/google/googletest/blob/main/googletest/test/googletest-output-test-golden-lin.txt\n\nIllustrates a GoogleTest failure during the setup phase of a test suite. The log shows a failure in 'TestSuiteThatFailsToSetUp' where an expected condition (true) was not met, resulting in the test not running.\n\n```cpp\nclass TestSuiteThatFailsToSetUp : public ::testing::Test {\nprotected:\n  static void SetUpTestSuite() {\n    ASSERT_TRUE(false); // This will cause the setup to fail\n  }\n};\n\nTEST_F(TestSuiteThatFailsToSetUp, ShouldNotRun) {\n  // This test will not run because SetUpTestSuite fails\n}\n```\n\n--------------------------------\n\n### CMake: Install Project\n\nSource: https://github.com/google/googletest/blob/main/googlemock/CMakeLists.txt\n\nInstalls the Google Mock and gmock_main targets. This command is typically defined in a separate include file (install_project.cmake) and handles the packaging and deployment of the built libraries.\n\n```cmake\ninstall_project(gmock gmock_main)\n```\n\n--------------------------------\n\n### ACTION_TEMPLATE Syntax and Example in C++\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nDemonstrates the syntax for defining action templates with explicit template parameters and value parameters using ACTION_TEMPLATE. Includes an example of DuplicateArg action for copying arguments.\n\n```cpp\n#define ACTION_TEMPLATE(ActionName, HAS_m_TEMPLATE_PARAMS, AND_n_VALUE_PARAMS) ... {\n  statements;\n}\n```\n\n```cpp\n// DuplicateArg<k, T>(output) converts the k-th argument of the mock\n// function to type T and copies it to *output.\nACTION_TEMPLATE(DuplicateArg,\n                // Note the comma between int and k:\n                HAS_2_TEMPLATE_PARAMS(int, k, typename, T),\n                AND_1_VALUE_PARAMS(output)) {\n  *output = T(std::get<k>(args));\n}\n```\n\n--------------------------------\n\n### gMock Action State Sharing Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nIllustrates the difference in behavior when using a shared stateful action versus separate actions from the same factory. The first example shows independent counters, while the second demonstrates a shared counter.\n\n```cpp\nEXPECT_CALL(foo, DoThis())\n    .WillRepeatedly(IncrementCounter(0));\nEXPECT_CALL(foo, DoThat())\n    .WillRepeatedly(IncrementCounter(0));\nfoo.DoThis();  // Returns 1.\nfoo.DoThis();  // Returns 2.\nfoo.DoThat();  // Returns 1 - DoThat() uses a different\n                 // counter than DoThis()'s.\n```\n\n```cpp\nusing ::testing::Action;\n...\n  Action<int()> increment = IncrementCounter(0);\n  EXPECT_CALL(foo, DoThis())\n      .WillRepeatedly(increment);\n  EXPECT_CALL(foo, DoThat())\n      .WillRepeatedly(increment);\n  foo.DoThis();  // Returns 1.\n  foo.DoThis();  // Returns 2.\n  foo.DoThat();  // Returns 3 - the counter is shared.\n```\n\n--------------------------------\n\n### Shell Output Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nProvides an example of the failure output from a Google Test assertion using a custom matcher, showing the expected versus actual values.\n\n```shell\n  Value of: some_expression\n  Expected: is divisible by 7\n    Actual: 27\n  ...\n  Value of: some_other_expression\n  Expected: not (is divisible by 7)\n    Actual: 21\n```\n\n```shell\n  Expected equality of these values:\n    remainder\n      Which is: 6\n    0\n```\n\n--------------------------------\n\n### Create Bazel Workspace Directory\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-bazel.md\n\nThis command creates a new directory for your Bazel workspace and navigates into it. This is the first step in setting up a new project with Bazel.\n\n```bash\nmkdir my_workspace && cd my_workspace\n```",
            "codeBlocks": [
              {
                "language": "bash",
                "code": "$ mkdir my_project && cd my_project",
                "context": "/blob/main/docs/quickstart-cmake.md\n\nA shell command to create a new project directory and navigate into it. This is the initial step for setting up a new project that will use CMake and GoogleTest."
              },
              {
                "language": "cpp",
                "code": "#include <gtest/gtest.h>\n\n// Demonstrate some basic assertions.\nTEST(HelloTest, BasicAssertions) {\n  // Expect two strings not to be equal.\n  EXPECT_STRNE(\"hello\", \"world\");\n  // Expect equality.\n  EXPECT_EQ(7 * 6, 42);\n}",
                "context": "basic GoogleTest case with simple assertions. It includes the GoogleTest header and defines a test suite 'HelloTest' with a test 'BasicAssertions' that checks string inequality and integer equality."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Build and Install GoogleTest on *nix\n\nSource: https://github.com/google/googletest/blob/main/googletest/README.md\n\nCommands to build the cloned GoogleTest project using 'make' and install it system-wide.",
                "context": "TEST_F(FooTest, Test2) {\n  ... you can refer to shared_resource_ here ...\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### GoogleTest Test Suite Setup Failure Example\n\nSource: https://github.com/google/googletest/blob/main/googletest/test/googletest-output-test-golden-lin.txt\n\nIllustrates a GoogleTest failure during the setup phase of a test suite. The log shows a failure in 'TestSuiteThatFailsToSetUp' where an expected condition (true) was not met, resulting in the test not running.",
                "context": "```bash\nmake\nsudo make install    # Install in /usr/local/ by default"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### CMake: Install Project\n\nSource: https://github.com/google/googletest/blob/main/googlemock/CMakeLists.txt\n\nInstalls the Google Mock and gmock_main targets. This command is typically defined in a separate include file (install_project.cmake) and handles the packaging and deployment of the built libraries.",
                "context": "TEST_F(TestSuiteThatFailsToSetUp, ShouldNotRun) {\n  // This test will not run because SetUpTestSuite fails\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### ACTION_TEMPLATE Syntax and Example in C++\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nDemonstrates the syntax for defining action templates with explicit template parameters and value parameters using ACTION_TEMPLATE. Includes an example of DuplicateArg action for copying arguments.",
                "context": "\n```cmake\ninstall_project(gmock gmock_main)"
              },
              {
                "language": "text",
                "code": "",
                "context": "#define ACTION_TEMPLATE(ActionName, HAS_m_TEMPLATE_PARAMS, AND_n_VALUE_PARAMS) ... {\n  statements;\n}"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### gMock Action State Sharing Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nIllustrates the difference in behavior when using a shared stateful action versus separate actions from the same factory. The first example shows independent counters, while the second demonstrates a shared counter.",
                "context": "                AND_1_VALUE_PARAMS(output)) {\n  *output = T(std::get<k>(args));\n}"
              },
              {
                "language": "text",
                "code": "",
                "context": "foo.DoThis();  // Returns 2.\nfoo.DoThat();  // Returns 1 - DoThat() uses a different\n                 // counter than DoThis()'s."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Shell Output Example\n\nSource: https://github.com/google/googletest/blob/main/docs/gmock_cook_book.md\n\nProvides an example of the failure output from a Google Test assertion using a custom matcher, showing the expected versus actual values.",
                "context": "  foo.DoThis();  // Returns 1.\n  foo.DoThis();  // Returns 2.\n  foo.DoThat();  // Returns 3 - the counter is shared."
              },
              {
                "language": "text",
                "code": "",
                "context": "  Value of: some_other_expression\n  Expected: not (is divisible by 7)\n    Actual: 21"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Create Bazel Workspace Directory\n\nSource: https://github.com/google/googletest/blob/main/docs/quickstart-bazel.md\n\nThis command creates a new directory for your Bazel workspace and navigates into it. This is the first step in setting up a new project with Bazel.",
                "context": "    remainder\n      Which is: 6\n    0"
              }
            ]
          }
        ]
      },
      {
        "packageId": "/typelevel/weaver-test",
        "packageName": "weaver-test",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:15.870Z",
        "content": "### Running Example Suite Report\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/features/asserting_equality.md\n\nCode to generate and display the report of the example Weaver test suite.\n\n```scala\nprintln(weaver.docs.Output.runSuites(ExpectationsSuite))\n```\n\n--------------------------------\n\n### Gradle Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Gradle to use weaver-test with the Gradle plugin for multi-backend Scala.\n\n```groovy\nplugins {\n  id 'org.podval.tools.scalajs' version '<latest version>'\n}\ndependencies {\n  testImplementation scalaBackend.testFramework(org.podval.tools.test.framework.WeaverTest)\n}\n```\n\n--------------------------------\n\n### Mill Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Mill to use weaver-cats as the test framework and dependency.\n\n```scala\nobject test extends Tests {\n  def ivyDeps = Agg(\n    ivy\"org.typelevel::weaver-cats:@VERSION@\"\n  )\n  def testFramework = \"weaver.framework.CatsEffect\"\n}\n```\n\n--------------------------------\n\n### SimpleIOSuite Example\n\nSource: https://github.com/typelevel/weaver-test/blob/main/README.md\n\nDemonstrates the usage of SimpleIOSuite for writing tests in Scala. It includes examples of pure tests and tests involving side-effects using cats-effect IO, along with a test that utilizes a logger.\n\n```scala\nimport weaver.SimpleIOSuite\nimport cats.effect._\n\nobject MySuite extends SimpleIOSuite {\n\n  pureTest(\"non-effectful (pure) test\"){\n    expect(\"hello\".size == 6)\n  }\n\n  private val random = IO(java.util.UUID.randomUUID())\n\n  test(\"test with side-effects\") {\n    for {\n      x <- random\n      y <- random\n    } yield expect(x != y)\n  }\n\n  loggedTest(\"test with side-effects and a logger\"){\n    log =>\n    for {\n      x <- random\n      _ <- log.info(s\"x : $x\")\n      y <- random\n      _ <- log.info(s\"y : $y\")\n    } yield expect(x != y)\n  }\n}\n```\n\n--------------------------------\n\n### scala-cli Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures scala-cli to use weaver-cats as a library dependency and specifies the test framework.\n\n```scala\n//> using lib \"org.typelevel::weaver-cats:@VERSION@\"\n//> using testFramework \"weaver.framework.CatsEffect\" // this may be neccessary if you have other testFramework on your dependencies\n```\n\n--------------------------------\n\n### SBT Installation (Newer Versions)\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency for newer SBT versions (1.9.0+). This integrates weaver automatically.\n\n```scala\nlibraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test\n```\n\n--------------------------------\n\n### SBT Installation (Older Versions)\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency and manually registers the Weaver test framework for older SBT versions.\n\n```scala\nlibraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test\ntestFrameworks += new TestFramework(\"weaver.framework.CatsEffect\")\n```\n\n--------------------------------\n\n### Example Weaver Test Suite\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/features/asserting_equality.md\n\nA comprehensive example of a Weaver test suite demonstrating the usage of `expect.eql` and `expect.same` with both standard and custom data types.\n\n```scala\nimport weaver._\nimport cats.Eq\n\nobject ExpectationsSuite extends SimpleIOSuite {\n\n  pureTest(\"expect.eql for standard data types\") {\n    expect.eql(1, 2)\n  }\n  \n  case class Pet(name: String)\n  implicit val eqPet: Eq[Pet] = Eq.by[Pet, String](_.name)\n  \n  pureTest(\"expect.eql for user-defined data types\") {\n    // Note: This test is written to fail to demonstrate expect.eql behavior\n    expect.eql(Pet(\"Maru\"), Pet(\"Fido\"))\n  }\n\n  case class Dog(name: String)\n\n  pureTest(\"expect.same relaxed equality comparison\") {\n    expect.same(Dog(\"Maru\"), Dog(\"Fido\"))\n  }\n}\n```\n\n--------------------------------\n\n### IOSuite Example with Shared Resource\n\nSource: https://github.com/typelevel/weaver-test/blob/main/README.md\n\nIllustrates how to use IOSuite for tests that require a shared resource. The `sharedResource` is defined using `cats.effect.Resource` and is available to all tests within the suite. It also shows how to access a logger.\n\n```scala\nimport weaver.IOSuite\nimport cats.effect._\n\nobject MySuite extends IOSuite {\n\n  type Res = Int\n\n  def sharedResource : Resource[IO, Int] = Resource\n    .make(\n      IO(println(\"Making resource\"))\n        .as(123)\n    )(n => IO(println(s\"Closing resource $n\")))\n\n  test(\"test, but resource not visible\"){\n    IO(expect(123 == 123))\n  }\n\n  test(\"test with resource\") { n =>\n    IO(expect(n == 123))\n  }\n\n  test(\"test with resource and a logger\") { (n, log) =>\n    log.info(\"log was available\") *> \\\n    IO(expect(n == 123))\n  }\n}\n```\n\n--------------------------------\n\n### Basic Weaver Test Example\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nDemonstrates a simple test suite using `SimpleIOSuite` in Weaver. It includes a test that generates two UUIDs and asserts they are not equal.\n\n```scala\nimport cats.effect._\n\n// Suites must be \"objects\" for them to be picked by the framework\nobject MySuite extends SimpleIOSuite {\n\n  val randomUUID = IO(java.util.UUID.randomUUID())\n\n  // A test for side-effecting functions\n  test(\"hello side-effects\") {\n    for {\n      x <- randomUUID\n      y <- randomUUID\n    } yield expect(x != y)\n  }\n\n}\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Running Example Suite Report\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/features/asserting_equality.md\n\nCode to generate and display the report of the example Weaver test suite.\n\n```scala\nprintln(weaver.docs.Output.runSuites(ExpectationsSuite))\n```\n\n--------------------------------\n\n### Gradle Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Gradle to use weaver-test with the Gradle plugin for multi-backend Scala.\n\n```groovy\nplugins {\n  id 'org.podval.tools.scalajs' version '<latest version>'\n}\ndependencies {\n  testImplementation scalaBackend.testFramework(org.podval.tools.test.framework.WeaverTest)\n}\n```\n\n--------------------------------\n\n### Mill Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Mill to use weaver-cats as the test framework and dependency.\n\n```scala\nobject test extends Tests {\n  def ivyDeps = Agg(\n    ivy\"org.typelevel::weaver-cats:@VERSION@\"\n  )\n  def testFramework = \"weaver.framework.CatsEffect\"\n}\n```\n\n--------------------------------\n\n### SimpleIOSuite Example\n\nSource: https://github.com/typelevel/weaver-test/blob/main/README.md\n\nDemonstrates the usage of SimpleIOSuite for writing tests in Scala. It includes examples of pure tests and tests involving side-effects using cats-effect IO, along with a test that utilizes a logger.\n\n```scala\nimport weaver.SimpleIOSuite\nimport cats.effect._\n\nobject MySuite extends SimpleIOSuite {\n\n  pureTest(\"non-effectful (pure) test\"){\n    expect(\"hello\".size == 6)\n  }\n\n  private val random = IO(java.util.UUID.randomUUID())\n\n  test(\"test with side-effects\") {\n    for {\n      x <- random\n      y <- random\n    } yield expect(x != y)\n  }\n\n  loggedTest(\"test with side-effects and a logger\"){\n    log =>\n    for {\n      x <- random\n      _ <- log.info(s\"x : $x\")\n      y <- random\n      _ <- log.info(s\"y : $y\")\n    } yield expect(x != y)\n  }\n}\n```\n\n--------------------------------\n\n### scala-cli Installation\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures scala-cli to use weaver-cats as a library dependency and specifies the test framework.\n\n```scala\n//> using lib \"org.typelevel::weaver-cats:@VERSION@\"\n//> using testFramework \"weaver.framework.CatsEffect\" // this may be neccessary if you have other testFramework on your dependencies\n```\n\n--------------------------------\n\n### SBT Installation (Newer Versions)\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency for newer SBT versions (1.9.0+). This integrates weaver automatically.\n\n```scala\nlibraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test\n```\n\n--------------------------------\n\n### SBT Installation (Older Versions)\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency and manually registers the Weaver test framework for older SBT versions.\n\n```scala\nlibraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test\ntestFrameworks += new TestFramework(\"weaver.framework.CatsEffect\")\n```\n\n--------------------------------\n\n### Example Weaver Test Suite\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/features/asserting_equality.md\n\nA comprehensive example of a Weaver test suite demonstrating the usage of `expect.eql` and `expect.same` with both standard and custom data types.\n\n```scala\nimport weaver._\nimport cats.Eq\n\nobject ExpectationsSuite extends SimpleIOSuite {\n\n  pureTest(\"expect.eql for standard data types\") {\n    expect.eql(1, 2)\n  }\n  \n  case class Pet(name: String)\n  implicit val eqPet: Eq[Pet] = Eq.by[Pet, String](_.name)\n  \n  pureTest(\"expect.eql for user-defined data types\") {\n    // Note: This test is written to fail to demonstrate expect.eql behavior\n    expect.eql(Pet(\"Maru\"), Pet(\"Fido\"))\n  }\n\n  case class Dog(name: String)\n\n  pureTest(\"expect.same relaxed equality comparison\") {\n    expect.same(Dog(\"Maru\"), Dog(\"Fido\"))\n  }\n}\n```\n\n--------------------------------\n\n### IOSuite Example with Shared Resource\n\nSource: https://github.com/typelevel/weaver-test/blob/main/README.md\n\nIllustrates how to use IOSuite for tests that require a shared resource. The `sharedResource` is defined using `cats.effect.Resource` and is available to all tests within the suite. It also shows how to access a logger.\n\n```scala\nimport weaver.IOSuite\nimport cats.effect._\n\nobject MySuite extends IOSuite {\n\n  type Res = Int\n\n  def sharedResource : Resource[IO, Int] = Resource\n    .make(\n      IO(println(\"Making resource\"))\n        .as(123)\n    )(n => IO(println(s\"Closing resource $n\")))\n\n  test(\"test, but resource not visible\"){\n    IO(expect(123 == 123))\n  }\n\n  test(\"test with resource\") { n =>\n    IO(expect(n == 123))\n  }\n\n  test(\"test with resource and a logger\") { (n, log) =>\n    log.info(\"log was available\") *> \\\n    IO(expect(n == 123))\n  }\n}\n```\n\n--------------------------------\n\n### Basic Weaver Test Example\n\nSource: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nDemonstrates a simple test suite using `SimpleIOSuite` in Weaver. It includes a test that generates two UUIDs and asserts they are not equal.\n\n```scala\nimport cats.effect._\n\n// Suites must be \"objects\" for them to be picked by the framework\nobject MySuite extends SimpleIOSuite {\n\n  val randomUUID = IO(java.util.UUID.randomUUID())\n\n  // A test for side-effecting functions\n  test(\"hello side-effects\") {\n    for {\n      x <- randomUUID\n      y <- randomUUID\n    } yield expect(x != y)\n  }\n\n}\n```",
            "codeBlocks": [
              {
                "language": "scala",
                "code": "println(weaver.docs.Output.runSuites(ExpectationsSuite))",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/features/asserting_equality.md\n\nCode to generate and display the report of the example Weaver test suite."
              },
              {
                "language": "groovy",
                "code": "plugins {\n  id 'org.podval.tools.scalajs' version '<latest version>'\n}\ndependencies {\n  testImplementation scalaBackend.testFramework(org.podval.tools.test.framework.WeaverTest)\n}",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Gradle to use weaver-test with the Gradle plugin for multi-backend Scala."
              },
              {
                "language": "scala",
                "code": "object test extends Tests {\n  def ivyDeps = Agg(\n    ivy\"org.typelevel::weaver-cats:@VERSION@\"\n  )\n  def testFramework = \"weaver.framework.CatsEffect\"\n}",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures Mill to use weaver-cats as the test framework and dependency."
              },
              {
                "language": "scala",
                "code": "import weaver.SimpleIOSuite\nimport cats.effect._\n\nobject MySuite extends SimpleIOSuite {\n\n  pureTest(\"non-effectful (pure) test\"){\n    expect(\"hello\".size == 6)\n  }\n\n  private val random = IO(java.util.UUID.randomUUID())\n\n  test(\"test with side-effects\") {\n    for {\n      x <- random\n      y <- random\n    } yield expect(x != y)\n  }\n\n  loggedTest(\"test with side-effects and a logger\"){\n    log =>\n    for {\n      x <- random\n      _ <- log.info(s\"x : $x\")\n      y <- random\n      _ <- log.info(s\"y : $y\")\n    } yield expect(x != y)\n  }\n}",
                "context": "emonstrates the usage of SimpleIOSuite for writing tests in Scala. It includes examples of pure tests and tests involving side-effects using cats-effect IO, along with a test that utilizes a logger."
              },
              {
                "language": "scala",
                "code": "//> using lib \"org.typelevel::weaver-cats:@VERSION@\"\n//> using testFramework \"weaver.framework.CatsEffect\" // this may be neccessary if you have other testFramework on your dependencies",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nConfigures scala-cli to use weaver-cats as a library dependency and specifies the test framework."
              },
              {
                "language": "scala",
                "code": "libraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency for newer SBT versions (1.9.0+). This integrates weaver automatically."
              },
              {
                "language": "scala",
                "code": "libraryDependencies +=  \"org.typelevel\" %% \"weaver-cats\" % \"@VERSION@\" % Test\ntestFrameworks += new TestFramework(\"weaver.framework.CatsEffect\")",
                "context": "Source: https://github.com/typelevel/weaver-test/blob/main/docs/overview/installation.md\n\nAdds the weaver-cats dependency and manually registers the Weaver test framework for older SBT versions."
              },
              {
                "language": "scala",
                "code": "import weaver._\nimport cats.Eq\n\nobject ExpectationsSuite extends SimpleIOSuite {\n\n  pureTest(\"expect.eql for standard data types\") {\n    expect.eql(1, 2)\n  }\n  \n  case class Pet(name: String)\n  implicit val eqPet: Eq[Pet] = Eq.by[Pet, String](_.name)\n  \n  pureTest(\"expect.eql for user-defined data types\") {\n    // Note: This test is written to fail to demonstrate expect.eql behavior\n    expect.eql(Pet(\"Maru\"), Pet(\"Fido\"))\n  }\n\n  case class Dog(name: String)\n\n  pureTest(\"expect.same relaxed equality comparison\") {\n    expect.same(Dog(\"Maru\"), Dog(\"Fido\"))\n  }\n}",
                "context": "test/blob/main/docs/features/asserting_equality.md\n\nA comprehensive example of a Weaver test suite demonstrating the usage of `expect.eql` and `expect.same` with both standard and custom data types."
              },
              {
                "language": "scala",
                "code": "import weaver.IOSuite\nimport cats.effect._\n\nobject MySuite extends IOSuite {\n\n  type Res = Int\n\n  def sharedResource : Resource[IO, Int] = Resource\n    .make(\n      IO(println(\"Making resource\"))\n        .as(123)\n    )(n => IO(println(s\"Closing resource $n\")))\n\n  test(\"test, but resource not visible\"){\n    IO(expect(123 == 123))\n  }\n\n  test(\"test with resource\") { n =>\n    IO(expect(n == 123))\n  }\n\n  test(\"test with resource and a logger\") { (n, log) =>\n    log.info(\"log was available\") *> \\\n    IO(expect(n == 123))\n  }\n}",
                "context": "e IOSuite for tests that require a shared resource. The `sharedResource` is defined using `cats.effect.Resource` and is available to all tests within the suite. It also shows how to access a logger."
              },
              {
                "language": "scala",
                "code": "import cats.effect._\n\n// Suites must be \"objects\" for them to be picked by the framework\nobject MySuite extends SimpleIOSuite {\n\n  val randomUUID = IO(java.util.UUID.randomUUID())\n\n  // A test for side-effecting functions\n  test(\"hello side-effects\") {\n    for {\n      x <- randomUUID\n      y <- randomUUID\n    } yield expect(x != y)\n  }\n\n}",
                "context": "vel/weaver-test/blob/main/docs/overview/installation.md\n\nDemonstrates a simple test suite using `SimpleIOSuite` in Weaver. It includes a test that generates two UUIDs and asserts they are not equal."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/hypercubed/chuhai",
        "packageName": "chuhai",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:20.167Z",
        "content": "### Install Chhai via npm\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nCommand to install Chhai as a development dependency using npm. This makes the library available for use in your project's development environment.\n\n```sh\nnpm i --save-dev chuhai\n```\n\n--------------------------------\n\n### Chhai `s.cycle` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.cycle` method, which defines a function to be executed between each benchmark run within a suite. This is typically used for assertion checks, setup, or teardown logic to ensure consistent test conditions and validate results.\n\n```APIDOC\n// runs between benchmarks (place assertions checks here)\ns.cycle(implementation: function)\n```\n\n--------------------------------\n\n### Chhai `suite` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `suite` function, which initializes a new benchmark suite. It can optionally take a title (string) and requires an implementation function. The function returns a promise, allowing for asynchronous handling of benchmark results.\n\n```APIDOC\n// creates a new benchmark suite\n// returns a promise\nsuite([title: string], implementation: function): promise\n```\n\n--------------------------------\n\n### Basic Chhai Benchmark Suite with Node.js and Assert\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDemonstrates how to create a benchmark suite using Chhai, `node-assert`, and Node.js. It defines a suite for array concatenation, includes a `cycle` function for assertions between benchmarks, and shows `burn` and `bench` methods for different types of performance tests. The suite returns a promise, suitable for modern async test runners.\n\n```js\nvar assert = require('assert');\nvar suite = require('chuhai');\n\n// starts a new benchmark suite\n// note this returns a promise\nsuite('array concat', function (s) {\n  var arr1 = ['a', 'b', 'c'];\n  var arr2 = ['d', 'e', 'f'];\n  var arr3 = null;\n\n  // run between each benchmark\n  s.cycle(function () {\n    // uses your assertion lib of choice\n    assert.deepEqual(arr3, ['a', 'b', 'c', 'd', 'e', 'f']);\n    arr3 = null;\n  });\n\n  // adds a bench that runs but doesn't get counted when comparing results to others.\n  s.burn('slice', function () {\n    arr3 = ['a', 'b', 'c', 'd', 'e', 'f'].slice();\n  });\n\n  // adds a bench.\n  s.bench('concat', function () {\n    arr3 = arr1.concat(arr2);\n  });\n\n  // adds a bench.\n  s.bench('for loop', function () {\n    var i;\n    var l1 = arr1.length;\n    var l2 = arr2.length;\n    arr3 = Array(l1 + l2);\n    for (i = 0; i < l1; i++) {\n      arr3[i] = arr1[i];\n    }\n    for (var i2 = 0; i2 < l2; i2++) {\n      arr3[i + i2] = arr2[i2];\n    }\n  });\n});\n```\n\n--------------------------------\n\n### Run Chhai Benchmark File\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nCommand to execute the Chhai benchmark file using Node.js. This assumes the benchmark file is named `bench.js` and is located in the current directory.\n\n```sh\nnode bench.js\n```\n\n--------------------------------\n\n### Chhai `s.set` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.set` method, used to configure options for the current benchmark or the entire suite. It takes a `key` (string) representing the option name and a `value` (any type) to set for that option.\n\n```APIDOC\n// sets benchmark/suite options\ns.set(key: string, value: any)\n```\n\n--------------------------------\n\n### Chhai `s.bench` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.bench` method, used within a suite's implementation function to define a standard benchmark test. It takes a title (string) for identification and an implementation function containing the code to be benchmarked.\n\n```APIDOC\n// creates a new benchmark test\ns.bench(title: string, implementation: function)\n```\n\n--------------------------------\n\n### Chhai `s.burn` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.burn` method, used within a suite to define a 'burn-in' benchmark test. These tests run but their results are not included when comparing performance against other benchmarks in the suite. It takes a title (string) and an implementation function.\n\n```APIDOC\n// creates a burn-in benchmark test\ns.burn(title: string, implementation: function)\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Install Chhai via npm\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nCommand to install Chhai as a development dependency using npm. This makes the library available for use in your project's development environment.\n\n```sh\nnpm i --save-dev chuhai\n```\n\n--------------------------------\n\n### Chhai `s.cycle` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.cycle` method, which defines a function to be executed between each benchmark run within a suite. This is typically used for assertion checks, setup, or teardown logic to ensure consistent test conditions and validate results.\n\n```APIDOC\n// runs between benchmarks (place assertions checks here)\ns.cycle(implementation: function)\n```\n\n--------------------------------\n\n### Chhai `suite` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `suite` function, which initializes a new benchmark suite. It can optionally take a title (string) and requires an implementation function. The function returns a promise, allowing for asynchronous handling of benchmark results.\n\n```APIDOC\n// creates a new benchmark suite\n// returns a promise\nsuite([title: string], implementation: function): promise\n```\n\n--------------------------------\n\n### Basic Chhai Benchmark Suite with Node.js and Assert\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDemonstrates how to create a benchmark suite using Chhai, `node-assert`, and Node.js. It defines a suite for array concatenation, includes a `cycle` function for assertions between benchmarks, and shows `burn` and `bench` methods for different types of performance tests. The suite returns a promise, suitable for modern async test runners.\n\n```js\nvar assert = require('assert');\nvar suite = require('chuhai');\n\n// starts a new benchmark suite\n// note this returns a promise\nsuite('array concat', function (s) {\n  var arr1 = ['a', 'b', 'c'];\n  var arr2 = ['d', 'e', 'f'];\n  var arr3 = null;\n\n  // run between each benchmark\n  s.cycle(function () {\n    // uses your assertion lib of choice\n    assert.deepEqual(arr3, ['a', 'b', 'c', 'd', 'e', 'f']);\n    arr3 = null;\n  });\n\n  // adds a bench that runs but doesn't get counted when comparing results to others.\n  s.burn('slice', function () {\n    arr3 = ['a', 'b', 'c', 'd', 'e', 'f'].slice();\n  });\n\n  // adds a bench.\n  s.bench('concat', function () {\n    arr3 = arr1.concat(arr2);\n  });\n\n  // adds a bench.\n  s.bench('for loop', function () {\n    var i;\n    var l1 = arr1.length;\n    var l2 = arr2.length;\n    arr3 = Array(l1 + l2);\n    for (i = 0; i < l1; i++) {\n      arr3[i] = arr1[i];\n    }\n    for (var i2 = 0; i2 < l2; i2++) {\n      arr3[i + i2] = arr2[i2];\n    }\n  });\n});\n```\n\n--------------------------------\n\n### Run Chhai Benchmark File\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nCommand to execute the Chhai benchmark file using Node.js. This assumes the benchmark file is named `bench.js` and is located in the current directory.\n\n```sh\nnode bench.js\n```\n\n--------------------------------\n\n### Chhai `s.set` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.set` method, used to configure options for the current benchmark or the entire suite. It takes a `key` (string) representing the option name and a `value` (any type) to set for that option.\n\n```APIDOC\n// sets benchmark/suite options\ns.set(key: string, value: any)\n```\n\n--------------------------------\n\n### Chhai `s.bench` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.bench` method, used within a suite's implementation function to define a standard benchmark test. It takes a title (string) for identification and an implementation function containing the code to be benchmarked.\n\n```APIDOC\n// creates a new benchmark test\ns.bench(title: string, implementation: function)\n```\n\n--------------------------------\n\n### Chhai `s.burn` Function API\n\nSource: https://github.com/hypercubed/chuhai/blob/master/README.md\n\nDocuments the `s.burn` method, used within a suite to define a 'burn-in' benchmark test. These tests run but their results are not included when comparing performance against other benchmarks in the suite. It takes a title (string) and an implementation function.\n\n```APIDOC\n// creates a burn-in benchmark test\ns.burn(title: string, implementation: function)\n```\n\n=== LAST PAGE === This is the final page of results. No more pages are available. Do not request additional pages.",
            "codeBlocks": [
              {
                "language": "sh",
                "code": "npm i --save-dev chuhai",
                "context": "thub.com/hypercubed/chuhai/blob/master/README.md\n\nCommand to install Chhai as a development dependency using npm. This makes the library available for use in your project's development environment."
              },
              {
                "language": "APIDOC",
                "code": "// runs between benchmarks (place assertions checks here)\ns.cycle(implementation: function)",
                "context": "a function to be executed between each benchmark run within a suite. This is typically used for assertion checks, setup, or teardown logic to ensure consistent test conditions and validate results."
              },
              {
                "language": "APIDOC",
                "code": "// creates a new benchmark suite\n// returns a promise\nsuite([title: string], implementation: function): promise",
                "context": "lizes a new benchmark suite. It can optionally take a title (string) and requires an implementation function. The function returns a promise, allowing for asynchronous handling of benchmark results."
              },
              {
                "language": "js",
                "code": "var assert = require('assert');\nvar suite = require('chuhai');\n\n// starts a new benchmark suite\n// note this returns a promise\nsuite('array concat', function (s) {\n  var arr1 = ['a', 'b', 'c'];\n  var arr2 = ['d', 'e', 'f'];\n  var arr3 = null;\n\n  // run between each benchmark\n  s.cycle(function () {\n    // uses your assertion lib of choice\n    assert.deepEqual(arr3, ['a', 'b', 'c', 'd', 'e', 'f']);\n    arr3 = null;\n  });\n\n  // adds a bench that runs but doesn't get counted when comparing results to others.\n  s.burn('slice', function () {\n    arr3 = ['a', 'b', 'c', 'd', 'e', 'f'].slice();\n  });\n\n  // adds a bench.\n  s.bench('concat', function () {\n    arr3 = arr1.concat(arr2);\n  });\n\n  // adds a bench.\n  s.bench('for loop', function () {\n    var i;\n    var l1 = arr1.length;\n    var l2 = arr2.length;\n    arr3 = Array(l1 + l2);\n    for (i = 0; i < l1; i++) {\n      arr3[i] = arr1[i];\n    }\n    for (var i2 = 0; i2 < l2; i2++) {\n      arr3[i + i2] = arr2[i2];\n    }\n  });\n});",
                "context": "cycle` function for assertions between benchmarks, and shows `burn` and `bench` methods for different types of performance tests. The suite returns a promise, suitable for modern async test runners."
              },
              {
                "language": "sh",
                "code": "node bench.js",
                "context": ".com/hypercubed/chuhai/blob/master/README.md\n\nCommand to execute the Chhai benchmark file using Node.js. This assumes the benchmark file is named `bench.js` and is located in the current directory."
              },
              {
                "language": "APIDOC",
                "code": "// sets benchmark/suite options\ns.set(key: string, value: any)",
                "context": "s the `s.set` method, used to configure options for the current benchmark or the entire suite. It takes a `key` (string) representing the option name and a `value` (any type) to set for that option."
              },
              {
                "language": "APIDOC",
                "code": "// creates a new benchmark test\ns.bench(title: string, implementation: function)",
                "context": "used within a suite's implementation function to define a standard benchmark test. It takes a title (string) for identification and an implementation function containing the code to be benchmarked."
              },
              {
                "language": "APIDOC",
                "code": "// creates a burn-in benchmark test\ns.burn(title: string, implementation: function)",
                "context": "urn-in' benchmark test. These tests run but their results are not included when comparing performance against other benchmarks in the suite. It takes a title (string) and an implementation function."
              }
            ]
          }
        ]
      },
      {
        "packageId": "/mirage/alcotest",
        "packageName": "alcotest",
        "version": "latest",
        "fetchedAt": "2025-12-04T02:18:25.089Z",
        "content": "### Setting up Alcotest Environment\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nCommands to clone the Alcotest repository, set up an Opam switch, and install dependencies. It also includes optional steps for installing js_of_ocaml-compiler.\n\n```sh\ngit clone https://github.com/mirage/alcotest.git    # Get the repository\ncd alcotest\n\nopam switch create ./ ocaml-base-compiler.5.0.0    # OPTIONAL: install a project-local Opam switch\nopam install -t --deps-only .\n\nopam install -t js_of_ocaml-compiler\n```\n\n--------------------------------\n\n### Alcotest Basic Test Suite Example\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nDemonstrates how to define and run a simple test suite using Alcotest. It includes defining functions to test, writing test cases with assertions using `Alcotest.(check string)`, and organizing tests into suites using `Alcotest.run`.\n\n```ocaml\n(* Build with `ocamlbuild -pkg alcotest simple.byte` *)\n\n(* A module with functions to test *)\nmodule To_test = struct\n  let lowercase = String.lowercase_ascii\n  let capitalize = String.capitalize_ascii\n  let str_concat = String.concat \"\"\n  let list_concat = List.append\nend\n\n(* The tests *)\nlet test_lowercase () =\n  Alcotest.(check string) \"same string\" \"hello!\" (To_test.lowercase \"hELLO!\")\n\nlet test_capitalize () =\n  Alcotest.(check string) \"same string\" \"World.\" (To_test.capitalize \"world.\")\n\nlet test_str_concat () =\n  Alcotest.(check string) \"same string\" \"foobar\" (To_test.str_concat [\"foo\"; \"bar\"])\n\nlet test_list_concat () =\n  Alcotest.(check (list int)) \"same lists\" [1; 2; 3] (To_test.list_concat [1] [2; 3])\n\n(* Run it *)\nlet () =\n  let open Alcotest in\n  run \"Utils\" [\n      \"string-case\", [\n          test_case \"Lower case\"     `Quick test_lowercase;\n          test_case \"Capitalization\" `Quick test_capitalize;\n        ];\n      \"string-concat\", [ test_case \"String mashing\" `Quick test_str_concat  ];\n      \"list-concat\",   [ test_case \"List mashing\"   `Slow  test_list_concat ];\n    ]\n\n```\n\n--------------------------------\n\n### Alcotest Expect Test Structure\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nAn example of the file structure for an expect test, including the test case (`.ml`), expected output (`.expected`), and optional command-line options (`.opts`).\n\n```sh\ntest/e2e/alcotest/passing/\n basic.ml          # The test case\n basic.expected    # The expected output of the test case\n basic.opts        # OPTIONAL: command-line options to pass to the test\n```\n\n--------------------------------\n\n### Lwt Test Case Example\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nIllustrates how to use `Alcotest_lwt` to run Lwt-compatible test cases. It demonstrates handling asynchronous exceptions and using a switch for resource management within Lwt tests.\n\n```ocaml\nlet free () = print_endline \"freeing all resources\"; Lwt.return ()\n\nlet test_lwt switch () = \n  Lwt_switch.add_hook (Some switch) free;\n  Lwt.async (fun () -> failwith \"All is broken\");\n  Lwt_unix.sleep 10.\n\nlet () = \n  Lwt_main.run @@ Alcotest_lwt.run \"foo\" [\n    \"all\", [\n      Alcotest_lwt.test_case \"one\" `Quick test_lwt\n    ]\n  ]\n```\n\n--------------------------------\n\n### Alcotest Test Runner Commands and Options\n\nSource: https://github.com/mirage/alcotest/blob/main/alcotest-help.txt\n\nThis section details the commands and options available for the test.exe utility. It covers listing tests, running tests with filtering, and various configuration options that affect test execution and output.\n\n```APIDOC\nNAME\n       test.exe - Run all the tests.\n\nSYNOPSIS\n       test.exe [COMMAND] \n\nCOMMANDS\n       list [--color=WHEN] [OPTION]\n           List all available tests.\n\n       test [OPTION] [NAME_REGEX] [TESTCASES]\n           Run a subset of the tests.\n\nARGUMENTS\n       NAME_REGEX\n           A regular expression matching the names of tests to run\n\n       TESTCASES\n           A comma-separated list of test case numbers (and ranges of\n           numbers) to run, e.g: '4,6-10,19'. When specifying ranges, both\n           '-' and '..' are accepted as valid separators.\n\nOPTIONS\n       --bail (absent ALCOTEST_BAIL env)\n           Stop running tests after the first failure.\n\n       -c, --compact (absent ALCOTEST_COMPACT env)\n           Compact the output of the tests.\n\n       --color=WHEN (absent ALCOTEST_COLOR env)\n           Colorize the output. WHEN must be one of auto, always or never.\n           Defaults to 'always' when running inside Dune, otherwise defaults\n           to 'auto'.\n\n       -e, --show-errors (absent ALCOTEST_SHOW_ERRORS env)\n           Display the test errors.\n\n       --json\n           Display JSON for the results, to be used by a script.\n\n       -o DIR\n           Where to store the log files of the tests.\n\n       -q, --quick-tests (absent ALCOTEST_QUICK_TESTS env)\n           Run only the quick tests.\n\n       --tail-errors=N (absent ALCOTEST_TAIL_ERRORS env)\n           Show only the last N lines of output in case of an error.\n\n       -v, --verbose (absent ALCOTEST_VERBOSE env)\n           Display the test outputs. WARNING: when using this option the\n           output logs will not be available for further inspection.\n\nCOMMON OPTIONS\n       --help[=FMT] (default=auto)\n           Show this help in format FMT. The value FMT must be one of auto,\n           pager, groff or plain. With auto, the format is pager or plain\n           whenever the TERM env var is dumb or undefined.\n\nEXIT STATUS\n       test.exe exits with:\n\n       0   on success.\n\n       123 on indiscriminate errors reported on standard error.\n\n       124 on command line parsing errors.\n\n       125 on unexpected internal errors (bugs).\n```\n\n--------------------------------\n\n### Running Quick and Slow Tests\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nDemonstrates how to control the execution of quick and slow tests using command-line flags. The `-q` flag suppresses slow tests, allowing only quick tests to run.\n\n```sh-session\n$ ./test.exe -q # run only the quick tests\n$ ./test.exe    # run quick and slow tests\n```\n\n--------------------------------\n\n### Alcotest Project Structure Overview\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nA hierarchical view of the Alcotest project's directories and key files, illustrating the organization of source code, backends, and project metadata.\n\n```sh\n src/\n  alcotest-engine/\n   core.ml           # The main test runner, parameterised over...\n   monad.ml          # ... a concurrency monad\n   platform.ml       # ... a platform implementation / OS bindings\n      \n   cli.ml            # Extends `core.ml` with a command-line API\n      \n   pp.ml             # Pretty-printers for Alcotest output\n   test.ml           # Combinators for test assertions (with `Alcotest.check`)\n   utils.ml          # Standard-library extension\n   \n                          # Specific backends:\n  alcotest/             # - Unix-specific API (compatible with js_of_ocaml)\n  alcotest-async/       # - Extends `alcotest/` for test-suites using Async concurrency\n  alcotest-lwt/         # - Extends `alcotest/` for test-suites using Lwt concurrency\n  alcotest-mirage/      # - MirageOS-specific API\n\n test/                     # Project tests (see ' Expect tests' below)\n\n dune-project              # Project metadata (opam files are generated from\n alcotest-async.opam       # data in the `dune-project` file by running\n alcotest-lwt.opam         # `dune build @install`).\n alcotest-mirage.opam\n alcotest.opam\n\n alcotest-help.txt         # Manpage output (generated by `dune test --auto-promote`)\n dune\n\n CHANGES.md                # All user-facing changes get an entry here\n README.md\n```\n\n--------------------------------\n\n### Alcotest Build and Test Commands\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nStandard Dune commands for building the project, running tests, running JavaScript tests using js_of_ocaml, and cleaning build artifacts.\n\n```sh\ndune build              # Build all libraries, executables and tests\ndune test               # Run the test-suite\ndune build @runtest-js  # Run the test-suite on nodejs (using js_of_ocaml)\ndune clean              # Delete all artefacts\n```\n\n--------------------------------\n\n### Alcotest Test Selection by Name and Number\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to filter which tests to run using regular expressions for test names and specific test numbers or ranges. Demonstrates runtime filtering options.\n\n```sh\n$ ./simple.native test '.*concat*'\nTesting Utils.\n[SKIP]     string-case            0   Lower case.\n[SKIP]     string-case            1   Capitalization.\n[OK]       string-concat          0   String mashing.\n[OK]       list-concat            0   List mashing.\nThe full test results are available in `_build/_tests`.\nTest Successful in 0.000s. 2 tests run.\n\n$ ./simple.native test 'string-case' '1..3'\nTesting Utils.\n[SKIP]     string-case            0   Lower case.\n[OK]       string-case            1   Capitalization.\n[SKIP]     string-concat          0   String mashing.\n[SKIP]     list-concat            0   List mashing.\nThe full test results are available in `_build/_tests`.\nTest Successful in 0.000s. 1 test run.\n\n```\n\n--------------------------------\n\n### Custom Test Options with Cmdliner\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to pass custom options to Alcotest test functions using Cmdliner. This involves defining a Cmdliner term for the custom argument and using `Alcotest.run_with_args` to integrate it.\n\n```ocaml\nlet test_nice i = Alcotest.(check int) \"Is it a nice integer?\" i 42\n\nlet int = \n  let doc = \"What is your preferred number?\" in\n  Cmdliner.Arg.(required & opt (some int) None & info [\"n\"] ~doc ~docv:\"NUM\")\n\nlet () = \n  Alcotest.run_with_args \"foo\" int [\n    \"all\", [\"nice\", `Quick, test_nice]\n  ]\n```",
        "sections": [
          {
            "title": "Introduction",
            "type": "other",
            "content": "### Setting up Alcotest Environment\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nCommands to clone the Alcotest repository, set up an Opam switch, and install dependencies. It also includes optional steps for installing js_of_ocaml-compiler.\n\n```sh\ngit clone https://github.com/mirage/alcotest.git    # Get the repository\ncd alcotest\n\nopam switch create ./ ocaml-base-compiler.5.0.0    # OPTIONAL: install a project-local Opam switch\nopam install -t --deps-only .\n\nopam install -t js_of_ocaml-compiler\n```\n\n--------------------------------\n\n### Alcotest Basic Test Suite Example\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nDemonstrates how to define and run a simple test suite using Alcotest. It includes defining functions to test, writing test cases with assertions using `Alcotest.(check string)`, and organizing tests into suites using `Alcotest.run`.\n\n```ocaml\n(* Build with `ocamlbuild -pkg alcotest simple.byte` *)\n\n(* A module with functions to test *)\nmodule To_test = struct\n  let lowercase = String.lowercase_ascii\n  let capitalize = String.capitalize_ascii\n  let str_concat = String.concat \"\"\n  let list_concat = List.append\nend\n\n(* The tests *)\nlet test_lowercase () =\n  Alcotest.(check string) \"same string\" \"hello!\" (To_test.lowercase \"hELLO!\")\n\nlet test_capitalize () =\n  Alcotest.(check string) \"same string\" \"World.\" (To_test.capitalize \"world.\")\n\nlet test_str_concat () =\n  Alcotest.(check string) \"same string\" \"foobar\" (To_test.str_concat [\"foo\"; \"bar\"])\n\nlet test_list_concat () =\n  Alcotest.(check (list int)) \"same lists\" [1; 2; 3] (To_test.list_concat [1] [2; 3])\n\n(* Run it *)\nlet () =\n  let open Alcotest in\n  run \"Utils\" [\n      \"string-case\", [\n          test_case \"Lower case\"     `Quick test_lowercase;\n          test_case \"Capitalization\" `Quick test_capitalize;\n        ];\n      \"string-concat\", [ test_case \"String mashing\" `Quick test_str_concat  ];\n      \"list-concat\",   [ test_case \"List mashing\"   `Slow  test_list_concat ];\n    ]\n\n```\n\n--------------------------------\n\n### Alcotest Expect Test Structure\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nAn example of the file structure for an expect test, including the test case (`.ml`), expected output (`.expected`), and optional command-line options (`.opts`).\n\n```sh\ntest/e2e/alcotest/passing/\n basic.ml          # The test case\n basic.expected    # The expected output of the test case\n basic.opts        # OPTIONAL: command-line options to pass to the test\n```\n\n--------------------------------\n\n### Lwt Test Case Example\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nIllustrates how to use `Alcotest_lwt` to run Lwt-compatible test cases. It demonstrates handling asynchronous exceptions and using a switch for resource management within Lwt tests.\n\n```ocaml\nlet free () = print_endline \"freeing all resources\"; Lwt.return ()\n\nlet test_lwt switch () = \n  Lwt_switch.add_hook (Some switch) free;\n  Lwt.async (fun () -> failwith \"All is broken\");\n  Lwt_unix.sleep 10.\n\nlet () = \n  Lwt_main.run @@ Alcotest_lwt.run \"foo\" [\n    \"all\", [\n      Alcotest_lwt.test_case \"one\" `Quick test_lwt\n    ]\n  ]\n```\n\n--------------------------------\n\n### Alcotest Test Runner Commands and Options\n\nSource: https://github.com/mirage/alcotest/blob/main/alcotest-help.txt\n\nThis section details the commands and options available for the test.exe utility. It covers listing tests, running tests with filtering, and various configuration options that affect test execution and output.\n\n```APIDOC\nNAME\n       test.exe - Run all the tests.\n\nSYNOPSIS\n       test.exe [COMMAND] \n\nCOMMANDS\n       list [--color=WHEN] [OPTION]\n           List all available tests.\n\n       test [OPTION] [NAME_REGEX] [TESTCASES]\n           Run a subset of the tests.\n\nARGUMENTS\n       NAME_REGEX\n           A regular expression matching the names of tests to run\n\n       TESTCASES\n           A comma-separated list of test case numbers (and ranges of\n           numbers) to run, e.g: '4,6-10,19'. When specifying ranges, both\n           '-' and '..' are accepted as valid separators.\n\nOPTIONS\n       --bail (absent ALCOTEST_BAIL env)\n           Stop running tests after the first failure.\n\n       -c, --compact (absent ALCOTEST_COMPACT env)\n           Compact the output of the tests.\n\n       --color=WHEN (absent ALCOTEST_COLOR env)\n           Colorize the output. WHEN must be one of auto, always or never.\n           Defaults to 'always' when running inside Dune, otherwise defaults\n           to 'auto'.\n\n       -e, --show-errors (absent ALCOTEST_SHOW_ERRORS env)\n           Display the test errors.\n\n       --json\n           Display JSON for the results, to be used by a script.\n\n       -o DIR\n           Where to store the log files of the tests.\n\n       -q, --quick-tests (absent ALCOTEST_QUICK_TESTS env)\n           Run only the quick tests.\n\n       --tail-errors=N (absent ALCOTEST_TAIL_ERRORS env)\n           Show only the last N lines of output in case of an error.\n\n       -v, --verbose (absent ALCOTEST_VERBOSE env)\n           Display the test outputs. WARNING: when using this option the\n           output logs will not be available for further inspection.\n\nCOMMON OPTIONS\n       --help[=FMT] (default=auto)\n           Show this help in format FMT. The value FMT must be one of auto,\n           pager, groff or plain. With auto, the format is pager or plain\n           whenever the TERM env var is dumb or undefined.\n\nEXIT STATUS\n       test.exe exits with:\n\n       0   on success.\n\n       123 on indiscriminate errors reported on standard error.\n\n       124 on command line parsing errors.\n\n       125 on unexpected internal errors (bugs).\n```\n\n--------------------------------\n\n### Running Quick and Slow Tests\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nDemonstrates how to control the execution of quick and slow tests using command-line flags. The `-q` flag suppresses slow tests, allowing only quick tests to run.\n\n```sh-session\n$ ./test.exe -q # run only the quick tests\n$ ./test.exe    # run quick and slow tests\n```\n\n--------------------------------\n\n### Alcotest Project Structure Overview\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nA hierarchical view of the Alcotest project's directories and key files, illustrating the organization of source code, backends, and project metadata.\n\n```sh\n src/\n  alcotest-engine/\n   core.ml           # The main test runner, parameterised over...\n   monad.ml          # ... a concurrency monad\n   platform.ml       # ... a platform implementation / OS bindings\n      \n   cli.ml            # Extends `core.ml` with a command-line API\n      \n   pp.ml             # Pretty-printers for Alcotest output\n   test.ml           # Combinators for test assertions (with `Alcotest.check`)\n   utils.ml          # Standard-library extension\n   \n                          # Specific backends:\n  alcotest/             # - Unix-specific API (compatible with js_of_ocaml)\n  alcotest-async/       # - Extends `alcotest/` for test-suites using Async concurrency\n  alcotest-lwt/         # - Extends `alcotest/` for test-suites using Lwt concurrency\n  alcotest-mirage/      # - MirageOS-specific API\n\n test/                     # Project tests (see ' Expect tests' below)\n\n dune-project              # Project metadata (opam files are generated from\n alcotest-async.opam       # data in the `dune-project` file by running\n alcotest-lwt.opam         # `dune build @install`).\n alcotest-mirage.opam\n alcotest.opam\n\n alcotest-help.txt         # Manpage output (generated by `dune test --auto-promote`)\n dune\n\n CHANGES.md                # All user-facing changes get an entry here\n README.md\n```\n\n--------------------------------\n\n### Alcotest Build and Test Commands\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nStandard Dune commands for building the project, running tests, running JavaScript tests using js_of_ocaml, and cleaning build artifacts.\n\n```sh\ndune build              # Build all libraries, executables and tests\ndune test               # Run the test-suite\ndune build @runtest-js  # Run the test-suite on nodejs (using js_of_ocaml)\ndune clean              # Delete all artefacts\n```\n\n--------------------------------\n\n### Alcotest Test Selection by Name and Number\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to filter which tests to run using regular expressions for test names and specific test numbers or ranges. Demonstrates runtime filtering options.\n\n```sh\n$ ./simple.native test '.*concat*'\nTesting Utils.\n[SKIP]     string-case            0   Lower case.\n[SKIP]     string-case            1   Capitalization.\n[OK]       string-concat          0   String mashing.\n[OK]       list-concat            0   List mashing.\nThe full test results are available in `_build/_tests`.\nTest Successful in 0.000s. 2 tests run.\n\n$ ./simple.native test 'string-case' '1..3'\nTesting Utils.\n[SKIP]     string-case            0   Lower case.\n[OK]       string-case            1   Capitalization.\n[SKIP]     string-concat          0   String mashing.\n[SKIP]     list-concat            0   List mashing.\nThe full test results are available in `_build/_tests`.\nTest Successful in 0.000s. 1 test run.\n\n```\n\n--------------------------------\n\n### Custom Test Options with Cmdliner\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to pass custom options to Alcotest test functions using Cmdliner. This involves defining a Cmdliner term for the custom argument and using `Alcotest.run_with_args` to integrate it.\n\n```ocaml\nlet test_nice i = Alcotest.(check int) \"Is it a nice integer?\" i 42\n\nlet int = \n  let doc = \"What is your preferred number?\" in\n  Cmdliner.Arg.(required & opt (some int) None & info [\"n\"] ~doc ~docv:\"NUM\")\n\nlet () = \n  Alcotest.run_with_args \"foo\" int [\n    \"all\", [\"nice\", `Quick, test_nice]\n  ]\n```",
            "codeBlocks": [
              {
                "language": "sh",
                "code": "git clone https://github.com/mirage/alcotest.git    # Get the repository\ncd alcotest\n\nopam switch create ./ ocaml-base-compiler.5.0.0    # OPTIONAL: install a project-local Opam switch\nopam install -t --deps-only .\n\nopam install -t js_of_ocaml-compiler",
                "context": "e/alcotest/blob/main/CONTRIBUTING.md\n\nCommands to clone the Alcotest repository, set up an Opam switch, and install dependencies. It also includes optional steps for installing js_of_ocaml-compiler."
              },
              {
                "language": "ocaml",
                "code": "(* Build with `ocamlbuild -pkg alcotest simple.byte` *)\n\n(* A module with functions to test *)\nmodule To_test = struct\n  let lowercase = String.lowercase_ascii\n  let capitalize = String.capitalize_ascii\n  let str_concat = String.concat \"\"\n  let list_concat = List.append\nend\n\n(* The tests *)\nlet test_lowercase () =\n  Alcotest.(check string) \"same string\" \"hello!\" (To_test.lowercase \"hELLO!\")\n\nlet test_capitalize () =\n  Alcotest.(check string) \"same string\" \"World.\" (To_test.capitalize \"world.\")\n\nlet test_str_concat () =\n  Alcotest.(check string) \"same string\" \"foobar\" (To_test.str_concat [\"foo\"; \"bar\"])\n\nlet test_list_concat () =\n  Alcotest.(check (list int)) \"same lists\" [1; 2; 3] (To_test.list_concat [1] [2; 3])\n\n(* Run it *)\nlet () =\n  let open Alcotest in\n  run \"Utils\" [\n      \"string-case\", [\n          test_case \"Lower case\"     `Quick test_lowercase;\n          test_case \"Capitalization\" `Quick test_capitalize;\n        ];\n      \"string-concat\", [ test_case \"String mashing\" `Quick test_str_concat  ];\n      \"list-concat\",   [ test_case \"List mashing\"   `Slow  test_list_concat ];\n    ]",
                "context": "a simple test suite using Alcotest. It includes defining functions to test, writing test cases with assertions using `Alcotest.(check string)`, and organizing tests into suites using `Alcotest.run`."
              },
              {
                "language": "sh",
                "code": "test/e2e/alcotest/passing/\n basic.ml          # The test case\n basic.expected    # The expected output of the test case\n basic.opts        # OPTIONAL: command-line options to pass to the test",
                "context": "/alcotest/blob/main/CONTRIBUTING.md\n\nAn example of the file structure for an expect test, including the test case (`.ml`), expected output (`.expected`), and optional command-line options (`.opts`)."
              },
              {
                "language": "ocaml",
                "code": "let free () = print_endline \"freeing all resources\"; Lwt.return ()\n\nlet test_lwt switch () = \n  Lwt_switch.add_hook (Some switch) free;\n  Lwt.async (fun () -> failwith \"All is broken\");\n  Lwt_unix.sleep 10.\n\nlet () = \n  Lwt_main.run @@ Alcotest_lwt.run \"foo\" [\n    \"all\", [\n      Alcotest_lwt.test_case \"one\" `Quick test_lwt\n    ]\n  ]",
                "context": "/main/README.md\n\nIllustrates how to use `Alcotest_lwt` to run Lwt-compatible test cases. It demonstrates handling asynchronous exceptions and using a switch for resource management within Lwt tests."
              },
              {
                "language": "APIDOC",
                "code": "NAME\n       test.exe - Run all the tests.\n\nSYNOPSIS\n       test.exe [COMMAND] \n\nCOMMANDS\n       list [--color=WHEN] [OPTION]\n           List all available tests.\n\n       test [OPTION] [NAME_REGEX] [TESTCASES]\n           Run a subset of the tests.\n\nARGUMENTS\n       NAME_REGEX\n           A regular expression matching the names of tests to run\n\n       TESTCASES\n           A comma-separated list of test case numbers (and ranges of\n           numbers) to run, e.g: '4,6-10,19'. When specifying ranges, both\n           '-' and '..' are accepted as valid separators.\n\nOPTIONS\n       --bail (absent ALCOTEST_BAIL env)\n           Stop running tests after the first failure.\n\n       -c, --compact (absent ALCOTEST_COMPACT env)\n           Compact the output of the tests.\n\n       --color=WHEN (absent ALCOTEST_COLOR env)\n           Colorize the output. WHEN must be one of auto, always or never.\n           Defaults to 'always' when running inside Dune, otherwise defaults\n           to 'auto'.\n\n       -e, --show-errors (absent ALCOTEST_SHOW_ERRORS env)\n           Display the test errors.\n\n       --json\n           Display JSON for the results, to be used by a script.\n\n       -o DIR\n           Where to store the log files of the tests.\n\n       -q, --quick-tests (absent ALCOTEST_QUICK_TESTS env)\n           Run only the quick tests.\n\n       --tail-errors=N (absent ALCOTEST_TAIL_ERRORS env)\n           Show only the last N lines of output in case of an error.\n\n       -v, --verbose (absent ALCOTEST_VERBOSE env)\n           Display the test outputs. WARNING: when using this option the\n           output logs will not be available for further inspection.\n\nCOMMON OPTIONS\n       --help[=FMT] (default=auto)\n           Show this help in format FMT. The value FMT must be one of auto,\n           pager, groff or plain. With auto, the format is pager or plain\n           whenever the TERM env var is dumb or undefined.\n\nEXIT STATUS\n       test.exe exits with:\n\n       0   on success.\n\n       123 on indiscriminate errors reported on standard error.\n\n       124 on command line parsing errors.\n\n       125 on unexpected internal errors (bugs).",
                "context": "n details the commands and options available for the test.exe utility. It covers listing tests, running tests with filtering, and various configuration options that affect test execution and output."
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Alcotest Project Structure Overview\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nA hierarchical view of the Alcotest project's directories and key files, illustrating the organization of source code, backends, and project metadata.",
                "context": "```sh-session\n$ ./test.exe -q # run only the quick tests\n$ ./test.exe    # run quick and slow tests"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Alcotest Build and Test Commands\n\nSource: https://github.com/mirage/alcotest/blob/main/CONTRIBUTING.md\n\nStandard Dune commands for building the project, running tests, running JavaScript tests using js_of_ocaml, and cleaning build artifacts.",
                "context": "\n CHANGES.md                # All user-facing changes get an entry here\n README.md"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Alcotest Test Selection by Name and Number\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to filter which tests to run using regular expressions for test names and specific test numbers or ranges. Demonstrates runtime filtering options.",
                "context": "dune test               # Run the test-suite\ndune build @runtest-js  # Run the test-suite on nodejs (using js_of_ocaml)\ndune clean              # Delete all artefacts"
              },
              {
                "language": "text",
                "code": "--------------------------------\n\n### Custom Test Options with Cmdliner\n\nSource: https://github.com/mirage/alcotest/blob/main/README.md\n\nShows how to pass custom options to Alcotest test functions using Cmdliner. This involves defining a Cmdliner term for the custom argument and using `Alcotest.run_with_args` to integrate it.",
                "context": "[SKIP]     list-concat            0   List mashing.\nThe full test results are available in `_build/_tests`.\nTest Successful in 0.000s. 1 test run."
              }
            ]
          }
        ]
      }
    ]
  },
  "web": {
    "sources": [],
    "selectedSources": [],
    "fetchedContent": []
  },
  "metadata": {
    "totalPages": 84,
    "totalCodeBlocks": 292,
    "primaryVersion": "latest"
  }
}