---
description: E2E Test Validation Pipeline - Validate test coverage and correctness through iterative manual validation
model: claude-opus-4-1-20250805
tools: Bash, BashOutput, Glob, Grep, KillBash, Read, Write, TodoWrite
---

**E2E Test Validation Pipeline**
**Do NOT perform manual analysis - ALL work through validation phases and agents**

You are orchestrating the **E2E Test Validation Pipeline**. Your goal is to validate test coverage and correctness by manually executing user workflows, analyzing coverage gaps, updating tests, and iterating until coverage is sufficient.

**Feature/Workflow to Validate**: $ARGUMENTS

# E2E Test Validation Pipeline

The E2E Validation system implements a **5-phase iterative validation pipeline** with satisfaction gates and intelligent environment detection:

ðŸ”§ **Initialization Phase**: Setup validation workspace, detect environment, and determine authentication strategy
ðŸ” **Manual Validation Phase**: chrome-devtools-explorer executes workflows manually with environment-aware authentication
ðŸ“Š **Coverage Analysis Phase**: Analyze gaps between manual execution and tests
âœï¸ **Test Update Phase**: e2e-test-engineer creates/updates tests
âœ… **Re-validation Phase**: Verify updated tests work correctly
ðŸ”„ **Iteration Decision**: Check satisfaction criteria or iterate

## Environment Detection & Authentication

The validation pipeline automatically detects your environment configuration and determines the appropriate authentication strategy:

### Environment Classification

- **Local Backend**: Custom stack names (e.g., 'chariot-dev', 'nathan-test') deployed via `make chariot`
- **UAT Backend**: VITE_BACKEND='chariot-uat' or API URL contains 'uat'
- **Production Backend**: VITE_BACKEND='chariot' with production API endpoint

### Authentication Strategies

1. **keychain-url** (Local with credentials):
   - Automatically uses CHARIOT_LOGIN_URL from .env
   - Generated by running `make user`
   - Direct navigation to keychain URL for instant authentication

2. **pre-authenticated** (UAT/Production):
   - Assumes browser already authenticated
   - No login attempt - verification only
   - Reports error if not authenticated

3. **make-user-required** (Local without credentials):
   - Stops validation with clear error
   - Provides instructions to run `make user`
   - Prevents wasted validation attempts

4. **manual** (Fallback):
   - Uses keychain.ini upload or demo account
   - For special cases or custom configurations

### Environment Mismatch Detection

The pipeline detects and warns about mismatched configurations:
- Local backend with remote frontend
- UAT backend with local frontend  
- Production backend with non-production frontend

### Prerequisites for Local Testing

For local backend testing, you MUST run `make user` first:
```bash
make user  # Generates test user and keychain URL
```

This creates:
- Test user with UUID credentials
- CHARIOT_LOGIN_URL in .env
- Automatic authentication capability

## Directory Structure

The validation pipeline creates workflow-specific directory structures:

```
.claude/
â””â”€â”€ workflows/
    â””â”€â”€ e2e-validation_{VALIDATION_ID}/     # Validation-specific workspace
        â”œâ”€â”€ pipeline/
        â”‚   â””â”€â”€ validation-pipeline.log     # Pipeline execution logs
        â”œâ”€â”€ metadata.json                   # Validation metadata
        â”œâ”€â”€ baseline/                       # Initial test inventory
        â”‚   â”œâ”€â”€ test-inventory.json
        â”‚   â”œâ”€â”€ coverage-baseline.json
        â”‚   â””â”€â”€ existing-tests/
        â”œâ”€â”€ iteration-{N}/                  # Per-iteration workspace
        â”‚   â”œâ”€â”€ manual-validation/
        â”‚   â”‚   â”œâ”€â”€ workflow-report.md
        â”‚   â”‚   â”œâ”€â”€ screenshots/
        â”‚   â”‚   â”œâ”€â”€ network-logs/
        â”‚   â”‚   â””â”€â”€ issues.json
        â”‚   â”œâ”€â”€ coverage-analysis/
        â”‚   â”‚   â”œâ”€â”€ gaps.json
        â”‚   â”‚   â”œâ”€â”€ recommendations.md
        â”‚   â”‚   â””â”€â”€ priority-matrix.json
        â”‚   â”œâ”€â”€ test-updates/
        â”‚   â”‚   â”œâ”€â”€ changes-summary.md
        â”‚   â”‚   â”œâ”€â”€ updated-tests/
        â”‚   â”‚   â””â”€â”€ new-tests/
        â”‚   â””â”€â”€ validation-results.md
        â””â”€â”€ final-coverage-report.md
```

**Key Benefits:**
- **Validation Isolation**: Each validation maintains its own state and logs
- **Iteration Tracking**: Complete history of validation cycles
- **Resume Capability**: Resume validation from any iteration
- **Evidence-Based**: Screenshots and network logs for all validations

## Pipeline Execution

### Phase 1: Initialization

**Step 1: Determine Execution Mode & Parse Input**

```bash
# Strategic decision (core orchestration logic)
if [[ "$ARGUMENTS" =~ ^e2e-validation-[a-z0-9-]+_[0-9]{8}_[0-9]{6}$ ]]; then
    VALIDATION_ID="$ARGUMENTS"
    EXECUTION_MODE="resume"
    echo "ðŸ”„ Resume Mode: ${VALIDATION_ID}"
else
    # Parse input: URL or description
    if [[ "$ARGUMENTS" =~ ^https?:// ]]; then
        # URL-based validation
        INPUT_TYPE="url"
        TARGET_URL="$ARGUMENTS"
        WORKFLOW_DESCRIPTION="Validate page: ${TARGET_URL}"
        echo "ðŸŒ URL Mode: ${TARGET_URL}"
    else
        # Description-based validation
        INPUT_TYPE="description"
        WORKFLOW_DESCRIPTION="$ARGUMENTS"
        TARGET_URL=""
        echo "ðŸ“ Description Mode: ${WORKFLOW_DESCRIPTION}"
    fi
    EXECUTION_MODE="new"
    echo "ðŸš€ New Validation: ${WORKFLOW_DESCRIPTION}"
fi

# Environment Detection and Classification
echo "ðŸ” Detecting environment configuration..." | tee -a "${PIPELINE_LOG}"

# Read .env configuration
ENV_FILE="modules/chariot/ui/.env"
if [ -f "${ENV_FILE}" ]; then
    VITE_BACKEND=$(grep "^VITE_BACKEND=" "${ENV_FILE}" | cut -d'=' -f2 | tr -d '"')
    VITE_API_URL=$(grep "^VITE_API_URL=" "${ENV_FILE}" | cut -d'=' -f2 | tr -d '"')
    VITE_CLIENT_ID=$(grep "^VITE_CLIENT_ID=" "${ENV_FILE}" | cut -d'=' -f2 | tr -d '"')
    VITE_USER_POOL_ID=$(grep "^VITE_USER_POOL_ID=" "${ENV_FILE}" | cut -d'=' -f2 | tr -d '"')
    CHARIOT_LOGIN_URL=$(grep "^CHARIOT_LOGIN_URL=" "${ENV_FILE}" | cut -d'=' -f2)
    
    echo "ðŸ“‹ Environment Variables Loaded:" | tee -a "${PIPELINE_LOG}"
    echo "  VITE_BACKEND: ${VITE_BACKEND}" | tee -a "${PIPELINE_LOG}"
    echo "  VITE_API_URL: ${VITE_API_URL}" | tee -a "${PIPELINE_LOG}"
else
    echo "âš ï¸ Warning: ${ENV_FILE} not found" | tee -a "${PIPELINE_LOG}"
fi

# Environment Classification Function
classify_environment() {
    local backend="${1:-$VITE_BACKEND}"
    local api_url="${2:-$VITE_API_URL}"
    
    # Check for UAT backend
    if [[ "${backend}" == "chariot-uat" ]] || [[ "${api_url}" =~ "uat" ]]; then
        echo "uat"
    # Check for production backend
    elif [[ "${backend}" == "chariot" ]] && [[ "${api_url}" =~ "praetorian" ]]; then
        echo "production"
    # Everything else is local development
    else
        echo "local"
    fi
}

# Frontend Target Detection
detect_frontend_target() {
    local url="${1:-$TARGET_URL}"
    
    if [[ -z "${url}" ]]; then
        # Default to localhost:3000 if no URL specified
        echo "https://localhost:3000"
    elif [[ "${url}" =~ localhost:[0-9]+ ]]; then
        echo "${url}"
    elif [[ "${url}" =~ uat\.chariot\.praetorian\.com ]]; then
        echo "${url}"
    elif [[ "${url}" =~ chariot\.praetorian\.com ]]; then
        echo "${url}"
    else
        echo "${url}"
    fi
}

# Keychain URL Detection
detect_keychain_url() {
    if [[ -n "${CHARIOT_LOGIN_URL}" ]]; then
        echo "${CHARIOT_LOGIN_URL}"
    else
        # Try to extract from keychain.ini if available
        local keychain_file="modules/chariot/backend/keychain.ini"
        if [ -f "${keychain_file}" ]; then
            # Extract keychain URL from file
            local keychain_content=$(cat "${keychain_file}" | base64)
            echo "https://localhost:3000/login-with-keychain?keychain=${keychain_content}"
        else
            echo ""
        fi
    fi
}

# Determine environment type
BACKEND_ENV=$(classify_environment)
FRONTEND_TARGET=$(detect_frontend_target)
KEYCHAIN_URL=$(detect_keychain_url)

echo "ðŸŽ¯ Environment Classification:" | tee -a "${PIPELINE_LOG}"
echo "  Backend Environment: ${BACKEND_ENV}" | tee -a "${PIPELINE_LOG}"
echo "  Frontend Target: ${FRONTEND_TARGET}" | tee -a "${PIPELINE_LOG}"

# Authentication Strategy Decision
determine_auth_strategy() {
    local backend_env="${1}"
    local keychain_url="${2}"
    
    if [[ "${backend_env}" == "local" ]]; then
        if [[ -n "${keychain_url}" ]]; then
            echo "keychain-url"
        else
            echo "make-user-required"
        fi
    elif [[ "${backend_env}" == "uat" ]] || [[ "${backend_env}" == "production" ]]; then
        echo "pre-authenticated"
    else
        echo "manual"
    fi
}

AUTH_STRATEGY=$(determine_auth_strategy "${BACKEND_ENV}" "${KEYCHAIN_URL}")
echo "  Authentication Strategy: ${AUTH_STRATEGY}" | tee -a "${PIPELINE_LOG}"

# Environment Mismatch Detection
check_environment_mismatch() {
    local backend_env="${1}"
    local frontend_target="${2}"
    
    # Check for mismatches
    if [[ "${backend_env}" == "local" ]] && [[ "${frontend_target}" =~ (uat|praetorian)\.com ]]; then
        echo "âš ï¸ Warning: Local backend with remote frontend" | tee -a "${PIPELINE_LOG}"
        return 1
    elif [[ "${backend_env}" == "uat" ]] && [[ "${frontend_target}" =~ localhost ]]; then
        echo "âš ï¸ Warning: UAT backend with local frontend" | tee -a "${PIPELINE_LOG}"
        return 1
    elif [[ "${backend_env}" == "production" ]] && [[ "${frontend_target}" =~ (localhost|uat) ]]; then
        echo "âš ï¸ Warning: Production backend with non-production frontend" | tee -a "${PIPELINE_LOG}"
        return 1
    fi
    
    echo "âœ… Environment configuration validated" | tee -a "${PIPELINE_LOG}"
    return 0
}

check_environment_mismatch "${BACKEND_ENV}" "${FRONTEND_TARGET}"

# Prerequisite Validation
validate_prerequisites() {
    local backend_env="${1}"
    local auth_strategy="${2}"
    
    echo "ðŸ” Validating prerequisites..." | tee -a "${PIPELINE_LOG}"
    
    if [[ "${auth_strategy}" == "make-user-required" ]]; then
        echo "âŒ Error: Local backend testing requires keychain URL" | tee -a "${PIPELINE_LOG}"
        echo "" | tee -a "${PIPELINE_LOG}"
        echo "**Local Backend Testing Prerequisites:**" | tee -a "${PIPELINE_LOG}"
        echo "1. Run 'make user' to generate test user and keychain URL" | tee -a "${PIPELINE_LOG}"
        echo "2. Verify CHARIOT_LOGIN_URL exists in .env" | tee -a "${PIPELINE_LOG}"
        echo "3. UI must be running on https://localhost:3000" | tee -a "${PIPELINE_LOG}"
        echo "" | tee -a "${PIPELINE_LOG}"
        echo "After running 'make user', retry this validation." | tee -a "${PIPELINE_LOG}"
        exit 1
    fi
    
    if [[ "${auth_strategy}" == "keychain-url" ]]; then
        echo "âœ… Keychain URL detected: ${KEYCHAIN_URL:0:50}..." | tee -a "${PIPELINE_LOG}"
    fi
    
    echo "âœ… Prerequisites validated" | tee -a "${PIPELINE_LOG}"
}

validate_prerequisites "${BACKEND_ENV}" "${AUTH_STRATEGY}"

# Mechanical pipeline initialization (delegated to script)
INIT_OUTPUT=$(.claude/scripts/workflows/e2e-validation/initialize-validation.sh "${EXECUTION_MODE}" "${ARGUMENTS}")
echo "${INIT_OUTPUT}"

# Extract initialization results
PIPELINE_LOG=$(echo "${INIT_OUTPUT}" | grep "PIPELINE_LOG=" | cut -d'=' -f2)
PIPELINE_DIR=$(echo "${INIT_OUTPUT}" | grep "PIPELINE_DIR=" | cut -d'=' -f2)
VALIDATION_ID=$(echo "${INIT_OUTPUT}" | grep "VALIDATION_ID=" | cut -d'=' -f2)
VALIDATION_WORKSPACE=$(echo "${INIT_OUTPUT}" | grep "VALIDATION_WORKSPACE=" | cut -d'=' -f2)
BASELINE_DIR=$(echo "${INIT_OUTPUT}" | grep "BASELINE_DIR=" | cut -d'=' -f2)
INPUT_TYPE=$(echo "${INIT_OUTPUT}" | grep "INPUT_TYPE=" | cut -d'=' -f2)
TARGET_URL=$(echo "${INIT_OUTPUT}" | grep "TARGET_URL=" | cut -d'=' -f2)
INIT_STATUS=$(echo "${INIT_OUTPUT}" | grep "INITIALIZATION_STATUS=" | cut -d'=' -f2)

# Validate initialization
if [ "${INIT_STATUS}" != "success" ] || [ ! -f "${PIPELINE_LOG}" ]; then
    echo "âŒ Validation initialization failed" | tee -a "${PIPELINE_LOG}"
    exit 1
fi

echo "âœ… Validation infrastructure ready - Mode: ${EXECUTION_MODE}" | tee -a "${PIPELINE_LOG}"
echo "ðŸ“ Validation Workspace: ${VALIDATION_ID}" | tee -a "${PIPELINE_LOG}"
```

**Wait for initialize-validation.sh to complete before continuing**

**Step 2: Baseline Test Inventory**

```bash
# For new validations, create baseline test inventory
if [ "${EXECUTION_MODE}" = "new" ]; then
    echo "ðŸ“‹ Creating baseline test inventory" | tee -a "${PIPELINE_LOG}"
    
    # Discover existing E2E tests
    TEST_INVENTORY="${BASELINE_DIR}/test-inventory.json"
    
    # Find all E2E test files
    find modules/chariot/e2e -name "*.spec.ts" -o -name "*.test.ts" > "${BASELINE_DIR}/test-files.txt"
    
    TEST_COUNT=$(wc -l < "${BASELINE_DIR}/test-files.txt")
    
    echo "Found ${TEST_COUNT} existing test files" | tee -a "${PIPELINE_LOG}"
    
    # Create structured inventory
    cat > "${TEST_INVENTORY}" << EOF
{
  "total_test_files": ${TEST_COUNT},
  "test_files": $(cat "${BASELINE_DIR}/test-files.txt" | jq -R . | jq -s .),
  "inventory_created": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
    
    echo "âœ… Test inventory created: ${TEST_INVENTORY}" | tee -a "${PIPELINE_LOG}"
fi
```

**Step 3: Determine Current Iteration**

```bash
# For resume mode, detect current iteration
if [ "${EXECUTION_MODE}" = "resume" ]; then
    CURRENT_ITERATION=$(cat "${VALIDATION_WORKSPACE}/metadata.json" | jq -r '.current_iteration // 1')
    VALIDATION_STATUS=$(cat "${VALIDATION_WORKSPACE}/metadata.json" | jq -r '.status // "in_progress"')
    
    echo "ðŸ“ Current Iteration: ${CURRENT_ITERATION}" | tee -a "${PIPELINE_LOG}"
    echo "ðŸ“ Status: ${VALIDATION_STATUS}" | tee -a "${PIPELINE_LOG}"
    
    if [ "${VALIDATION_STATUS}" = "completed" ]; then
        echo "âœ… Validation already completed" | tee -a "${PIPELINE_LOG}"
        exit 0
    fi
else
    CURRENT_ITERATION=1
    echo "ðŸŽ¯ Starting iteration ${CURRENT_ITERATION}" | tee -a "${PIPELINE_LOG}"
fi
```

### Phase 2: Iterative Validation Cycle

**Iteration Loop Structure**

```bash
# Maximum iterations before manual review required
MAX_ITERATIONS=5

while [ ${CURRENT_ITERATION} -le ${MAX_ITERATIONS} ]; do
    echo "===========================================" | tee -a "${PIPELINE_LOG}"
    echo "ðŸ”„ ITERATION ${CURRENT_ITERATION} of ${MAX_ITERATIONS}" | tee -a "${PIPELINE_LOG}"
    echo "===========================================" | tee -a "${PIPELINE_LOG}"
    
    # Setup iteration workspace
    ITERATION_SETUP=$(.claude/scripts/workflows/e2e-validation/setup-iteration.sh "${VALIDATION_ID}" "${CURRENT_ITERATION}")
    echo "${ITERATION_SETUP}"
    
    # Extract iteration paths
    ITERATION_DIR=$(echo "${ITERATION_SETUP}" | grep "ITERATION_DIR=" | cut -d'=' -f2)
    MANUAL_VALIDATION_DIR=$(echo "${ITERATION_SETUP}" | grep "MANUAL_VALIDATION_DIR=" | cut -d'=' -f2)
    COVERAGE_ANALYSIS_DIR=$(echo "${ITERATION_SETUP}" | grep "COVERAGE_ANALYSIS_DIR=" | cut -d'=' -f2)
    TEST_UPDATES_DIR=$(echo "${ITERATION_SETUP}" | grep "TEST_UPDATES_DIR=" | cut -d'=' -f2)
    
    echo "ðŸ“ Iteration workspace: ${ITERATION_DIR}" | tee -a "${PIPELINE_LOG}"
```

**Step 2.1: Manual Validation (chrome-devtools-explorer)**

```bash
    echo "ðŸ” Phase: Manual Workflow Validation" | tee -a "${PIPELINE_LOG}"
    
    WORKFLOW_REPORT="${MANUAL_VALIDATION_DIR}/workflow-report.md"
    SCREENSHOTS_DIR="${MANUAL_VALIDATION_DIR}/screenshots"
    NETWORK_LOGS_DIR="${MANUAL_VALIDATION_DIR}/network-logs"
    ISSUES_FILE="${MANUAL_VALIDATION_DIR}/issues.json"
    
    mkdir -p "${SCREENSHOTS_DIR}" "${NETWORK_LOGS_DIR}"
    
    # Prepare context for chrome-devtools-explorer
    VALIDATION_CONTEXT="${MANUAL_VALIDATION_DIR}/validation-context.md"
    cat > "${VALIDATION_CONTEXT}" << EOF
# Manual Validation Context - Iteration ${CURRENT_ITERATION}

## Environment Configuration
- **Backend Environment**: ${BACKEND_ENV}
- **Frontend Target**: ${FRONTEND_TARGET}
- **Authentication Strategy**: ${AUTH_STRATEGY}
- **Keychain URL**: ${KEYCHAIN_URL:-Not Available}
- **API Endpoint**: ${VITE_API_URL}
- **Backend Stack**: ${VITE_BACKEND}

## Authentication Instructions
$(case ${AUTH_STRATEGY} in
    keychain-url)
        echo "âœ… **Use keychain URL for automatic login**"
        echo "Navigate to: ${KEYCHAIN_URL}"
        ;;
    pre-authenticated)
        echo "âœ… **Assume pre-authenticated session**"
        echo "Browser should already be logged in for ${BACKEND_ENV} environment"
        echo "Verify authentication state before proceeding"
        ;;
    make-user-required)
        echo "âŒ **Authentication not possible - make user required**"
        echo "This validation cannot proceed without keychain credentials"
        ;;
    *)
        echo "âš ï¸ **Manual authentication required**"
        echo "Use demo account or manual login process"
        ;;
esac)

## Input Type
${INPUT_TYPE:-description}

## Target URL (if provided)
${TARGET_URL:-N/A - use description below}

## Workflow to Validate
${WORKFLOW_DESCRIPTION}

## Existing Test Inventory
$(cat "${BASELINE_DIR}/test-inventory.json" | jq -r '.test_files[]' | head -20)

## Previous Iteration Issues (if any)
EOF
    
    if [ ${CURRENT_ITERATION} -gt 1 ]; then
        PREV_ITERATION=$((CURRENT_ITERATION - 1))
        cat ".claude/workflows/${VALIDATION_ID}/iteration-${PREV_ITERATION}/validation-results.md" >> "${VALIDATION_CONTEXT}" 2>/dev/null || echo "No previous results" >> "${VALIDATION_CONTEXT}"
    fi
```

**Spawn chrome-devtools-explorer for manual validation:**

Use the `chrome-devtools-explorer` subagent to manually execute the workflow.

Instruct the chrome-devtools-explorer:
"Execute the workflow and document all behaviors using your existing browser connection.

**Read validation context using the Read tool on this exact file path:** ${VALIDATION_CONTEXT}
**NOTE: This is a FILE path, not a directory. Use Read tool with the complete path above.**

**CRITICAL: Environment-Aware Authentication**
1. **First, read the validation context** to understand:
   - Backend Environment (local/uat/production)
   - Authentication Strategy (keychain-url/pre-authenticated/make-user-required)
   - Target Frontend URL
   - Any environment mismatches

2. **Follow the authentication strategy** specified in the context:
   - **keychain-url**: Navigate to the provided CHARIOT_LOGIN_URL
   - **pre-authenticated**: Verify existing authentication, DO NOT attempt login
   - **make-user-required**: STOP and report that 'make user' must be run first
   - **manual**: Use keychain.ini upload or demo account

3. **Verify authentication** before proceeding:
   - Check for user menu/avatar in top-right
   - Monitor API responses for 401/403 errors
   - Report any authentication failures immediately

4. **Handle authentication failures**:
   - If keychain URL fails: Report expired credentials, suggest re-running 'make user'
   - If pre-authenticated fails: Report browser not authenticated for ${BACKEND_ENV} environment
   - If 401/403 errors occur: Stop validation and report authentication failure
   - Document the specific failure type and provide remediation steps

**Your mission:**

**URL Mode (if TARGET_URL provided):**
1. Use \`list_pages\` to find tabs
2. Navigate to ${TARGET_URL} or select existing tab with that URL
3. Explore ALL clickable elements on this page
4. Test every interactive element (buttons, links, forms, dropdowns, etc.)
5. Document behavior of each interaction
6. Capture screenshots of important states
7. Monitor network requests for all interactions

**Description Mode (if no URL):**
1. Use \`list_pages\` to understand current browser state
2. Follow the workflow description from validation context
3. Execute each step described
4. Document behaviors and interactions
5. Capture evidence at each step

**For ALL modes:**
- Capture screenshots at key interaction points
- Monitor network requests for all API calls using \`list_network_requests\`
- Document every user action and system response
- Identify any UI issues, timing problems, or unexpected behaviors
- Test edge cases and error conditions

**Save your findings to:**
- Workflow report: ${WORKFLOW_REPORT}
- Screenshots: ${SCREENSHOTS_DIR}/step-{N}.png
- Issues found: ${ISSUES_FILE}

**Report format for workflow-report.md:**

```markdown
# Manual Workflow Validation - Iteration ${CURRENT_ITERATION}

## Workflow Steps Executed

### Step 1: [Action Name]
- **User Action**: [What was clicked/typed]
- **Expected Behavior**: [What should happen]
- **Actual Behavior**: [What happened]
- **Screenshot**: step-1.png
- **Network Requests**: [List API calls]
- **Issues**: [Any problems]

[... repeat for all steps ...]

## Summary
- Total steps executed: [N]
- Steps working correctly: [N]
- Issues found: [N]
- Critical blockers: [N]

## Issues Discovered
[Detailed list of any problems]
```

**For issues.json format:**

```json
{
  "iteration": ${CURRENT_ITERATION},
  "issues": [
    {
      "severity": "critical|major|minor",
      "type": "selector-invalid|timing-issue|missing-wait|assertion-failure",
      "description": "Detailed issue description",
      "step": "Which workflow step",
      "screenshot_ref": "step-N.png"
    }
  ]
}
```

Be thorough and document everything you observe."

**Wait for chrome-devtools-explorer to complete before continuing**

**Step 2.2: Coverage Analysis**

```bash
    echo "ðŸ“Š Phase: Coverage Analysis" | tee -a "${PIPELINE_LOG}"
    
    COVERAGE_GAPS="${COVERAGE_ANALYSIS_DIR}/gaps.json"
    RECOMMENDATIONS="${COVERAGE_ANALYSIS_DIR}/recommendations.md"
    
    # Analyze manual validation against existing tests
    echo "Analyzing coverage gaps between manual validation and existing tests..."
    
    # Read manual validation results
    MANUAL_STEPS=$(grep -c "### Step" "${WORKFLOW_REPORT}" 2>/dev/null || echo "0")
    ISSUES_COUNT=$(cat "${ISSUES_FILE}" | jq '.issues | length' 2>/dev/null || echo "0")
    
    echo "Manual validation: ${MANUAL_STEPS} steps, ${ISSUES_COUNT} issues" | tee -a "${PIPELINE_LOG}"
    
    # Create coverage analysis
    cat > "${COVERAGE_GAPS}" << EOF
{
  "iteration": ${CURRENT_ITERATION},
  "manual_validation": {
    "steps_executed": ${MANUAL_STEPS},
    "issues_found": ${ISSUES_COUNT}
  },
  "coverage_gaps": [],
  "test_recommendations": [],
  "analysis_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
    
    # Create recommendations for e2e-test-engineer
    cat > "${RECOMMENDATIONS}" << EOF
# Test Coverage Recommendations - Iteration ${CURRENT_ITERATION}

## Manual Validation Summary
- Steps executed: ${MANUAL_STEPS}
- Issues found: ${ISSUES_COUNT}

## Coverage Gaps Identified

[To be filled by analysis of manual validation vs existing tests]

## Recommended Test Updates

### Tests to Create
[List new test files needed]

### Tests to Update  
[List existing tests needing updates]

### Tests to Fix
[List broken/flaky tests]

## Priority Matrix
- Critical (blocks release): [count]
- High (important coverage): [count]
- Medium (nice to have): [count]
EOF
    
    echo "âœ… Coverage analysis complete" | tee -a "${PIPELINE_LOG}"
```

**Step 2.3: Test Updates (e2e-test-engineer)**

```bash
    echo "âœï¸ Phase: Test Updates" | tee -a "${PIPELINE_LOG}"
    
    CHANGES_SUMMARY="${TEST_UPDATES_DIR}/changes-summary.md"
    UPDATED_TESTS_DIR="${TEST_UPDATES_DIR}/updated-tests"
    NEW_TESTS_DIR="${TEST_UPDATES_DIR}/new-tests"
    
    mkdir -p "${UPDATED_TESTS_DIR}" "${NEW_TESTS_DIR}"
```

**Spawn e2e-test-engineer for test updates:**

Use the `e2e-test-engineer` subagent to create/update tests.

Instruct the e2e-test-engineer:
"Update E2E tests based on manual validation findings and coverage analysis.

**Read context from:**
- Manual validation: ${WORKFLOW_REPORT}
- Issues found: ${ISSUES_FILE}
- Coverage recommendations: ${RECOMMENDATIONS}
- Existing tests: ${BASELINE_DIR}/test-inventory.json

**Your mission:**
1. Review manual validation results
2. Identify missing test coverage
3. Fix any broken/flaky tests identified
4. Create new tests for uncovered workflows
5. Ensure selectors are stable and maintainable

**Save your work to:**
- Changes summary: ${CHANGES_SUMMARY}
- Updated tests: ${UPDATED_TESTS_DIR}/
- New tests: ${NEW_TESTS_DIR}/

**Changes summary format:**

```markdown
# Test Updates - Iteration ${CURRENT_ITERATION}

## Summary
- Tests created: [N]
- Tests updated: [N]
- Tests fixed: [N]
- Total coverage improvement: [X]%

## New Tests Created
[List with file paths and purpose]

## Tests Updated
[List with file paths and changes]

## Tests Fixed
[List with file paths and fixes]

## Testing Notes
[Any important testing considerations]
```

Follow Playwright best practices and Chariot E2E patterns."

**Wait for e2e-test-engineer to complete before continuing**

**Step 2.4: Validation Results & Iteration Decision**

```bash
    echo "âœ… Phase: Validation Results" | tee -a "${PIPELINE_LOG}"
    
    VALIDATION_RESULTS="${ITERATION_DIR}/validation-results.md"
    
    # Count test changes
    NEW_TESTS=$(find "${NEW_TESTS_DIR}" -name "*.spec.ts" -o -name "*.test.ts" 2>/dev/null | wc -l)
    UPDATED_TESTS=$(find "${UPDATED_TESTS_DIR}" -name "*.spec.ts" -o -name "*.test.ts" 2>/dev/null | wc -l)
    
    # Create validation results
    cat > "${VALIDATION_RESULTS}" << EOF
# Validation Results - Iteration ${CURRENT_ITERATION}

## Iteration Summary
- Manual steps validated: ${MANUAL_STEPS}
- Issues found: ${ISSUES_COUNT}
- New tests created: ${NEW_TESTS}
- Tests updated: ${UPDATED_TESTS}

## Status
$(if [ ${ISSUES_COUNT} -eq 0 ] && [ ${NEW_TESTS} -gt 0 ]; then
    echo "âœ… Coverage improved, no critical issues"
else
    echo "âš ï¸ Issues remain or coverage gaps exist"
fi)

## Next Steps
$(if [ ${ISSUES_COUNT} -eq 0 ] && [ ${NEW_TESTS} -eq 0 ]; then
    echo "Validation complete - coverage sufficient"
else
    echo "Continue to next iteration for additional validation"
fi)
EOF
    
    echo "Results: ${NEW_TESTS} new, ${UPDATED_TESTS} updated, ${ISSUES_COUNT} issues" | tee -a "${PIPELINE_LOG}"
    
    # Update metadata
    jq ".current_iteration = ${CURRENT_ITERATION} | .iterations[${CURRENT_ITERATION}] = {\"issues\": ${ISSUES_COUNT}, \"new_tests\": ${NEW_TESTS}, \"updated_tests\": ${UPDATED_TESTS}}" \
        "${VALIDATION_WORKSPACE}/metadata.json" > "${VALIDATION_WORKSPACE}/metadata.json.tmp" && \
        mv "${VALIDATION_WORKSPACE}/metadata.json.tmp" "${VALIDATION_WORKSPACE}/metadata.json"
    
    # Check satisfaction criteria
    if [ ${ISSUES_COUNT} -eq 0 ] && [ ${NEW_TESTS} -eq 0 ] && [ ${UPDATED_TESTS} -eq 0 ]; then
        echo "âœ… Validation satisfied - no gaps, no issues" | tee -a "${PIPELINE_LOG}"
        break
    fi
    
    # Increment iteration
    CURRENT_ITERATION=$((CURRENT_ITERATION + 1))
    
    if [ ${CURRENT_ITERATION} -le ${MAX_ITERATIONS} ]; then
        echo "ðŸ”„ Proceeding to iteration ${CURRENT_ITERATION}" | tee -a "${PIPELINE_LOG}"
    fi
    
done  # End iteration loop
```

### Phase 3: Completion

```bash
# Complete validation workflow
echo "===========================================" | tee -a "${PIPELINE_LOG}"
echo "ðŸ“Š VALIDATION PIPELINE COMPLETE" | tee -a "${PIPELINE_LOG}"
echo "===========================================" | tee -a "${PIPELINE_LOG}"

FINAL_REPORT=".claude/workflows/${VALIDATION_ID}/final-coverage-report.md"

# Generate final report
cat > "${FINAL_REPORT}" << EOF
# E2E Test Validation Final Report

**Validation ID:** ${VALIDATION_ID}
**Workflow:** ${WORKFLOW_DESCRIPTION}
**Total Iterations:** ${CURRENT_ITERATION}
**Completion Date:** $(date -u +%Y-%m-%dT%H:%M:%SZ)

## Validation Summary

### Baseline
- Initial test files: $(cat "${BASELINE_DIR}/test-inventory.json" | jq -r '.total_test_files')

### Iteration Results
EOF

# Add each iteration summary
for i in $(seq 1 ${CURRENT_ITERATION}); do
    if [ -f ".claude/workflows/${VALIDATION_ID}/iteration-${i}/validation-results.md" ]; then
        echo "### Iteration ${i}" >> "${FINAL_REPORT}"
        grep -A 10 "## Iteration Summary" ".claude/workflows/${VALIDATION_ID}/iteration-${i}/validation-results.md" >> "${FINAL_REPORT}"
        echo "" >> "${FINAL_REPORT}"
    fi
done

cat >> "${FINAL_REPORT}" << EOF

## Overall Improvements
- Total new tests created: $(find ".claude/workflows/${VALIDATION_ID}"/iteration-*/test-updates/new-tests -name "*.spec.ts" -o -name "*.test.ts" 2>/dev/null | wc -l)
- Total tests updated: $(find ".claude/workflows/${VALIDATION_ID}"/iteration-*/test-updates/updated-tests -name "*.spec.ts" -o -name "*.test.ts" 2>/dev/null | wc -l)
- Total issues resolved: [calculated from iterations]

## Coverage Status
âœ… E2E test coverage validated and improved

## Next Steps
- Run full E2E test suite: \`cd modules/chariot/e2e && npm test\`
- Review test changes in: .claude/workflows/${VALIDATION_ID}/
- Commit updated tests to repository
EOF

# Update final metadata
jq ".status = \"completed\" | .completed_at = \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\" | .total_iterations = ${CURRENT_ITERATION}" \
    "${VALIDATION_WORKSPACE}/metadata.json" > "${VALIDATION_WORKSPACE}/metadata.json.tmp" && \
    mv "${VALIDATION_WORKSPACE}/metadata.json.tmp" "${VALIDATION_WORKSPACE}/metadata.json"

echo "âœ… Final report generated: ${FINAL_REPORT}" | tee -a "${PIPELINE_LOG}"
echo "ðŸ“Š Validation completed in ${CURRENT_ITERATION} iterations" | tee -a "${PIPELINE_LOG}"
echo "ðŸŽ‰ E2E test validation pipeline complete!" | tee -a "${PIPELINE_LOG}"
```

## Resume Capability

To resume an existing validation:

```bash
/validate-e2e e2e-validation-{workflow-name}_{timestamp}
```

The pipeline will:
1. Load existing metadata
2. Resume from current iteration
3. Continue validation cycle
4. Preserve all previous iteration data

## Success Criteria

Validation is considered complete when:
- âœ… No critical issues found in manual validation
- âœ… No coverage gaps identified
- âœ… All recommended tests created/updated
- âœ… E2E test engineer confirms satisfaction
- âœ… Maximum iterations reached (requires manual review)

## Evidence Tracking

Each iteration maintains complete evidence:
- Screenshots of manual validation
- Network logs of API calls
- Issues discovered and resolved
- Test changes and justifications
- Validation results and metrics
